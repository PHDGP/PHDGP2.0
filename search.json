[{"path":"index.html","id":"public-health-disparities-geocoding-project-2.0-training-manual","chapter":"Public Health Disparities Geocoding Project 2.0 Training Manual","heading":"Public Health Disparities Geocoding Project 2.0 Training Manual","text":"Harvard T.H. Chan School Public Health, Boston MA Authors: Christian Testa, Jarvis T. Chen, Enjoli Hall, Dena Javadi, Justin Morgan, Tamara Rushovich, Sudipta Saha, Pamela D. Waterman, Nancy KriegerNovember 2022Visit Public Health Disparities Geocoding Project website!Source funding: American Cancer Society Clinical Research Professor Award \nN. KriegerThis work licensed Creative Commons Attribution-NonCommercial 4.0 International License. Credit use required.Please cite work : Testa C, Chen JT, Hall E, Javadi D, Morgan J, Rushovich T, Saha S, Waterman PD, Krieger N. Public Health Disparities Geocoding Project 2.0. Training Manual. Available October 30, 2022","code":""},{"path":"preface.html","id":"preface","chapter":"Preface","heading":"Preface","text":": Nancy Krieger PhD, Dena JavadiIn June 2004, team based Harvard School Public Health\npublished monograph (Krieger et al, 2004) based research trainings \ncreated Public Health Disparities Geocoding Project (Krieger et al, 2004).\nproject built systematized approaches team members \ndeveloping, since early 1990s, using geocoding \narea-based socioeconomic measures overcome absence \nsocioeconomic data US health records (Krieger, 1992; Krieger, 1998, Krieger et al, 2004).objective boost efforts document inform efforts \naddress socioeconomic inequities health, overall relation \nUS racialized health inequities.note capitalization convention used racial groups: “Mindful respectful different views among anti-racist scholars activists regarding conventions designating US racialized groups,1-3 specifically whether use “w” “W” group “white” / “White” (contrast disagreement capitalizing “b” “Black”), manual opted employ terminology conventions primary source data case studies: US Census Bureau, capitalizes names group. One set arguments favor upper-case “w” “White”1, 2 “[t]o name ‘White’ race , fact, anti-Black act frames Whiteness neutral standard.[…] removes accountability White people’s White institutions’ involvement racism.”2 Conversely, arguments favor lower-case “w” “white” white supremacy note : (1) White-supremacists capitalize “w” “White,”1 (2) “leaving white lowercase represents righting long-standing wrong demand dignity racial equity.”1,31 Appiah KA. Case Capitalizing B Black. Atlantic. June 18, 2020. Accessed 10/25/2022 https://www.theatlantic.com/ideas/archive/2020/06/time--capitalize-blackand-white/613159/2 Thúy Nguyễn . Pendleton M. Recognizing Race Language: Capitalize “Black” “White”. Center Study Social Policy. March 23, 2020. Accessed 10/25/2022 https://cssp.org/2020/03/recognizing-race--language---capitalize-black--white/ 3 Price . Spell Capital “B”. Insight Center Community Economic Development. Oct 1, 2019. Accessed 10/25/2022 https://insightcced.medium.com/spell----capital-b-9eab112d759a.Oriented academic researchers, health department staff, cancer\nregistries, public health students, training offered solution\nproblem lack socioeconomic data US public health\nsurveillance systems: geocoding public health surveillance data\nusing census-derived area-based socioeconomic measures (ABSMs) \ncharacterize cases populations catchment area,\nthereby enabling computation rates stratified area-based\nmeasure socioeconomic position. addition informing analyses\nnumerous scientific investigations, work Public Health\nDisparities Geocoding Project adopted numerous US state local\nhealth departments cancer registries, also informed \ndecisions US National Cancer Institute’s cancer registry system\ngeocode data census tract level enable researchers\naccess county-level ABSMs (public access data) also\ncensus tract level ABSMs (restricted data).Since 2004, huge increase conceptual methodological work regarding use ABSMs document analyze health inequities, computing tools statistical methods measure evolved. particular, availability software\nfacilitates data access, mapping, visualization, fitting multilevel spatial models greatly enhanced accessibility analytic methods public health scientists, advocates, activists, policy makers interested advancing health equity. \nupdated Public Health Disparities Geocoding Project 2.0 training (PHDGP2.0, 2022), held June July 2022 now available manual, builds prior 2004 training, expands & analyze population health health inequities relation census tract, county, georeferenced societal\nenvironmental data.online manual serves guide help users explore following topics:history context , rationale , conducting type workgetting data Census sourcesvisualizing geocoded data social metricsconducting analyses data aggregated specified level geography vs. multi-level analyses 2 levels geographyinterpreting data health equity lensThe PHDGP 2.0 manual also offers three case examples go analysis mortality data (including aggregation spatial analysis), two case examples illustrate use aggregated data American Community Survey (ACS) CDC's PLACES dataset. access data case studies, please visit PHDGP 2.0 website (https://www.hsph.harvard.edu/thegeocodingproject/save--date--public-health-disparities-geocoding-project-2-0/).theory informing work project ecosocial theory disease distribution, first proposed 1994 elaborated since (Krieger, 1994; Krieger 2011, Krieger 2021). theory provides conceptual tools analyzing multilevel spatiotemporal processes embodying ()justice, also calls attention responsible health inequities, holds blocks agency accountability achieve health\njustice (Krieger, 1994; Krieger 2011, Krieger 2021). course materials written much emphasis monitoring health accountability action etiological studies.REFERENCESKrieger N. Ecosocial Theory, Embodied Truths, People’s Health. New York: Oxford University Press, 2021.Krieger N. Epidemiology People’s Health: Theory Context. New York: Oxford University Press, 2011.Krieger N, Waterman PD, Chen JT, Rehkopf DH, Subramanian SV. Public Health Disparities Geocoding Project Monograph. Available June 30, 2004 : http://www.hsph.harvard.edu/thegeocodingprojectKrieger N, Waterman PD, Chen JT, Rehkopf DH, Subramanian SV. Public Health Disparities Geocoding Project Monograph: Publications. Available June 30, 2004 : https://www.hsph.harvard.edu/thegeocodingproject/publications/Krieger N. Epidemiology web causation: anyone seen spider? Soc Sci Med. 1994 Oct;39(7):887-903. doi: 10.1016/0277-9536(94)90202-x.Krieger N. Overcoming absence socioeconomic data medical records: validation application census-based methodology. J Public Health. 1992 May;82(5):703-10. doi: 10.2105/ajph.82.5.703.Krieger N (PI). Area-based socioeconomic measures health data. NIH (NICHD) R01 HD3865-01 (1998-2003).PHDGP.Public Health Disparities Geocoding Project 2.0. Available March 15, 2022 : https://www.hsph.harvard.edu/thegeocodingproject/save--date--public-health-disparities-geocoding-project-2-0/","code":""},{"path":"background.html","id":"background","chapter":"1 Background and History of Analytic Methods","heading":"1 Background and History of Analytic Methods","text":": Nancy Krieger PhD, Dena JavadiIn 2004, team based Harvard T.H Chan School Public Health’s\nDepartment Social Behavioral Sciences published Project\nMonograph\ndescribing \nmotivation\nbehind Public Health Disparities Geocoding Project \nanalytic approaches, conceptually methodologically. \nmonograph dove methods \ngeocoding,\ngenerating area-based social metrics\n(ABSMs),\nMulti-level\nModeling,\ndata\nvisualization,\nbasic epidemiologic methods generating descriptive\nstatistics,\nintent providing population health scientists, health\ndepartment staff, cancer registries, policy makers, advocates \nactivists health justice tools put map – literally\n– rampant underreported socioeconomic inequities health \nlinks racialized spatial health inequities (Krieger et al, 2005; Krieger, 2009).Key publications topics can also found project’s\npublications\npage.Public Health Disparities Geocoding Project informed \necosocial theory disease distribution, developed Dr. Nancy Krieger\n\n1994,\nspecifically “focus people literally biologically embody\nsocietal ecological context multiple levels, across \nlife course historical generations” (Krieger, 2012).Administrative health data, disease surveillance systems, routine\nhealth surveys important tools understanding disease distribution\ninforming public health programming, advocacy, policy\ndevelopment. However, social patterning disease distribution \noften obscured lack robust social metrics, including\nsocioeconomic data, pertinent understanding health inequities -\ndefined unfair, avoidable, preventable health differences across\nsocial groups (Krieger, 2011). social groups, co-defined social\nrelationships involving power, among many groups together\ncomprise “populations” embody health, experience health\ninjustice (health justice), focus public health\nmonitoring, research, action. defined Krieger (2012),\n“populations dynamic beings constituted intrinsic relationships\namong members populations together\nproduce existence.” Addressing health inequities across social\ngroups within populations accordingly requires data \npopulation-defined defining relationships characteristics \ncreate created structures systems. , theories \ndisease distribution underlying agendas, ideologies, \nmotivations contributing implicit explicit use turn shape\ndata get analyzed, analysis interpreted, \nvisualizations used disseminate findings (Krieger, 2011). Misuse poor\nuse data analysis visualization tools can contribute obscuring\nhealth inequities, leaving certain subpopulations misrepresenting\ntrends disease distribution, resulting poor policy decisions \ninadequate misleading data inform community advocacy\norganizing health justice. Therefore, availability\nrobust health information systems important, use \nappropriate methods health equity lens analyses.Using routine information systems inform disease prevention \n21st even 20th century concept. 1829, William Farr, “Compiler \nAbstracts” General Register included letter Register’s\nfirst report stated “Diseases easily prevented cured, first step \nprevention discovery exciting causes. \nRegistry show agency causes numerical facts \nmeasure intensity influence collect information\nlaws vitality variation laws two\nsexes different ages influence civilisation, occupation,\nlocality, seasons physical agencies whether generating\ndiseases inducing death improving public health”\n(Whitehead, 2000).US, linking public health data US census-based\nsocioeconomic data carried National Tuberculosis\nAssociation 1920s 1930s (Green, 1932; Nathan, 1932).\nSimilarly, cancer epidemiologists used geocoded data generate\nstratify cancer incidence, categorizing social groups using\nvariables defined relation “race/ethnicity,” sex, socioeconomic\nposition, many decades (Krieger, 2001). note, US health data \nlong reported US government agencies (federal, state, local)\nstratified “race” “sex,” informed long history biological\nessentialism treats variables matter innate biology,\nattention inequitable racial, gender, class relations\n(Krieger, 2021; Hunter et al, 2005). Adding socioeconomic data mix can aid understanding\ncontribution socioeconomic inequities racialized gender\nhealth inequities, caveat 20th century CE\nframework eugenics (whose shadow continues cast well \n21st c CE) also held people’s socioeconomic position reflects\ngenetic inheritance (Krieger, 2018; Levine, 2017).Despite early emphases social metrics critical \nunderstanding inequitable differential population rates \ndistributions morbidity mortality, integration \nsocioeconomic data national surveillance systems slow. \nrecent OECD report national monitoring systems health\ninequalities socioeconomic status found seven 26\nhigh-income countries included study national routine\nmonitoring systems regular reports socioeconomic inequalities \nhealth time (Frank Matsunaga, 2020).first Public Health Disparities Geocoding Project \nlaunched, presented solution form area-based\nsocioeconomic measures (ABSMs) multilevel approaches \nunderstanding area-based measures, classified socioeconomic\ncharacteristics, used calculate stratified rates render\ninvisible, visible. project articulated lack \nstandardized approach choice geographic levels types \nABSMs used monitoring disease distribution, making comparison across\nheterogeneous methods difficult.project took task identifying ABSMs \napt monitoring socioeconomic inequalities health \ngeographic level. Findings suggested census tract poverty level -\ndefined “percent persons poverty” - apt (Krieger et\nal, 2003).Since , significant development \nconceptualization geocoded health disparities, types ABSMs, \ntechnologies available capture, analyze, visualize .Many ABSMs developed around world. Globally,\nGini coefficient one standards measuring income\ninequality (caveats around use beyond larger aggregations \nissues around spatial social polarization) (Shaw et al, 2007; Krieger et\nal, 2016). Canada, examples include Socioeconomic Factor Index\n(SEFI), General Deprivation Index (GDI), Deprivation Index\nHealth Welfare Planning Quebec (DIHWPQ) (Schuurman et al,\n2007). Starting early 20th c CE, UK began using Registrar\nGeneral’s social class classification systems (ad hoc approach based\nskill-level demarcations occupational class), \nreplaced 2000 theoretically-grounded National Statistics\nSocio-economic Classification (NS-SEC), emphasizes employment\nrelations conditions occupations (UK Office National Statistics, 2022). Also commonly used English Index Multiple Deprivation (McLennan et al, 2019). US, studies\ngenerated used composite indices deprivation social\nvulnerability based selected census variables (O’Campo et al, 2008;\nMesser et al, 2006; Hu et al, 2021; ATSDR 2022).However, one problem common many indices, also single-variable\nmeasures (percent poverty), provide\ninsight power relations spatial social polarization driving\nhealth inequities (Krieger et al, 2016; Krieger et al, 2017). example, metric “percent \npoverty,” useful describing socioeconomic gradients health,\nnotably provides information income distribution \n“poverty,” can range barely poverty extremely\naffluent. Similarly, commonly used variable US, \n“percent population classified Black Americans” says nothing\ndistribution racialized groups social\nrelationships core racialized economic segregation. \nadditional problem diverse metrics intended measure\ninequality across full population distribution, Gini\ncoefficient dissimilarity index (measures proportion \npopulation need move within geographic area achieve\nevenness distribution), meaningful higher\ngeographic levels (Krieger et al, 2016; Krieger et al, 2017). issue , within US, policies \npractices, past present, generate enforce racialized economic\nsegregation worked buttress neighborhood boundaries, especially\nkeep areas White affluent relegate lower-income\npopulations, disproportionately Black, Latinx, Indigenous immigrant\nUS underdeveloped neighborhoods lacking resources people\nthrive (Krieger et al, 2016; Krieger et al, 2017; Rothstein, 2017; Bailey et al, 2017).new approach capturing extreme range concentrations \neconomic deprivation privilege, termed “Index Concentration\nExtremes” (ICE), developed 2001 Douglass Massey, \nleading US scholar residential segregation (Massey, 2001; Massey, 1996; Massey, 2012). measure, ranges -1 1 captures extent area’s population concentrated \none end extremes privilege deprivation, notably\ncan used meaningfully multiple levels geography, census\nblock counties higher. recent years, members \nPublic Health Disparities Geocoding Project team produced work\npromoting use ICE public health research, also extending\nMassey’s original work develop ICE measures quantify \nracialized residential segregation also racialized economic\nsegregation, latter comprising first metric kind\n(Krieger et al, 2016; Krieger et al, 2016B; Scally et al, 2018; Krieger\net al, 2018; Chen & Krieger, 2021; Krieger et al, 2015). intent provide insight\ndrives health inequities, just focus solely \nharmed (Krieger et al, 2010; Beckfield, 2018; Bambra et al, 2021; Bailey et al, 2017).Covid-19 pandemic - due social geographic patterning \nspread virus associated hospitalizations deaths - \nhighlighted critical need improved surveillance systems \nsystematic monitoring health inequities (Presidential Task Force, 2021; Bambra, 2021). result, \nPublic Health Disparities Geocoding Project compiled resources \nsupport efforts carrying analyses health inequities \ncontext (Krieger, Chen, Waterman, 2020).2020 update \nproject,\nshared May 2020 thick first months pandemic,\nprovides:List conceptual empirical publicationsList conceptual empirical publicationsList variables constructed using US Census American Community\nSurvey (ACS) dataList variables constructed using US Census American Community\nSurvey (ACS) dataR code extracting ABSMs ACS replicating analyses \npublished empirical papersR code extracting ABSMs ACS replicating analyses \npublished empirical papersThe Public Health Disparities Geocoding Project 2.0 training (held online\nJune July 2022) now available manual builds \nwork team throughout pandemic offering updated revised\ntraining & analyze population health health inequities \nrelation census tract, county, georeferenced societal \nenvironmental data. area-based metrics employ study\ninclude diverse social metrics, longer restricted solely economic\nmeasures, employ updated terminology “area-based social metrics”\n– continue abbreviate “ABSMs.”online manual walk step training\nincluding:Chapter 2: Getting Set UpChapter 2: Getting Set UpChapter 3: Getting DataChapter 3: Getting DataChapter 4: Visualizing DataChapter 4: Visualizing DataChapter 5: Analyzing dataChapter 5: Analyzing dataChapter 6: Case Study - Premature MortalityChapter 6: Case Study - Premature MortalityChapter 7: Case Study - Breast Cancer MortalityChapter 7: Case Study - Breast Cancer MortalityChapter 8: Case Study - Cook County Covid-19Chapter 8: Case Study - Cook County Covid-19Chapter 9: Case Study - Temporal Trends using American Community Survey (ACS) data (2012-2019)Chapter 9: Case Study - Temporal Trends using American Community Survey (ACS) data (2012-2019)Chapter 10: Case Study - Comparing County Analyses Inequities Health Insurance using ACS vs. CDC PLACES data (2019)\nSurveyChapter 10: Case Study - Comparing County Analyses Inequities Health Insurance using ACS vs. CDC PLACES data (2019)\nSurveyChapter 11: ConclusionsChapter 11: ConclusionsWe hope resource use . questions \ncomments, please reach : geoproj@hsph.harvard.eduREFERENCESAgency Toxic Substances Disease Registry (ATSDR). CDC/ATSDR Social Vulnerability Index. https://www.atsdr.cdc.gov/placeandhealth/svi/index.html ; accessed June 14, 2022.Bailey ZD, Krieger N, Agénor M, Graves J, Linos N, Bassett MT. Structural racism health inequities USA: evidence interventions. Lancet. 2017 Apr 8;389(10077):1453-1463. doi: 10.1016/S0140-6736(17)30569-X.Bambra C, Lynch J, Smith KE. Unequal Pandemic: COVID-19 Health Inequalities. Bristol, UK: Policy Press, University Bristol, 2021.Beckfield J. Political Sociology People’s Health. New York: Oxford University Press, 2018.Chen JT Krieger N. Revealing unequal burden \nCOVID-19 income, race/ethnicity, household crowding: US county\nversus zip code analyses. Journal Public Health Management \nPractice. 2021; 27(1), pp.S43-S56.Frank JW Matsunaga E. National monitoring systems \nhealth inequalities socioeconomic status–OECD snapshot. Critical\nPublic Health. 2020; pp.1-8. doi: 10.1080/09581596.2020.1862761Green HW. Tuberculosis economic strata, Cleveland’s Five-City\nArea, 1928-1931. Cleveland, OH: Anti-Tuberculosis League, 1932.Hu J, Bartels CM, Rovin RA, Lamb LE, Kind AJH, Nerenz DR. Race, Ethnicity, Neighborhood Characteristics, -Hospital Coronavirus Disease-2019 Mortality. Med Care. 2021 Oct 1;59(10):888-892. doi: 10.1097/MLR.0000000000001624. 1.Hunter E, Friedman D, Parrish R (eds). Health statistics : Shaping policy practice improve population’s health. New York ; Oxford: Oxford University Press, 2005.Krieger N. Epidemiology web causation: anyone seen spider? Soc Sci Med. 1994 Oct;39(7):887-903. doi: 10.1016/0277-9536(94)90202-x.Krieger, N. Socioeconomic data cancer registries. J Public Health. 2001; 91(1), p.156.Krieger, N. Chen, J.T., Waterman, P.D., Rehkopf, D.H. Subramanian,\nS.V. Race/ethnicity, gender, monitoring socioeconomic\ngradients health: comparison area-based socioeconomic\nmeasures—public health disparities geocoding project. J Public Health. 2003; 93(10), pp.1655-1671.Krieger N, Chen JT, Waterman PD, Rehkopf DH, Subramanian SV. Painting truer picture US socioeconomic racial/ethnic health inequalities: Public Health Disparities Geocoding Project. J Public Health. 2005 Feb;95(2):312-23. doi: 10.2105/AJPH.2003.032482.Krieger N. Putting health inequities map: social epidemiology meets medical/health geography––ecosocial perspective. GeoJournal. 2009 Apr;74(2):87-97.Krieger N, Alegría M, Almeida-Filho N et al. , , causes health inequities? Reflections emerging debates exploratory Latin American/North American workshop. J Epidemiol Community Health. 2010;64(9):747–749.Krieger N. Epidemiology People’s Health: Theory Context. New York: Oxford University Press, 2011.Krieger N, 2012. “population”? Historical debates,\ncurrent controversies, implications understanding “population\nhealth” rectifying health inequities. Milbank Quarterly, 90(4),\npp.634-681.Krieger N, Waterman PD, Gryparis , Coull BA. Black carbon exposure, socioeconomic racial/ethnic spatial polarization, Index Concentration Extremes (ICE) Health Place. 2015;34:215–228.Krieger N, Waterman PD, Spasojevic J, Li W, Maduro G. Van\nWye, G. Public health monitoring privilege deprivation\nindex concentration extremes. J Public Health. 2016; 106(2), pp.256-263.Krieger N, Singh N, Waterman PD. Metrics monitoring\ncancer inequities: residential segregation, Index Concentration\nExtremes (ICE), breast cancer estrogen receptor status (USA,\n1992–2012). Cancer Causes & Control. 2016B; 27(9), pp.1139-1151.Krieger N, Feldman JM, Waterman PD, Chen JT, Coull BA, Hemenway D. Local Residential Segregation Matters: Stronger Association Census Tract Compared Conventional City-Level Measures Fatal Non-Fatal Assaults (Total Firearm Related), Using Index Concentration Extremes (ICE) Racial, Economic, Racialized Economic Segregation, Massachusetts (US), 1995-2010. J Urban Health. 2017 Apr;94(2):244-258. doi: 10.1007/s11524-016-0116-z.Krieger N, Feldman JM, Kim R, Waterman, PD. Cancer\nincidence multilevel measures residential economic racial\nsegregation cancer registries. JNCI Cancer Spectrum. 2018; 2(1), p.pky009.Krieger N. Inheritance Health: Really Matters? J Public Health. 2018 May;108(5):606-607. doi: 10.2105/AJPH.2018.304353.Krieger N, Chen JT, Waterman PD. Using methods Public Health Disparities Geocoding Project monitor COVID-19 inequities guide action social justice. Available May 15, 2020 :\nhttps://www.hsph.harvard.edu/thegeocodingproject/covid-19-resources/Krieger N. Structural Racism, Health Inequities, Two-Edged Sword Data: Structural Problems Require Structural Solutions. Front Public Health. 2021 Apr 15;9:655447. doi: 10.3389/fpubh.2021.655447.Levine P. Eugenics: Short Introduction. New York: Oxford University Press, 2017.Massey DS. age extremes: concentrated affluence poverty twenty-first century. Demography. 1996;33(4):395–412.Massey DS. prodigal paradigm returns: ecology comes back sociology. : Booth , Crouter , editors. Take Village? Community Effects Children, Adolescents, Families. Mahwah, NJ: Lawrence Erlbaum Associates; 2001. pp. 41–48.Massey DS. Reflections dimensions segregation. Soc Forces. 2012;91(1):39–43.McLennan D, Noble S, Noble M, Plunkett E, Wright G, Gutacker N. English indices deprivation 2019: technical report. 2019. https://www.gov.uk/government/statistics/english-indices--deprivation-2019. Accessed June 5th, 2022.Messer LC, Laraia BA, Kaufman JS, Eyster J, Holzman C,\nCulhane J, Elo , Burke JG, O’campo P. development\nstandardized neighborhood deprivation index. Journal Urban\nHealth. 2006; 83(6), pp.1041-1062.Nathan WB. Health conditions North Harlem 1923-1927. New York:\nNational Tuberculosis Association, 1932.O’Campo P, Burke JG, Culhane J, Elo , Eyster J, Holzman C, Messer LC, Kaufman JS, Laraia BA. Neighborhood\ndeprivation preterm birth among non-Hispanic Black White women\neight geographic areas United States. J Epidemiology. 2008; 167(2), pp.155-163.Presidential COVID-19 Health Equity Task Force. Final reportand recommendations. HHS, Office Minority Health. https://minorityhealth.hhs.gov/omh/browse.aspx?lvl=2&lvlid=100. Updated November 10, 2021. Accessed June 14, 2022.Rothstein R. color law : forgotten history government segregated America (First ed., Democracy urban landscapes). New York ; London: Liveright Publishing Corporation, division W.W. Norton & Company, 2017.Schuurman N, Bell N, Dunn JR, Oliver L. Deprivation\nindices, population health geography: evaluation spatial\neffectiveness indices multiple scales. Journal Urban Health. 2007;\n84(4), pp.591-603.Scally BJ, Krieger N. Chen JT. Racialized economic\nsegregation stage diagnosis colorectal cancer United\nStates. Cancer Causes & Control. 2018; 29(6), pp.527-537.Shaw M, Galobardes B, Lawlor DA, Lynch J, Wheeler B, Davey Smith G.\nHandbook Inequality Socioeconomic Position: Concepts\nMeasures. Bristol, UK: Policy Press, 2007.UK Office National Statistics. National Statistics Socio-economic classification (NS-SEC). https://www.ons.gov.uk/methodology/classificationsandstandards/otherclassifications/thenationalstatisticssocioeconomicclassificationnssecrebasedonsoc2010 ; accessed June 14, 2022.Whitehead M. William Farr’s legacy study inequalities\nhealth. Bulletin World Health Organization. 2000; 78(1), p.86.","code":""},{"path":"bios.html","id":"bios","chapter":"2 Author Bios","heading":"2 Author Bios","text":"Nancy Krieger, PhD (/) (Principle Investigator Public Health\nDisparities Training Project 2.0) Professor Social Epidemiology American Cancer\nSociety Clinical Research Professor, Department Social Behavioral Sciences,\nHarvard T.H. Chan School Public Health (HSPH), also Director HSPH\nInterdisciplinary Concentration Women, Gender, Health. internationally\nrecognized social epidemiologist (PhD, Epidemiology, UC Berkeley, 1989), \nbackground biochemistry, philosophy science, history public health, plus 35+\nyears activism involving social justice, science, health. Dr. Krieger’s work\naddresses: (1) conceptual frameworks understand, analyze, improve people’s\nhealth, including ecosocial theory disease distribution focused embodiment\nequity; (2) etiologic research societal determinants population health \nhealth inequities, including structural racism types adverse discrimination;\n(3) methodologic research improve monitoring health inequities. launched\ninitial Public Health Disparities Geocoding Project late 1990s improve\nmonitoring, analysis action entangled impacts social class racism \npopulation health health inequities.Jarvis Chen (/) social epidemiologist Lecturer Social Behavioral\nSciences Harvard T.H. Chan School Public Health. research focuses \nmethods analyzing understanding social inequities health, particularly \nrelation structural racism socioeconomic deprivation. methodologist, Dr.\nChen’s interests include development methods geospatial spatiotemporal\nanalysis, disease mapping, causal inference social epidemiology. Dr. Chen also\nAssociate Director PhD Population Health Sciences Program Harvard\nUniversity’s Graduate School Arts Sciences teaches several quantitative\nresearch methods courses school.Enjoli Hall (//) PhD student Department Urban Studies \nPlanning Massachusetts Institute Technology (MIT). work focuses \nbuilding infrastructures collective care action understand intervene \npolitical economic determinants health.Dena Javadi (/) PhD student Population Health Sciences \nDepartment Social Behavioral Sciences Harvard T.H. Chan School Public\nHealth. prior work Health Policy Systems Research, focus \nintersectoral action health. Currently, research explores structural\ndeterminants work-related health wellbeing.Justin Morgan (/) currently pursuing Ph.D. Population Health Sciences\nSocial Behavioral Sciences department. research interests center \npower political determinants health, focus practition \ncommunity engaged research assess address health equity.Tamara Rushovich (/) s current PhD candidate Population Health Sciences.\nresearch focuses ways social factors societal structures shape health.\nPrior starting PhD, Tamara worked social services Washington, DC \nepidemiologist Chicago Department Public Health. BA Sociology\nMPH Epidemiology University Michigan.Sudipta Saha (/) Population Health Sciences PhD student \nDepartment Social Behavioral Sciences Harvard University. current\nresearch interests intersection social epidemiologic theories \ninfectious disease models. particularly interested treating racial capitalism \nfundamental cause health inequities understand/illustrate broader\npolitical-economic forces shape inequities. BSc Microbiology \nUniversity Toronto, Master Science Global Health Population \nHarvard T.H. Chan School Public Health.Christian Testa (/) statistical analyst programmer focused \nmodeling health outcomes characterizing health inequities. ongoing work \nfocused COVID-19, epigenetic aging, survey data collection analysis,\ndiscrimination, area based social metrics. Christian particularly interested \napplication flexible machine learning approaches causal inference well\nuse data visualization effective communication scientific findings\nassociated uncertainty.","code":""},{"path":"getting-setup.html","id":"getting-setup","chapter":"3 Getting Setup with R and RStudio","heading":"3 Getting Setup with R and RStudio","text":": Christian TestaWe recommend moving , readers download install R RStudio\nrecommended R packages dependencies.","code":""},{"path":"getting-setup.html","id":"principles-for-reproducible-workflow-and-programming","chapter":"3 Getting Setup with R and RStudio","heading":"3.0.1 Principles for Reproducible Workflow and Programming","text":"Two principles underlie R RStudio chosen use throughout\ncourse materials. First, matter equity, use free (free\nuse freely licensed) software R RStudio removes financial \nadministrative barriers engaging work facilitates inclusion\npeople diverse backgrounds science. Second, data analysis \nscience open transparent wherever whenever possible, \npromote external reproduction, verification, validation findings.code examples book assume familiarity R programming\nlanguage. can still benefit book reading exposition without\nfocusing code examples familiar R programming.want become familiar R programming, may want start \nfamiliarizing R. R Data Science book, available free \nonline : https://r4ds..co.nz/, provides free, online, accessible\nintroduction.","code":""},{"path":"getting-setup.html","id":"downloading-and-installing-r-and-rstudio","chapter":"3 Getting Setup with R and RStudio","heading":"3.1 Downloading and Installing R and RStudio","text":"order follow along resources, need R \nRStudio installed setup. can download R \nhttps://www.r-project.org/. can download RStudio \nhttps://www.rstudio.com/.","code":""},{"path":"getting-setup.html","id":"basic-features-of-r","chapter":"3 Getting Setup with R and RStudio","heading":"3.2 Basic Features of R","text":"Throughout text, example code given depend various R packages\navailable free open-source, easily installed R \ninstall.packages function.work case examples , need install least \nfollowing packages:","code":"\n# for data manipulation and visualization\ninstall.packages(\"tidyverse\")\ninstall.packages(\"Hmisc\") # mostly for the weighted quantile function\ninstall.packages(\"fastDummies\") # for creating \"dummy\"/ \"indicator\" variables\n\n\n# for retrieving census data\ninstall.packages(\"tidycensus\") # note the special instructions below\n\n# for mapping\ninstall.packages(\"sf\") # note the special instructions below\ninstall.packages(\"mapview\")\ninstall.packages(\"tigris\")\ninstall.packages(\"leaflet\")\n\n\n# for visualization/color palettes\ninstall.packages(\"RColorBrewer\")\ninstall.packages(\"viridis\")\ninstall.packages(\"cowplot\")\n\n# for multilevel modeling\ninstall.packages(\"lme4\")\n\n# for spatial modeling\ninstall.packages(\"INLA\",repos=c(getOption(\"repos\"),INLA=\"https://inla.r-inla-download.org/R/stable\"), dep=TRUE)"},{"path":"getting-setup.html","id":"note-about-compiling-from-source","chapter":"3 Getting Setup with R and RStudio","heading":"3.2.1 Note about “compiling from source”","text":", installing packages, R prompts asking whether like\ninstall packages “source,” need . Sometimes compiling\nsource can difficult installing pre-built packages \nCRAN compilation errors.","code":""},{"path":"getting-setup.html","id":"tidycensus-special-instructions","chapter":"3 Getting Setup with R and RStudio","heading":"3.2.2 Tidycensus Special Instructions","text":"need register Census API key part setup\nprocedures use tidycensus.See instructions installing tidycensus\nwebsite.Note need run census_api_key(\"API KEY GOES \")\ncommand .","code":""},{"path":"getting-setup.html","id":"sf-special-instructions","chapter":"3 Getting Setup with R and RStudio","heading":"3.2.3 sf Special Instructions","text":"sf package, additional instructions : https://r-spatial.github.io/sf/\nOS (Mac, Windows, Linux) specific, walk getting setup\ngdal (Geospatial Data Abstraction Library, translator library\nraster vector geospatial data formats) dependency sf.","code":""},{"path":"getting-setup.html","id":"inla-install-instructions","chapter":"3 Getting Setup with R and RStudio","heading":"3.2.4 INLA Install Instructions","text":"code installs stable version INLA. ever need\nupgrade installation, instructions online :\nhttps://www.r-inla.org/download-install","code":""},{"path":"getting-setup.html","id":"references-for-spatial-programming-in-r","chapter":"3 Getting Setup with R and RStudio","heading":"3.3 References for Spatial Programming in R","text":"additional reference material supplement R programming code provided\n, recommend:Geocomputation R Robin Lovelace, Jakub Nowosad, Jannes Muenchow.\nhttps://geocompr.robinlovelace.net/.Spatial Data Science Applications R Edzer Pebesma Roger Bivand.\nhttps://r-spatial.org/book/.Geospatial Health Data: Modeling Visualization R-INLA Shiny Paula Moraga. https://www.paulamoraga.com/book-geospatial/index.html.","code":""},{"path":"getting-your-data.html","id":"getting-your-data","chapter":"4 Getting your data","heading":"4 Getting your data","text":": Christian Testa, Jarvis Chen ScD, Enjoli Hall, Dena Javadi, Tamara Rushovich","code":""},{"path":"getting-your-data.html","id":"high-level-overview","chapter":"4 Getting your data","heading":"4.1 High Level Overview","text":"application mapping area health outcome rates, georeferenced data \nmade three components: health outcome counts, population estimates, \ngeographic boundaries areal units interest. data may\ncome different data sources, linked together practice\nmerging data together geographic area identifiers.\nFigure 4.1: workflow schematic, components creating georeferenced data shown first two rows.\nexamples area identifiers common United States context include\ncodes ZIP codes, county FIPS codes, census block census tract IDs, etc.\nshown Figure 4.2 (), two\nmajor sets geographies employed US Census Bureau. first concerns\nfundamental administrative political units (referred “spine” US\nCensus geography) relevant primary rationale US census\nallocate political representation (Krieger 2019); areas include census\nblock, block-groups, tracts, counties, states, corresponding units \nAmerican Indian, Alaska Native Native Hawaiian areas, census block\ncore area used create voting districts (local, state, \nfederal levels). second set includes array additional areas, \noften cross boundaries core Census geographic areas, ZIP\nCodes, school districts, traffic analysis zones, etc.studying area level health outcome rates, may find must\nsource different necessary pieces different places. Population\nestimates can often come Census related data products, health\noutcomes may come healthcare providers, public health departments,\nelectronic health records (EHR), sources, geography shapefiles \nsimilarly available numerous sources including Census.","code":""},{"path":"getting-your-data.html","id":"data-sources","chapter":"4 Getting your data","heading":"4.2 Data Sources","text":"Health data can found numerous formats, download \nwebpage .csv .xlsx files, accessible querying application\nprogramming interface (API), available direct request \nhealth department agency.sources data ’ve used case examples follows:US Census American Community Survey data retrieved API https://www.census.gov/data/developers/data-sets.html via tidycensus\npackageCDC Places data downloaded directly .csv files https://www.cdc.gov/places/Cook County Medical Case Examiner Archive data COVID-19 deaths \ndownloaded directly https://datacatalog.cookcountyil.gov/Public-Safety/Medical-Examiner-Case-Archive-COVID-19-Related-Dea/3trz-enysMassachusetts mortality data requested directly Massachusetts\nDepartment Public HealthThe Social Vulnerability Index (SVI) available online download csv \nshapefiles available https://www.atsdr.cdc.gov/placeandhealth/svi/index.html","code":""},{"path":"getting-your-data.html","id":"loading-spreadsheet-data-into-r","chapter":"4 Getting your data","heading":"4.3 Loading Spreadsheet Data into R","text":".csv file downloaded, can use readr package\n(part tidyverse) load R.Health data often come kinds delimited formats \ntab-delimited fixed-width spaced, case can use \nread_tsv read_fwf functions readr package similarly\nuse read_csv. Learn readr : https://readr.tidyverse.org/read Excel data, recommend using readxl package, also part \ntidyverse. Learn : https://readxl.tidyverse.org/Even data delimited text document Excel file, \ncommon file format, still quite likely can read data\nR using packages. example haven package allows users \nread SAS, SPSS, Stata files. Read https://haven.tidyverse.org/","code":"\nlibrary(readr)\nexample_df <- read_csv(\"filename.csv\")\n\n# learn more about the options read_csv has by running:\n?read_csv\nlibrary(readxl)\nexample_df <- read_excel(\"filename.xlsx\", sheet = 1)\n# to learn more about the options in the read_excel function, run:\n?read_excel\n\n# one particularly helpful feature to know about is the range argument which\n# allows the user to specify they want to read a dataframe from a specific \n# range of cells using Excel-style range syntax:\nexample_df <- read_excel(\"filename.xlsx\", sheet = 1, range = \"A3:C17\")"},{"path":"getting-your-data.html","id":"connecting-to-databases","chapter":"4 Getting your data","heading":"4.4 Connecting to Databases","text":"Many online health datasets accessible via query remote database\nserver. References interact databases R available :\nhttps://db.rstudio.com/, following reference shows interact \nremote database tidyverse style: https://dbplyr.tidyverse.org/","code":""},{"path":"getting-your-data.html","id":"tidycensus","chapter":"4 Getting your data","heading":"4.5 tidycensus","text":"databases, R programmers already written packages help users\nsubmit queries get back data interest. One example\nU.S. Census, tidycensus package exists automate\nfetching Census data R.tidycensus package R allows download data US Census\nBureau products, including decennial Census 1-year, 3-year, \n5-year American Community Survey (ACS). Find detailed reference materials \nintroduction tidycensus : https://walker-data.com/tidycensus/section, walk example code downloads \npercent residents poverty line computing Index \nConcentration Extremes (ICE) racialized economic segregation \n2015-2019 ACS. example, demonstrate download measures\ncensus tract level Suffolk County, Massachusetts noting \ncounty includes city Boston, PHDGP 2.0 training team members\nbased!can find variables available 5-year ACS following\nlink, changing 2019 desired year starting 2009\nACS began: https://api.census.gov/data/2019/acs/acs5/variables.html\n(#fig:geoid_structure)structure Census tract GEOIDs\n\nFigure 4.2: Hierarchy geographic units assignged US Census Bureau. Reproduced https://www.census.gov/content/dam/Census/data/developers/geoareaconcepts.pdf\n’s example code downloading data percent people living\nhouseholds household income less poverty threshold.\npoverty thresholds defined US Census\n(see: https://www.census.gov/topics/income-poverty/poverty/guidance/poverty-measures.html).can also use mapview package render interactive maps estimates data computed/downloaded. particularly helpful understanding geography relate estimates — especially see airports, green-space, schools, municipal zones may located zoned include residential units (hence NaN “number” estimates shown maps), may nevertheless locations unsheltered unhoused persons reside.\nEstimates percent individuals poverty based American Community Survey 2015-2019\nreference, poverty thresholds US dollars 2017 (.e. \nmiddle year 2015-2019 time-period) Census:Now ’ve created static interactive maps poverty estimates \nSuffolk County, can move downloading computing Index \nConcentration Extremes Racialized Economic Segregation (High Income\nWhite non-Hispanic High Income vs. Low Income People Color).\nEstimates Index Concentration Extremes Racialized Economic Segregation\n(white non-Hispanic high income vs. People Color low income) based American Community Survey 2015-2019\n","code":"\n# example code for downloading poverty measures from the American Community\n# Survey through tidycensus and visualizing them through maps\n\n# load the packages we'll use for this section\nlibrary(tidycensus)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(RColorBrewer)\nlibrary(mapview)\n\n# download the data from the ACS using the get_acs method from tidycensus\n# \n# the B05010_002E variable refers to the count of residents who live in\n# households with household income below the poverty line; the B05010_001E\n# variable refers to the count of residents for whom household income was\n# ascertained by the ACS, e.g. the relevant denominator.\n# \npoverty <- get_acs(\n  state = 'MA',\n  county = '025', # this is the FIPS code for Suffolk County, MA\n  geography = 'tract',\n  year = 2019, # this indicates the 2015-2019 5-year acs \n  geometry = TRUE,\n  variables = c(\n    in_poverty = 'B05010_002E', \n    total_pop_for_poverty_estimates = 'B05010_001E') \n)\n\n\n# we're going to recode the variable names to more human-readable names to \n# make it easier to work with the data in subsequent steps\npoverty <- poverty %>% \n  mutate(\n    variable = recode(variable,\n                     # you may notice that tidycensus drops the 'E' from the \n                     # end of the variable code names\n                     B05010_002 = 'in_poverty',\n                     B05010_001 = 'total_pop_for_poverty_estimates'))\n\n# pivot the data wider so that the in_poverty and\n# total_pop_for_poverty_estimates; this follows the \"tidy\" format and approach\n# where each row corresponds to an observation.\n# \n# because the pivot_wider method can mess up your data when your data contains\n# geometry/shapefile information, we will remove the geomemtry information\n# and add it back in later\npoverty_geometry <- poverty %>% select(GEOID) %>% unique() # save the geometry data\npoverty <- poverty %>% \n  sf::st_drop_geometry() %>% # remove geometry data\n  tidyr::pivot_wider(\n    id_cols = GEOID,\n    names_from = variable,\n    values_from = c(estimate, moe))\n\n# calculate the proportion in poverty\npoverty <- poverty %>% \n  mutate(\n    proportion_in_poverty = estimate_in_poverty / estimate_total_pop_for_poverty_estimates,\n    percent_in_poverty = proportion_in_poverty * 100)\n\n# add the geometry back in -- \n# make sure to merge the data into the sf object with the sf object on the \n# left hand side so the output has the sf type including your geometry data\npoverty <- poverty_geometry %>% \n  left_join(poverty)\n\n# visualize our point estimates \nggplot(poverty, aes(fill = proportion_in_poverty)) +\n  geom_sf() +\n  scale_fill_viridis_c(label = scales::percent_format(),\n                       limits = c(0, 1)) +\n  labs(fill = \"Percent in Poverty\") +\n  ggtitle(\"Poverty Estimates in Suffolk County, Massachusetts\",\n          subtitle = \"Based on American Community Survey 2015-2019 Estimates\")\n\n# visualize the denominator counts -- \n# of significance, note that there are several census tracts where the\n# denominator is 0 resulting in NaN estimates for the percent in poverty.\nggplot(poverty, aes(fill = estimate_total_pop_for_poverty_estimates)) + \n  geom_sf() + \n  scale_fill_viridis_c(label = scales::comma_format(), direction = -1, \n                       breaks = c(0, 10, 100, 1000), trans = \"log1p\") + \n  labs(fill = \"Number of People\") + \n  ggtitle(\"Number of People in Denominator for Poverty Estimates\", \n          paste0(\"Suffolk County, Massachusetts\\n\",\n          \"Based on American Community Survey 2015-2019 Estimates\"))\nlibrary(mapview)\nmapview::mapview(poverty, zcol = 'percent_in_poverty')\n# example code for creating the index of concentration at the extremes for the \n# measure of racialized economic segregation (high income white non-hispanic \n# vs. low income people of color) using tidycensus\n\n# create a data dictionary detailing the variables we're going to use - \n# \n# associating each of the variables a more readable/friendly `shortname` and a\n# description can help make the subsequent code more readable and thus easier\n# to debug in case you run into any errors.\n# \nvariables_dict <-\n  tibble::tribble(\n  ~var,          ~shortname,      ~desc,\n  \"B19001_001\",  'hhinc_total',   \"total population for household income estimates\",\n  \"B19001A_002\", 'hhinc_w_1',     \"white n.h. pop with household income <$10k\",\n  \"B19001A_003\", 'hhinc_w_2',     \"white n.h. pop with household income $10k-14 999k\",\n  \"B19001A_004\", 'hhinc_w_3',     \"white n.h. pop with household income $15k-19 999k\",\n  \"B19001A_005\", 'hhinc_w_4',     \"white n.h. pop with household income $20k-24 999k\",\n  \"B19001A_014\", 'hhinc_w_5',     \"white n.h. pop with household income $100 000 to $124 999\",\n  \"B19001A_015\", 'hhinc_w_6',     \"white n.h. pop with household income $125k-149 999k\",\n  \"B19001A_016\", 'hhinc_w_7',     \"white n.h. pop with household income $150k-199 999k\",\n  \"B19001A_017\", 'hhinc_w_8',     \"white n.h. pop with household income $196k+\",\n  \"B19001_002\",  'hhinc_total_1', \"total pop with household income <$10k\",\n  \"B19001_003\",  'hhinc_total_2', \"total pop with household income $10k-14 999k\",\n  \"B19001_004\",  'hhinc_total_3', \"total pop with household income $15k-19 999k\",\n  \"B19001_005\",  'hhinc_total_4', \"total pop with household income $20k-24 999k\"\n )\n\n# fetch data from the american community survey API (or application programming interface)\nICEraceinc <- get_acs(\n  geography = 'tract',\n  state = 'MA',\n  county = '025',\n  geometry = TRUE,\n  year = 2019,\n  variables = variables_dict$var)\n\n# save the geommetry data separately\nICEraceinc_geometry <- ICEraceinc %>% select(GEOID) %>% unique()\n\n# remove geometry data so we can use pivot_wider\nICEraceinc <- ICEraceinc %>% sf::st_drop_geometry()\n\n# pivot to a wide format for renaming, dropping the margin of error data\nICEraceinc <- ICEraceinc %>% select(-moe) %>% \n  pivot_wider(names_from = variable, values_from = estimate)\n\n# rename the columns using our rename_vars\n# \n# first we create a named vector, rename_vars, which has elements that are the\n# acs variables we request and convenient, human readable names.\n# \n# then we use rename_vars with the rename function from dplyr. \n# typically the rename function takes a syntax as follows: \n#   data %>% rename(newname1 = oldname1, newname2 = oldname2, ...)\n# but in our case, we already have a named vector (rename_vars) that we \n# want to use, and so to use the rename_vars named vector inside rename\n# we use the injection-operator `!!`.  you can learn more about the injection\n# operator by running ?`!!` in your R console. \nrename_vars <- setNames(variables_dict$var, variables_dict$shortname)\nICEraceinc <- ICEraceinc %>% rename(!!rename_vars)\n\n# calculate the ICE for racialized economic segregation\nICEraceinc <- ICEraceinc %>% \n  mutate(\n    # we calculate the people of color low income counts as the overall \n    # low income counts minus the white non-hispanic low income counts\n    people_of_color_low_income = \n      (hhinc_total_1 + hhinc_total_2 + hhinc_total_3 + hhinc_total_4) - \n      (hhinc_w_1 + hhinc_w_2 + hhinc_w_3 + hhinc_w_4),\n    # sum up the white non-hispanic high income counts\n    white_non_hispanic_high_income = \n      (hhinc_w_5 + hhinc_w_6 + hhinc_w_7 + hhinc_w_8),\n    # calculate the index of concentration at the extremes for racialized \n    # economic segregation (high income white non-hispanic vs. low income \n    # people of color)\n    ICEraceinc = \n      (white_non_hispanic_high_income - people_of_color_low_income) / \n      hhinc_total\n  )\n\n# now we can merge our spatial geometry data back in\nICEraceinc <- ICEraceinc_geometry %>% \n  left_join(ICEraceinc %>% select(GEOID, ICEraceinc))\n\n# visualize our data - \n# here we use a divergent color palette since the ICEraceinc measure \n# is divergent in nature\nggplot(ICEraceinc, aes(fill = ICEraceinc)) +\n  geom_sf() +\n  scale_fill_distiller(palette = 'BrBG') +\n  labs(fill = \"ICE for Racialized Economic Segregation:\\nWhite non-Hispanic (High Income) vs.\\nPeople of Color (Low Income)\") +\n  ggtitle(\n    \"Index of Concentration at the Extremes, Racialized Economic Segregation\",\n    paste0(\"Suffolk County, MA\\n\",\n           \"Based on American Community Survey 2015-2019 Estimates\")\n  ) \nmapview(ICEraceinc, zcol = 'ICEraceinc', \n        col.regions=rev(brewer.pal(11, \"BrBG\")))"},{"path":"getting-your-data.html","id":"cleaning-area-identifiers","chapter":"4 Getting your data","heading":"4.6 Cleaning Area Identifiers","text":"One important aspects pay attention cleaning\ngeoreferenced data area identifiers stored \ncharacter factor data types numeric types stored\nnumeric types leading 0s dropped may cause issues \nmerging multiple datasets together area-keys coded \nway (e.g. one dataset area-keys might coded numeric another dataset\narea-keys might coded character factor).can happen often FIPS codes, like 2-character state FIPS\ncodes 3-digit county FIPS code. states like Alabama Alaska FIPS\ncodes 01 02, mistakenly stored numeric values, \ntruncated 1 2 can introduce errors trying merge\nmultiple datasets using FIPS codes.county FIPS codes, tigris package handy built-reference.’ve installed tigris (e.g. run install.packages('tigris')) \nload package (library(tigris)) can access built-fips_codes\ndata.frame.Suppose, can happen, download data find due \ncoding error 5-digit combined state county FIPS codes stored\nnumeric, causing leading zeroes truncated . check FIPS\ncodes reasonably confident error leading\nleft-hand-side zeroes omitted, following correct\nmistake:code uses dplyr stringr packages, part tidyverse\nintroductions : https://dplyr.tidyverse.org/ : https://stringr.tidyverse.org/.","code":"## \n## Attaching package: 'dplyr'## The following object is masked from 'package:kableExtra':\n## \n##     group_rows## The following objects are masked from 'package:stats':\n## \n##     filter, lag## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\nhead(example_df)## # A tibble: 6 × 3\n##    FIPS Name    State\n##   <int> <chr>   <chr>\n## 1  1001 Autauga AL   \n## 2  1003 Baldwin AL   \n## 3  1005 Barbour AL   \n## 4  1007 Bibb    AL   \n## 5  1009 Blount  AL   \n## 6  1011 Bullock AL\nlibrary(stringr)\nlibrary(dplyr)\nexample_df <- example_df %>% mutate(FIPS = ifelse(\n  nchar(FIPS) == 4,\n  str_pad(\n    FIPS,\n    width = 5,\n    pad = '0',\n    side = 'left'\n  ),\n  FIPS\n))\n\nhead(example_df)## # A tibble: 6 × 3\n##   FIPS  Name    State\n##   <chr> <chr>   <chr>\n## 1 01001 Autauga AL   \n## 2 01003 Baldwin AL   \n## 3 01005 Barbour AL   \n## 4 01007 Bibb    AL   \n## 5 01009 Blount  AL   \n## 6 01011 Bullock AL"},{"path":"getting-your-data.html","id":"numerators-and-denominators","chapter":"4 Getting your data","heading":"4.7 Numerators and Denominators","text":"","code":""},{"path":"getting-your-data.html","id":"epi-primer","chapter":"4 Getting your data","heading":"4.7.1 Epi Primer","text":"presenting data disease distribution measures health disparities, following types measures often used:PrevalenceCumulative incidenceIncidence rateThese measures serve answer different types research questions:Descriptive: describes distribution outcome interestPredictive: predicts may experience outcome interestCausal: seeks determine modifiable causes outcome interestPrevalence defined \\(Pr[Y=1] = \\frac{\\text{# existing cases}}{\\text{# individuals study population point time}}\\). Prevalence dimensionless, ranges 0 1, requires time reference (.e. prevalence certain year, age, etc.). Prevalence measures useful descriptive causal questions.Cumulative incidence, also known incidence proportion, risk, attack rate (although rate), defined \\(Pr[Y=1] = \\frac{\\text{# incident cases time period}}{\\text{#individuals risk baseline}}\\). Cumulative incidence also dimensionless ranges 0 1, must state time period anyone included denominator must eligible move numerator meeting case definition. Cumulative incidence limited lack information exact timing outcome interest, time-varying exposures, competing risks, loss follow-.Incidence rate, also known incidence density hazard rate, defined \\(\\frac{\\text{# incident cases $t_0$ $t_1$}}{\\text{$\\sum$ person-time risk accumulated $t_0$ $t_1$}}\\). Person-time can expressed years, months, weeks, days. Incidence rate proportion ranges 0 \\(\\infty\\).Measures disparityRatio measuresDifference measuresAttributable fractionsRatio measures include cumulative incidence ratios (CIR), incidence rate ratios (IRR), odds ratios (). range 0 \\(\\infty\\) association indicated ratio 1.Odds ratios defined \\[\\frac{(Pr[Y=1|=1]/ Pr[Y=0|=1])}{(Pr[Y=1|=0]/ Pr[Y=0|=0])}\\]Differences measures include cumulative incidence difference (CID), also known attributable risk, incidence rate difference (IRD), also known attributable rate. range CID -1 1 range IRD \\(-\\infty\\) \\(\\infty\\). association indicated difference 0.Attributable Risk Percent (AR) excess fraction proportion disease burden among exposed associated exposure defined :\n\\[AR\\%=\\frac{CI_{Exposed} – CI_{unexposed}}{CI_{Exposed}}*100 \\]Population Attributable Risk Percent (PAR) proportion disease burden among total population associated exposure defined :\n\\[AR\\%*Pr[=1|Y=1]=\\frac{CI_{Total} – CI_{unexposed}}{CI_{Total}} \\]analytic methods section Public Health Disparities Geocoding Project monograph demonstrates measures described identified aggregated areas strata area-based socioeconomic \nsocial metrics (ABSMs).","code":""},{"path":"getting-your-data.html","id":"numeratordenominator-issues","chapter":"4 Getting your data","heading":"4.7.2 Numerator/Denominator Issues","text":"calculating population rates health outcomes, important consider avoid bias resulting mismatch numerator denominator rate. Typically, can occur data sources numerator denominator differ - e.g. calculate rate deaths resulting drug overdose particular county, data numerator rate come death certificates data denominator national census. Using different data sources necessarily cause bias. mismatch two data sources bias can result. example, previous scenario, numerator data included drug overdoses occurred particular county, denominator included residents county, bias result numerator includes individuals denominator.epidemiologic analyses health disparities, data analysts generally rely routinely collected data health surveillance systems define numerators rates. data available ongoing basis lag two years allow data compilation cleaning. contrast, data population risk may obtained variety sources depending need stratification demographic variables, time frame, desired level geography. example, US Decennial Census provides detailed population estimates age, sex-gender, selected racialized groups decennial years. intercensal years, US Census uses demographic modeling takes consideration births, deaths, migrations provide estimates demographic groups national, state, county levels via Population Estimates Program (PEP) [US Census PEP]. smaller levels geography, US Census Bureau’s American Community Survey includes estimates population census geographies larger census blocks based five year rolling averages. also typically made available two year delay.Formally, US Census Bureau recommends use PEP decennial census counts population size estimates intercensal years, recommending ACS data used information changing socioeconomic demographic features. However, estimation small area disease rates analyses health disparities ABSM intercensal years, decennial counts usually outdated PEP estimates available desired geographic level. Thus, practice, many epidemiologic studies continue rely ACS small-area estimates population denominators. private companies increasingly producing high-resolution gridded population estimates (typically total population, stratified sub-groups, whether age diverse social groups) based machine-learning models combining census, remote sensing, land use, information, promise providing population sizes small geographies near real time, date found data products provide substantial improvement using ACS small area estimates cases can induce bias (Nethery et al. 2021).numerators (case data) denominators (population estimates) come different sources, moreover, intercensal population estimates particular may subject uncertainty, possible obtain sociodemographic strata particular areas years observed cases exceed reported population risk. particularly problematic number cases greater zero reported population risk exactly zero. term phenomenon “numerator/denominator mismatch.”ensure numerator/denominator mismatch cases aggregated numerator coming population pool aggregated denominator denominator potential become cases, one must understand sources data used. therefore critical explore data dictionaries survey methodology various sources data better understand eligibility criteria gets included denominator appears numerators used aggregating data analysis. includes answering questions like:cases defined?data source include just adults ages?data source limited non-institutionalized populations?data source include solely residents jurisdiction?happens individuals address residence?happens individuals institutionalized, including incarcerated?Additionally, one exclude records geocoded, meet case definition, missing data important covariates (e.g. age, gender, race/ethnicity, etc. depending question asked). However, exclusions need clearly documented, careful thought given resulting selection bias can affect interpretation results.Numerator/denominator mismatch can also introduce bias aggregating data strata ABSMs. Understanding sources bias arising incompatibility identifying population size estimates limit bias critical part designing analytical approach. example, temporally incompatible numerator denominator data tends result greater bias race-stratified models involving numerically smaller populations, important implications studying disparities (Nethery et al, 2021).","code":""},{"path":"getting-your-data.html","id":"geocoding","chapter":"4 Getting your data","heading":"4.8 Geocoding","text":"Address geocoding, just geocoding, process converting location description (usually address) form geographic representation (usually latitude longitude coordinates). geocoding software program parse input data standard, recognizable values, compare information internal reference database points, lines, polygons, return best match values database input data. use geocoding software regularly everyday life – anytime look directions friends house, search nearby restaurants dinner, hail ride-sharing service, geocoding tool operating hood provide information need.need geocode many addresses, often need working large datasets, called ‘batch’ geocoding. Many services offer batch geocoding, varying degrees speed, accuracy, pricing. recommend reviewing multiple services deciding right team. provide table geocoding services might interested .variety geocoding services available internet ranging free expensive. table contains list available geocoding services.several important questions consider choosing geocoding services, including:free? , kind subscription packages offer?free? , kind subscription packages offer?caching locally/permanently storing geocoding results permitted?caching locally/permanently storing geocoding results permitted?using geocoding results third-party basemaps permitted?using geocoding results third-party basemaps permitted?service right store geocoding requests results transmit data third parties?service right store geocoding requests results transmit data third parties?privacy data protection policy?privacy data protection policy?order access geocoding service, interact application program interface (API). services, like ArcGIS, interfaces internal software might download computer. Others like Google, Nominatum, require programming access. utilize R packages access APIs within R environment. walk process using Google’s Geocoding API example.","code":""},{"path":"getting-your-data.html","id":"getting-your-data-in-shape","chapter":"4 Getting your data","heading":"4.8.1 Getting your Data in Shape","text":"order batch geocoding process run smoothly, need first get data want geocoded appropriate format processed. service may different preferences address data formatted, generally require address information combined single, string (text) variable. prefer commas separating different features string (Ellicott City, Maryland, 21042). using R clean data, multiple functions stringr package prove useful.preparing data, watch naming idiosyncrasies dataset may confuse geocoder lead mismatches failed attempts. Massachusetts mortality data, example, addresses often used shorthand street designation (road = RD, path = PA, CT = court). human reader, rarely problem, Google’s Geocoding API often misidentified addresses ending PA Pennsylvania, similarly identified CT Connecticut. Several “drives” (DR) struggled identify marked doctor’s offices (!). reviewing editing data running geocoding program, can avoid run twice.","code":""},{"path":"getting-your-data.html","id":"setting-up-the-service","chapter":"4 Getting your data","heading":"4.8.2 Setting up the Service","text":"Depending geocoding service use, may take several steps gain access service. Google, unlike services, requires register API key Google Cloud Console. API keys allow services track using (don’t share !), charge services. appropriate API key, can save R environment. recommended, opposed calling code, may wish share code without sharing unique key.","code":""},{"path":"getting-your-data.html","id":"geocoding-1","chapter":"4 Getting your data","heading":"4.8.3 Geocoding","text":"data set correctly, credentialed utilize geocoding service, ’s time geocode data. example code package ‘ggmap’ supports Google geocoding API. Depending quality internet service, whether service allows cache results, many items geocode, may want split geocoding smaller batches interrupted start , aren’t starting scratch.possible, request much output program possible. addition latitude longitude data, can often receive information confidence program match, precision match, additional geographic data, useful bits information. , importantly, using services cost money per request, sure save results. find mistakes later, can mark , separate main file, fix , rerun geocoding service smaller sample.","code":""},{"path":"getting-your-data.html","id":"checking-your-results","chapter":"4 Getting your data","heading":"4.8.4 Checking your Results","text":"’ve geocoded results, important check accuracy. programs can tell confidence match, can choose review matches certain threshold verify accuracy. can broadly verify results mapping points ensure things look appropriate. broad scale, like premature mortality data, may hard note small unexpected patterns sea dots, obvious issues (clusters points different state) can identified using method. can also use helpful output geocoding program. example, Google offers variable called “type” tells kind building point represents. mapping residences, may good explore addresses come back “electronics_store” “dentist”.go process, mindful source data. Addresses Massachusetts mortality data, example, recorded human beings. geocoding service struggles find match, may help verify street name spelled correctly proper designation. One check used, example, verify address Google returned zip code address submitted. Large clusters data zip code wrong may reveal common misspelling.key process nitpick every small mistake geocoding service makes, identify broad issues might seriously bias results.","code":""},{"path":"getting-your-data.html","id":"data-governance","chapter":"4 Getting your data","heading":"4.9 Data Governance","text":"much data users can account poor data absence data. Therefore, improvements availability quality health data , particular, improvements use social metrics conceptualizing analyzing data, necessary. Data governance therefore critical feature addressing health disparities. Data governance “input making decisions data required, informed tandem expertise health equity researchers members communities whose data stake, affording expertise lived experience” (Krieger, 2021). See Structural Racism, Health Inequities, Two-Edged Sword Data: Structural Problems Require Structural Solutions proposed “two-part institutional mandate regarding reporting analysis publicly-funded work involving racialized groups health data documentation proposed mandates feasible” (Krieger, 2021).Part demanding improved data governance interrogating sources data currently available, including sampling strategies, underlying theories inform design, involved generation - line ecosocial theory’s construct accountability agency. exploring origins one’s data (actual records history data systems categories issue) processes generated, data analysts proponents health justice can better articulate potential bias data, identify means improving data collection mitigate bias, actively call structural change data collection governance.Another key component equity-oriented data governance make data openly accessible, relevant safeguards data privacy use, enable communities ask data-driven questions, just rely analyses get published governmental agencies academic researchers. also requires robust infrastructure community science, transparent review methods findings, strengthened capacity community-oriented knowledge translation enhance agency. Although beyond scope technical PHDGP 2.0 training provide guidance specifics equity-oriented data governance particular projects diverse groups engaged, issues data governance, sovereignty, privacy increasingly focus critical analysis practice (Else 2022, Carroll, Rodriguez-Lonebear Martinez 2019, Committee National Statistics 2019, US Census Bureau 2020, Krieger et al. 2020).","code":""},{"path":"getting-your-data.html","id":"references","chapter":"4 Getting your data","heading":"4.10 References","text":"Krieger N. US Census People’s Health: Public Health Engagement Enslavement “Indians Taxed” Census Tracts Health Equity (1790-2018). J Public Health. 2019 Aug;109(8):1092-1100. doi: 10.2105/AJPH.2019.305017. Epub 2019 Jun 20.Nethery, Rachel C., Tamara Rushovich, Emily Peterson, Jarvis T. Chen, Pamela D. Waterman, Nancy Krieger, Lance Waller, Brent . Coull. “Comparing denominator sources real-time disease incidence modeling: American Community Survey WorldPop.” SSM-Population Health 14 (2021): 100786.Rothman, Kenneth J., Sander Greenland, Timothy L. Lash. Modern Epidemiology. Vol. 3. Philadelphia: Wolters Kluwer Health/Lippincott Williams & Wilkins, 2008.Else H. African researchers lead campaign equity global collaborations. Nature. 2022 Jun 10. doi: 10.1038/d41586-022-01604-3. Epub ahead print.Carroll SR, Rodriguez-Lonebear D, Martinez . Indigenous Data Governance: Strategies United States Native Nations. Data Sci J. 2019;18:31. doi: 10.5334/dsj-2019-031.Committee National Statistics. National Academies Sciences, Engineering, Medicine. Workshop 2020 Census Data Products: Data Needs Privacy Considerations. Washington, DC: December 11–12, 2019. Available : https://sites.nationalacademies.org/DBASSE/CNSTAT/DBASSE_196518. Accessed June 14, 2022.US Census Bureau. Understanding differential privacy. https://www.census.gov/programs-surveys/decennial-census/decade/2020/planning-management/process/disclosure-avoidance/differential-privacy.html ; accessed June 14, 2022.Krieger N, Nethery RC, Chen JT, Waterman PD, Wright E, Rushovich T, Coull BA. Impact Differential Privacy Census Tract Data Source (Decennial Census Versus American Community Survey) Monitoring Health Inequities. J Public Health. 2021 Feb;111(2):265-268. doi: 10.2105/AJPH.2020.305989. Epub 2020 Dec 22.","code":""},{"path":"visualizing-your-data.html","id":"visualizing-your-data","chapter":"5 Visualizing your data","heading":"5 Visualizing your data","text":": Christian Testa, Enjoli HallData visualization critical component communicating advocating \nhealth equity makes data accessible transparent. However, \nwithout pitfalls, chapter discuss important\npoints consider visualizing data advancing health equity.Firstly, matter accessibility, strive use colorblind friendly\ncolor palettes using color individuals one different\nkinds colorblindness can still interpret visualizations.\nencourage learn colorblindness colorblind friendly\npalettes number resources (Katsnelson, 2021) (Ou, 2021). color\nvision deficiency simulator colorblindr package especially helpful\ntesting visual creating colorblind friendly.looking help learning create data visualizations R, \nrecommend checking online, free book:\nggplot2: Elegant Graphics Data Analysis.looking help learning work spatial data R, recommend\nfollowing free, online books:Spatial Data Science applications R Edzer Pebesma Roger Bivand https://r-spatial.org/book/Geocomputation R Robin Lovelace, Jakub Nowosad, Jannes Muenchow https://geocompr.robinlovelace.net/Analyzing US Census Data: Methods, Maps, Models R, Kyle Walker https://walker-data.com/census-r/Geospatial Health Data: Modeling Visualization R-INLA Shiny Paula Moraga. https://www.paulamoraga.com/book-geospatial/index.html.One main points urge caution around respect visualization\nmapping area based health outcome data presentation rates\nunstable due small population sizes. broader principle, \nemphasize need careful choice area level results \npresented. part, well established changing \nareal units data aggregated analyzed can change \nrelationships observed data. known Modifiable Areal Unit\nProblem, can read (Wong 2004) (Buzzelli 2020). note,\nchoice area arbitrary, guided priori\nreasons, including areas make sense use answer \nquestions. example, analyses population-wide health inequities within \nentire US may wish use census tract county level data, whereas analyses\nspecifically focused political geography health inequities may\nwish use areas political boundaries, e.g., state congressional\nlegislative districts (Keena et al, 2021; Krieger, 2019; Krieger et al, 2022).example small area levels like Census tract level, underlying\npopulation may small health outcomes observed quite noisy\nsingle additional case represents large shift rate. \nsituation, potentially erroneous infer Census tract\nhighest observed rate greatest underlying risk \nartifact noise. motivates need spatial smoothing, \nencourage use spatially smoothed estimates choropleth maps showing\nhealth outcome rates avoid potential pitfall.\nintroduction pitfalls, recommend Pitfalls avoid chapter (Gimond 2022).worthwhile remark different areal units can either center \nmarginalize social groups based geographic boundaries employed. \nalso crucial clear social groups excluded inaccurately\nrepresented area-based data, including limited people \nunhoused, incarcerated, otherwise institutionalized experiencing kinds\nmarginalization. Warranting scrutiny protocols employed data\nholders assign addresses /georeferenced codes records persons\nliving non-institutional residences, well counted\ntowards population totals specified geographic areas.example,\none lesson learned analyzing Boston area mortality data \ncan quite useful know locations areas containing homeless\nshelters, found individuals designated experiencing\nhomelessness time death place death \nresidential field death certificate listed address homeless\nshelter. net impact can inflate mortality rate census\ntracts shelters located.Oftentimes georeferenced geocoded data anomalous idiosyncratic\nfeatures areas rate appears infinite population\nestimates zero area despite observed health outcomes,\nespecially data small areas particular sub-populations.\nAlthough single solution problems, key starting place\nexamine data critically understand socially-produced data\nprotocols social distribution populations issue, \nlookout real problems arise manifest social\nspatial inequities. accordingly encourage data analysts visualizers \nconsider carefully impacts choice areas, boundaries, \nprocesses locations people assigned areas (whether\n“numerators” “denominators”), power relations affecting \nlikely geocoded areas – missed – whether “case”\nmember population cases arise (.e., “denominator”).start develop wider perspective data visualization mapping\ncan used, list range recommended books articles. \ndata visualization mapping powerful tools communicate (\nmiscommunicate) data, critical aware can reflect bias\ntell lies (Deluca Nelson, 2017), (Fleckenstein 1991), (Monmonier 2021),\nwell reveal powerful truths (Koch 2017). Tom Koch outlines history \ndisease mapping book Disease Maps, Epidemics Ground (2011), \n2017 follow-book, Ethics Everyday Places: Mapping Moral Stress,\nDistress, Injury, takes broader perspective.","code":""},{"path":"visualizing-your-data.html","id":"health-equity","chapter":"5 Visualizing your data","heading":"5.1 Health Equity","text":"understand distribution health disease place, necessary collect, analyze, visualize health data area-based social metrics. equally important, however, documenting mapping health inequities, contextualize data adequate analysis social, political, ecological context. Shared observations disparities health necessarily translate common understandings cause, especially population patterns disease health mirror population distributions deprivation privilege. Mapping visualizing uneven social spatial distribution health disease areas – can particularly effective way communicating health inequities decision makers – without offering explanatory context allocation resources hazards areas can perpetuate harmful ideas actions actually undermine goal eliminating health inequities.increasing availability local health data CDC PLACES program, well advancements availability accessibility geocoding services, ample opportunity disaggregate health data, particularly neighborhood (census tract) level. Geographic disaggregation allows fine-grained analyses, including multilevel spatial modeling, can inform “targeted” interventions. presented explanatory context, granular data can create reinforce sociologist Loïc Wacquant refers “territorial stigmatization,” whereby characteristics features place associated moral character behavior residents, vice versa—especially people places already politically, economically, socially marginalized /materially deprived important resources (Chowkwanyun Reed 2020). example, places found high concentration illness disease, narratives representations places “diseased,” “contaminated,” produce reinforce existing stigma lead targeted interventions heightened policing surveillance attempt contain control residents, reclamation demolition physical structures, social neglect abandonment (many historical case studies , potential risks downplayed ignored: Craddock 2004; Molina 2006; Roberts 2009; Lopez 2009; Krupar Ehlers 2017).various approaches countering territorial stigmatization. Mapping place-based risks resource deficits might help explain spatial distribution disease, illness, injury along racial socioeconomic lines can focus public policy attention shifting context health rather individual behaviors attitudes. example, case Covid-19, look like mapping visualizing uneven geographic distribution preventive health care facilities concentration respiratory hazards areas racialized concentrated poverty. Furthermore, one map historical political variables historical redlining can offer important insight social spatial patterning life-enhancing harmful resources exists area result racist policies practices (Rothstein 2017; Mapping Inequality 2022; Krieger et al 2020a, 2020b; Wright et al. 2022). Additionally, asset mapping can provide helpful information strengths resources community facilitate discussion action around building assets address community needs improve community health.Analyzing visualizing patterns White wealth health also important understanding addressing patterns population health health inequities. Mapping visualizing “racially concentrated areas affluence” can help move research policy attention away predominant concern racially concentrated areas poverty toward holistic consideration full range health outcomes, resources, hazards area (Goetz et al. 2019). focus racially concentrated areas affluence underscores reality structural racism produces racialized concentrations poverty (hazards) racialized concentrations wealth (health-enabling resources). measures Index Concentration Extremes (ICE), quantifies distribution persons extremes relationships privilege deprivation, also bring full population power relations view, can scaled use multiple levels geography (e.g., census block, census block group, census tract, city/town, county, etc.) (Massey 2001; Krieger et al. 2015, 2016, 2017, 2018). Initially developed measure spatial polarization economic terms (.e., economic residential segregation), public health studies extended use include novel measures racialized residential segregation racialized economic segregation (Krieger et al. 2015, 2016, 2017, 2018).summary, addressing health inequities requires relational understanding systems power resource allocation simultaneously produce poor health good health others. approach may require analyzing visualizing patterns population health health inequities large geographic scales counties regions, rather city level example, capture wider range values health outcome data area-based social metrics.REFERENCESBuzzelli M. (2020) ‘Modifiable Areal Unit Problem’, International Encyclopedia Human Geography, pp. 169–173. doi:10.1016/B978-0-08-102295-5.10406-8.Chowkwanyun M Reed Jr AL. (2020). Racial health disparities Covid-19—Caution context. New England Journal Medicine, 383(3), 201-203. doi:10.1056/NEJMp2012910Craddock, S. (2004). City Plagues: Disease, Poverty, Deviance San Francisco. University Minnesota Press.Deluca E. Nelson S. (2017) ‘Lying Maps’. Available : https://open.lib.umn.edu/mapping/chapter/7-lying--maps/ (Accessed: 7 June 2022).Dorling D, Fairbairn D (1997). Mapping: Ways Representing World. Old Tappen, UK: Routledge.Fleckenstein L. (1991) ‘Maps Lie’, Syracuse University Magazine, December. Available : https://surface.syr.edu/cgi/viewcontent.cgi?article=1245&context=sumagazine.Gimond M. (2022) Intro GIS Spatial Analysis. Available : https://mgimond.github.io/Spatial/index.html (Accessed: 7 June 2022).Goetz EG, Damiano , Williams RA. Racially concentrated areas affluence: preliminary investigation. Cityscape, 21(1), 99-123.Katsnelson, . (2021) ‘Colour better: fixing figures colour blindness’, Nature, 598(7879), pp. 224–225. doi:10.1038/d41586-021-02696-z.Keena , Latner M, McGann AJM, Smith CA. Gerrymandering States: Partisanship, Race, Transformation American Federalism. Cambridge, UK: Cambridge University Press, 2021.Koch T. (2011) Disease Maps: Epidemics Ground. Chicago, IL: University Chicago Press. Available : https://press.uchicago.edu/ucp/books/book/chicago/D/bo8490164.html (Accessed: 7 June 2022).Koch T. (2017) Ethics Everyday Places: Mapping Moral Stress, Distress, Injury | Esri Press (2017). Available : https://www.esri.com/en-us/esri-press/browse/ethics--everyday-places-mapping-moral-stress-distress--injury (Accessed: 7 June 2022).Krieger N, Van Wye G, Huynh M, Waterman PD, Maduro G, Li W, Gwynn C, Barbot O, Bassett MT. Historical redlining, structural racism, preterm birth risk New York City (2013-2017). J Public Health 2020; 110(7):1046-1053.Krieger N, Wright E, Chen JT, Waterman PD, Huntley ER, Arcaya M. Cancer stage diagnosis, historical redlining, current neighborhood characteristics: breast, cervical, lung, colorectal cancer, Massachusetts, 2001-2015. J Epidemiol 2020; 189(10):1065-1075.Krieger N, Waterman PD, Spasojevic J, Li W, Maduro G, Van Wye G. Public health monitoring privilege deprivation using Index Concentration Extremes (ICE). J Public Health 2016; 106: 256-253Krieger N, Waterman PD, Gryparis , Coull BA. Black carbon exposure, socioeconomic racial/ethnic spatial polarization, Index Concentration Extremes (ICE). Health & Place 2015; 34:215-228.Krieger N, Feldman JM, Waterman PD, Chen JT, Coull BA, Hemenway D. Local residential segregation matters: stronger association census tract compared conventional city-level measures fatal non-fatal assaults (total firearm related), using Index Concentration Extremes (ICE) racial, economic, racialized economic segregation, Massachusetts (US), 1995-2010. J Urban Health 2017; 94:244-258.Krieger N, Kim R, Feldman J, Waterman PD. Using Index Concentration Extremes multiple geographic levels monitor health inequities era growing spatial social polarization: Massachusetts, USA (2010-2014). Int J Epidemiol 2018; 47:788-819.Krieger N. US Census people’s health: Public health engagement enslavement “Indians Taxed” census tracts health equity (1790-2018). J Public Health 2019; 109(8):1092-1100.Krieger N, Testa C, Chen JT, Hanage WP, McGregor AJ. Relationship political ideology US federal state elected officials key COVID pandemic outcomes vaccine era: April 2021-March 2022. Lancet – Regional Health Americas (press).Krupar S, & Ehlers N. (2017). Biofutures: Race governance health. Environment Planning D: Society Space, 35(2), 222-240. doi:10.1177/0263775816654475Lopez RP. (2009). Public health, APHA, urban renewal. American Journal Public Health, 99(9), 1603-1611. doi:10.2105/AJPH.2008.150136Lovelace R, Nowosad J, Muenchow J. (2022) Geocomputation R. Available : https://geocompr.robinlovelace.net/ (Accessed: 7 June 2022).Molina N. (2006). Fit Citizens? Public Health Race Los Angeles, 1879-1939. University California Press.Monmonier M. (2018) Lie Maps, Third Edition. Chicago, IL: University Chicago Press. Available : https://press.uchicago.edu/ucp/books/book/chicago/H/bo27400568.html (Accessed: 7 June 2022).Nelson R, Winling L, Connolly NDB, Madron J, Marciano, R. Mapping Inequality: Redlining New Deal America. https://dsl.richmond.edu/panorama/redlining/#loc=5/39.1/-94.58&text=; accessed June 17, 2022.Ou, J. (2021) Safe colorsets. Available : https://cran.r-project.org/web/packages/colorBlindness/vignettes/colorBlindness.html (Accessed: 7 June 2022).Roberts Jr SK. (2009). Infectious Fear: Politics, disease, Health Effects Segregation. University North Carolina Press.Rothstein R. (2017). Color Law: Forgotten History Government Segregated America. New York: Liveright Publishing Co., W.W. Norton & Co.Tufte D. (2001). Visual Display Quantitative Information. 2nd ed. Cheshire, CT: Graphics Press.Tufte ER. (2020). Seeing Fresh Eyes: Meaning, Space, Data, Truth. Cheshire, CT: Graphics Press.Wong D. (2004) ‘modifiable areal unit problem (MAUP)’, WorldMinds: Geographical Perspectives 100 Problems. Springer Netherlands. Available : https://link.springer.com/chapter/10.1007/978-1-4020-2352-1_93.Wright E, Waterman PD, Testa C, Chen JT, Krieger N. Breast Cancer Incidence, Hormone Receptor Status, Historical Redlining, Current Neighborhood Characteristics Massachusetts, 2005-2015. JNCI Cancer Spectrum 2022; 6(2); https://doi.org/10.1093/jncics/pkac016; epub -line: Feb 18, 2022.","code":""},{"path":"analyzing-your-data.html","id":"analyzing-your-data","chapter":"6 Analyzing your data","heading":"6 Analyzing your data","text":": Jarvis Chen, ScD","code":""},{"path":"analyzing-your-data.html","id":"overview-of-methods","chapter":"6 Analyzing your data","heading":"6.1 Overview of Methods","text":"years since initially presented Public Health Disparities Geocoding Training 2004, huge increase conceptual methodological work regarding use ABSMs document analyze health inequities, computing tools statistical methods conduct analyses. particular, availability software facilitates data access, mapping, visualization, fitting multilevel spatial models greatly enhanced accessibility analytic methods public health scientists advocates interested advancing health equity. goal presenting updated methods illustrate applicability monitoring, reporting, investigating health inequities. , highlight many assumptions underlying use methods show can applied real-life data arise public health surveillance health equity monitoring. comment pitfalls may arise working imperfect data real time discuss analytic interpretive strategies keeping mind overarching goal accurately documenting health inequities accountability. appropriate, also comment ongoing methodologic work improve analytic approaches.","code":""},{"path":"analyzing-your-data.html","id":"motivating-questions","chapter":"6 Analyzing your data","heading":"6.2 Motivating Questions","text":"choice methods data analysis depends fundamentally goals analysis, focus presentation following motivating questions:“geographic distribution area-based social metrics across study area?” particular interest exploratory analyses order understand areas affected social advantage disadvantage captured area-based social metrics. Mapping visualization tools can used communicate geographic patterns elicit local knowledge order spur hypothesis generation prioritization research questions.“geographic distribution area-based social metrics across study area?” particular interest exploratory analyses order understand areas affected social advantage disadvantage captured area-based social metrics. Mapping visualization tools can used communicate geographic patterns elicit local knowledge order spur hypothesis generation prioritization research questions.“social inequities health associated area-based social metrics across study area?” highlight importance descriptive epidemiology first step understanding inequities health populations living areas characterized area-based social metrics. , accurate description burden social inequities falls necessary pre-requisite causally-oriented investigations evaluations interventions. discuss aggregated analyses, advantages straightforward implement report using tabulation methods avoiding problems small area estimation, well non-spatial regression models. focus health equity settings understanding joint patterning inequities racialized group area-based social metric interest, comment analyses may need consider heterogeneity across age strata.“social inequities health associated area-based social metrics across study area?” highlight importance descriptive epidemiology first step understanding inequities health populations living areas characterized area-based social metrics. , accurate description burden social inequities falls necessary pre-requisite causally-oriented investigations evaluations interventions. discuss aggregated analyses, advantages straightforward implement report using tabulation methods avoiding problems small area estimation, well non-spatial regression models. focus health equity settings understanding joint patterning inequities racialized group area-based social metric interest, comment analyses may need consider heterogeneity across age strata.“can geographic variation health outcomes modeled order facilitate mapping small-area disease estimates estimation social inequities?” review concepts extensive statistical literature small-area estimation disease mapping goal visualizing geographic patterns health outcomes studies health equity. methods entail complex modeling frameworks computational details, focus application health equity interpretation use model outputs context monitoring reporting health disparities. consider settings multilevel modeling approaches spatial modeling approaches may preferred discuss considerations contributing choice modeling framework.“can geographic variation health outcomes modeled order facilitate mapping small-area disease estimates estimation social inequities?” review concepts extensive statistical literature small-area estimation disease mapping goal visualizing geographic patterns health outcomes studies health equity. methods entail complex modeling frameworks computational details, focus application health equity interpretation use model outputs context monitoring reporting health disparities. consider settings multilevel modeling approaches spatial modeling approaches may preferred discuss considerations contributing choice modeling framework.","code":""},{"path":"analyzing-your-data.html","id":"choice-of-geographic-level","chapter":"6 Analyzing your data","heading":"6.3 Choice of geographic level","text":"pragmatic question approach: level(s) data available?pragmatic question approach: level(s) data available?small area level analyses, number events support small area-level analyses?small area level analyses, number events support small area-level analyses?census tract (CT) vs. city/town, e.g. breast cancer mortality analysiscensus tract (CT) vs. city/town, e.g. breast cancer mortality analysisZIP Codes US Census defined ZIP Code Tabulation Areas (ZCTAs)ZIP Codes US Census defined ZIP Code Tabulation Areas (ZCTAs)Modifiable Areal Unit Problem1Modifiable Areal Unit Problem1note smallest level Census geography intercensal population estimates county. US Census Population Estimates Program (PEP) estimates rely demographic modeling.note smallest level Census geography intercensal population estimates county. US Census Population Estimates Program (PEP) estimates rely demographic modeling.CT level population estimates US Census American Community Survey (ACS), ACS specifically cautions using population denominator estimates small area estimation. However, generally sources population denominators rely. Five year average estimates may help, still year year variability. Margin error estimates available, incorporate denominator uncertainty analyses still area active research.CT level population estimates US Census American Community Survey (ACS), ACS specifically cautions using population denominator estimates small area estimation. However, generally sources population denominators rely. Five year average estimates may help, still year year variability. Margin error estimates available, incorporate denominator uncertainty analyses still area active research.review issues connected numerator/ denominator mismatch, please visit Section 4.7 “Getting Data” chapter.review issues connected numerator/ denominator mismatch, please visit Section 4.7 “Getting Data” chapter.","code":""},{"path":"analyzing-your-data.html","id":"example-age-specific-all-cause-mortality-by-racialized-group-in-massachusetts-2013-2017","chapter":"6 Analyzing your data","heading":"6.3.1 Example: Age-specific all-cause mortality by racialized group in Massachusetts, 2013-2017","text":"compared use aggregation method non-spatial Poisson model analyzing deaths causes age racialized group. Using aggregation method, deaths population person-time risk aggregated strata age (0-4, 5-9, 10-14, 15-19, 20-24, 25-29, 30-34, 35-44, 45-54, 55-64, 65-74, 75-84, 85+) racialized group (White non-Hispanic Black), mortality rates per 100,000 person-years computed using formulas listed Section 6.5.1. non-spatial Poisson model, inputs age- racialized-group specific deaths census tract study area, log(population person-time risk) offset, set indicators stratum age category racialized group.log(population person-time risk) variable zero, however, presents problem Poisson model-fitting algorithm rate undefined. Census tracts zero deaths zero person-time risk particular strata can deleted dataset observations contribute information. situations count deaths stratum greater zero population person-time risk zero continue issue (numerator/denominator mismatch). may tempting delete observations dataset well, reasoning usually involve just one death occurring stratum population estimate zero, deleting observations unduly affect analysis.Unfortunately, turns numerator/denominator mismatch occurs frequently Black observations dataset, due patterns racialized residential segregation fact Black population accounts much smaller proportion population compared White non-Hispanic population (10.1% vs. 72.9% Massachusetts according ACS population estimates used population denominators). , census tract level, likely age strata deaths reported zero population person-time risk Black population compared White non-Hispanic population. result, delete observations, end deleting larger proportion Black deaths across whole state, impact estimates rates rate ratios.see , consider following table show number deaths population age racialized group aggregated across census tracts. columns 2-8, show deaths, population, estimated rates per 100,000 person-years White Non-Hispanics Blacks full dataset, columns 9-15 show deaths population deleting observations numerator greater zero denominator zero. columns 16-22 show percent bias strata comparing dataset deleted observations full dataset. evident column 19, effect deleting observations reduce Black death count age strata (4-30% across strata) much greater degree among White non-Hispanic age strata (0-2%). result, age-specific incidence rate ratios can depressed much 30% relative IRRs calculated using full aggregated data.instead deleting strata deaths>0 denominator=0, replace denominator number deaths, end slightly larger population counts overall, effect rates bring line aggregated analysis. comparable table comparing full aggregated dataset (columns 2-8) dataset denominators adjusted increase person time risk equal number deaths affected strata.even better solution, one ultimately recommend, add small number (e.g. 0.001) population person-time denominators allow Poisson model fitting algorithm incorporate observations analysis. resulting effect overall population denominators negligible, allows estimation rates rate ratios interest. Note needed fitting non-spatial regression models data. aggregation method incur problem small discrepancies averaged areas, spatial models discuss Section 6.6 address problem zero infinite rates smoothing.visualize effect adjustments estimates age-specific disparity racialized group, plot estimates aggregated analyses Poisson models deletion denominator adjustment Figure 1.1. plot confirms either () adjusting denominators match number observed deaths problematic strata census tracts (b) adding small number population person time estimates (preferred) yields comparable estimates aggregation method, whereas removing problematic observations results bias.\nFigure 6.1: Comparison estimated age-specific Black/White mortality rate ratios using different methods address numerator/denominator mismatch\n","code":""},{"path":"analyzing-your-data.html","id":"aggregation-method","chapter":"6 Analyzing your data","heading":"6.4 Aggregation Method","text":"presented original Public Health Disparities Geocoding Project Monograph (Krieger et al, 2004), one straightforward ways incorporate area-based social metrics analyses health inequities use term Aggregation Method. method entails using geocodes append area-based social metrics health surveillance records, stratify records discrete categories based ABSM, aggregate numerators denominators areas, within levels defined ABSM. Rates, including age-standardized rates, measures association (e.g. rate differences rate ratios) can easily computed using tabulation methods formulae taught basic epidemiologic textbooks. analyses straightforward require specialized software: can even done Excel spreadsheet programs. One key advantages aggregation avoids problems numerator/denominator mismatch since small discrepancies case counts population risk tend get averaged aggregating numerators denominators small areas within ABSM strata.can describe inequities categories ABSMs aggregating deaths population risk census tracts share values ABSMs. analogous health departments typically reporting, example, statewide cancer mortality rates age gender.method avoids problem unstable rates arising small areas assuming cases population denominators areas similar socioeconomic characteristics can validly combined strata.following steps used generate age-standardized disease rates stratified area-based social metrics case data geocoded appropriate ABSMs generated census data.Aggregate case data numerators (age cells within areas/geocodes).Aggregate population denominator data age cells within areas/geocodes.Merge numerators denominators ABSMs, area/geocode.Aggregate areas strata defined categorical ABSM age category.Generate age-standardized rates summary measures.Advantages:easy implement using tabulation methods; can even done MS Exceleasy implement using tabulation methods; can even done MS Excelformulae taught basic epidemiologic textbooksformulae taught basic epidemiologic textbookspresentation interpretation social inequities straightforwardpresentation interpretation social inequities straightforwardaggregation avoids problems numerator-denominator mismatchaggregation avoids problems numerator-denominator mismatchage-standardization using direct method coherent interpretation even presence effect heterogeneity ageage-standardization using direct method coherent interpretation even presence effect heterogeneity ageConfidence intervals may wider particularly age strata relatively less information get upweightedConfidence intervals may wider particularly age strata relatively less information get upweightedDisadvantages:precludes identification small areas unusually high low disease ratesprecludes identification small areas unusually high low disease ratesmay preclude adjustment compositional differences area-level covariates vary within ABSM stratamay preclude adjustment compositional differences area-level covariates vary within ABSM strataThroughout section, using data Greater Boston Area years 2013-2017, drawing Massachusetts Mortality data American Community Survey (ACS) population data.","code":""},{"path":"analyzing-your-data.html","id":"direct-age-standardization","chapter":"6 Analyzing your data","heading":"6.4.1 Direct Age Standardization","text":"rate estimationconfidence intervals ratesmeasures disparity: incidence rate difference incidence rate ratiopopulation attributable fractiona comment life expectancy","code":""},{"path":"analyzing-your-data.html","id":"non-spatial-regression-methods","chapter":"6 Analyzing your data","heading":"6.5 Non-Spatial Regression Methods","text":"","code":""},{"path":"analyzing-your-data.html","id":"poisson-regression","chapter":"6 Analyzing your data","heading":"6.5.1 Poisson regression","text":"aggregation method attractive conceptual simplicity ease implementation, may prefer take regression approach analysis health inequities ABSM. describe , makes possible relax strong assumption homogenous Poisson process within category ABSM. might therefore choose model data (deaths population risk age stratum within census tracts) using Poisson loglinear model – , generalized linear model Poisson error distribution log link.Let \\(Y_{ij}\\) count deaths age-stratum \\(j\\) census tract \\(\\) \\(n_{ij}\\) corresponding person-time risk. fit Poisson log linear model data include dummy variables ABSM categories, e.g. \\(x_1\\), \\(x_2\\), \\(x_3\\), \\(x_4\\) coded 1 census tract corresponding category CT % poverty 0 otherwise. model age categories set dummy variables. also include \\(\\log(n_{ij})\\) offset model \\(\\log(\\mu_{ij})\\).\n\\[\n\\begin{aligned}\nY_{ij} & \\sim \\mbox{Poisson}(\\mu_i) \\\\\n\\log(\\mu_{ij}) & = \\beta_0 + \\beta_1 (pov=5-9.9\\%) + \\beta_2 (pov=10-19.9\\%) + \\beta_3 (pov=20-100\\%) \\\\&+\\sum_{j=2}^{J} \\alpha_j (age=age_j) + \\log(n_{ij})\n\\end{aligned}\n\\]","code":""},{"path":"analyzing-your-data.html","id":"quasi-poisson-regression","chapter":"6 Analyzing your data","heading":"6.5.2 Quasi-poisson regression","text":"key assumption Poisson distribution variance expected count equals mean,\n\\[ \\mbox{Var}(Y_{ij}) = \\mathbb{E}(Y_{ij}) = \\mu_{ij} \\]\nreal life, however, often find data empirical variance larger mean. known overdispersion.diagnose potential overdispersion, can look standardized residuals, defined \n\\[\n\\begin{align*}\nz_{}  & = \\frac{y_{} - \\hat{y}_{}}{sd(\\hat{y}_{})}  \n\\end{align*}\n\\]\nPoisson model true, \\(\\{z_{}\\}\\)s approximately independent, mean 0 standard deviation 1. overdispersion, expect \\(\\{z_{}\\}\\)s larger absolute value, reflecting extra variation beyond predicted Poisson model.can test overdispersion computing sum squares standardized\nresiduals comparing \n\\(\\chi^{2}_{n-k}\\) distribution \\(n\\) sample size \\(k\\) number parameters Poisson log linear model.can also visualize predicted counts vs. standardized residuals.\nFigure 6.2: Predicted counts vs. standardized residuals. standardized residuals mean 0 standard deviation 1 (hence dashed lines \\(\\pm2\\) indicating approximate 95% error bounds). variance standardized residuals larger 1, indicating case moderate amount overdispersion.\n","code":"> df_overdispersion <- df_forModel \n> df_overdispersion$yhat <-  predict(poisson_apINDPOV,\n+                                          newdata=df_forModel %>%\n+                                          dplyr::select(denominator, \n+                                                        apINDPOV_2, \n+                                                        apINDPOV_3, \n+                                                        apINDPOV_4,\n+                                                        apINDPOV_NA,\n+                                                        agecat), type=\"response\")\n> \n> df_overdispersion$z <- (df_overdispersion$numerator - df_overdispersion$yhat)/sqrt(df_overdispersion$yhat)\n> n.obs <- length(df_overdispersion$numerator)\n> k <- 4\n> cat(\"overdispersion ratio is \",sum(df_overdispersion$z^2, na.rm=T)/(n.obs-k),\"\\n\")\noverdispersion ratio is  2.144269 \n> cat(\"p-value of overdispersion test is \", pchisq(sum(df_overdispersion$z^2, na.rm=T), n.obs-k, lower.tail=FALSE), \"\\n\")\np-value of overdispersion test is  0 "},{"path":"analyzing-your-data.html","id":"negative-binomial-regression","chapter":"6 Analyzing your data","heading":"6.5.3 Negative binomial regression","text":"aggregation method Poisson regression coincide analysis crude ratesPoisson regression assumes variance equal mean, real-life data often exhibit overdispersion (actually complicated age-specific data, age strata may underdispersed others may overdispersed)Quasi-poisson regression estimates extra scale parameter: results wider confidence limits overdispersion. ABSM estimates identical Poisson model fit.Negative binomial model assumes different distribution allow overdispersion. Can conceived mixture Poisson distributions latent variable gamma distributedNegative binomial model yields ABSM estimates can vary Poisson Quasi-Poisson fitsHow models handle numerator/denominator mismatch? Zero numerator/zero denominator observations need deleted. Non-zero numerator/zero denominator areas present problem. delete areas, undercount cases can differential racialized group; better approach add small number (e.g., 0.1) (effect total denominators negligible, yields rates ABSM estimates similar aggregation method).","code":""},{"path":"analyzing-your-data.html","id":"comparison-of-non-spatial-regression-estimates","chapter":"6 Analyzing your data","heading":"6.5.4 Comparison of non-spatial regression estimates","text":"\nFigure 6.3: Comparison estimates social inequities ABSM aggregated method non-spatial regression models\n","code":""},{"path":"analyzing-your-data.html","id":"sae","chapter":"6 Analyzing your data","heading":"6.6 Small Area Estimation","text":"","code":""},{"path":"analyzing-your-data.html","id":"indirect-age-standardization","chapter":"6 Analyzing your data","heading":"6.6.1 Indirect Age Standardization","text":"observe \\(y_{ij}\\), count deaths area \\(\\) age stratum \\(j\\). define \\(O_{}\\) observed number deaths area \\(\\), summed age strata: \\(O_{} = \\sum_{j} y_{ij}\\). expected number deaths, \\(E_{}\\), computed applying age-specific mortality rates reference population age-specific population counts area:\n\\[ E_{} = \\sum_{j} N_{ij} \\times R_{j} \\]\n\\(N_{ij}\\) person-time risk age group \\(j\\) area \\(\\) (computed taking population age-group \\(j\\) area \\(\\times\\) number years) \\(R_{j}\\) mortality rate age group \\(j\\) suitable reference population. ratio \\(O_{}/E_{}\\) known Standardized Incidence Ratio (SIR), , case mortality outcomes, Standardized Mortality Ratio (SMR). ratio estimate within area, \\(\\theta_{}\\). rare, non-communicable diseases, standard statistical model \\(O_{}\\) Poisson distribution:\n\\[ O_{} \\sim \\mathrm{Poisson} (\\theta_{}E_{}). \\]\nmaximum likelihood estimator \\(\\theta_{}\\) \n\\[\\begin{align*}\n\\hat{\\theta}_{} = \\mathrm{SMR}_{} & = \\frac{O_{}}{E_{}}\n\\end{align*}\\]\n\\(\\mathrm{Var}(\\mathrm{SMR}_{}) = \\theta_{}/E_{}\\), estimated \\(O_{}/E_{}^2\\). Note \\(\\mathrm{Var}(\\mathrm{SMR}_{})\\) inversely proportional \\(E_{}\\). translate SMR age-standardized rate, can multiply SMR overall mortality rate reference population. examples, however, present small area estimates SMR scale, since interested pattern increased decreased relative risk across small areas.Indirect age standardization SMRs particularly useful age-specific counts deaths available area, since required age-specific population estimates area, set age-specific reference rates, total cases area. Interestingly, method also produces rate estimates smaller asymptotic variance corresponding direct standardization method (Pickle White, 1995).summary measures, SMRs also certain drawbacks. based ratio estimators, thus sensitive small changes \\(E_{}\\). particular, \\(E_{}\\) close zero, SMR large positive count. estimate \\(\\mathrm{Var}(\\mathrm{SMR}_{})\\) proportional \\(1/E_{}\\), SMRs zero distinguish variation expected counts. importantly, interpretability comparability SMRs based indirectly age standardized data across areas depends assumption independent area age effects respect standard population. known proportionality assumption, assumes \\(r_{ij} = \\theta_{} \\times \\alpha_{j}\\) \\(\\alpha_{j}\\) age effect vary areas.determine whether \\(SMR>1\\) inconsistent chance occurrence, can carry statistical hypothesis test:\n\\[\n\\begin{align*}\n\\mbox{Null hypothesis} & : \\theta_{}=1 \\\\\n\\mbox{Alternative hypothesis} & :  \\theta_{}>1\n\\end{align*}\n\\]\ntest hypothesis follows: null hypothesis, mean Poisson distribution \\(E_{}\\)\n\\[ O_{} \\sim \\mathrm{Poisson}(E_{}) \\]Calculate probability obtaining least \\(O_{}\\) cases chance Poisson distribution mean \\(E_{}\\):\n\\[\n\\begin{align*}\n\\Pr(X \\geq O_{} | \\mathrm{Exp}=E_{}) & = 1 - \\Pr(X < O_{} | \\mathrm{Exp}=E_{}) \\\\\n& = 1 - \\sum^{O_{}-1}_{X=0} \\frac{E_{}^{X} e^{-E_{}}}{X!} \\\\\n& = p\n\\end{align*}\n\\]\nprobability \\(p\\) (1-sided) p-value. \\(p \\leq 0.05\\), usually reject null hypothesis accept alternative. say excess risk disease area \\(\\) statistically significant 5% level. finding might warrant epidemiological investigation.Consider, however, 306 census tracts Greater Boston mortality example. statistical test described just comparison one census tract’s SMR relative null hypothesis. want identify census tracts study area significantly elevated rates, repeat tests many times, create multiple hypothesis testing situation. Moreover, imagine dependence tests nearby areas neighboring areas shared risk factor increased SMR clusters census tracts.overcome variability, hierarchical models can used “smooth” raw rates. faced problem making inferences many parameters \\(\\{\\theta_{}\\}=\\theta_{1}, \\ldots, \\theta_{n}\\), measured \\(n\\) areas, one can imagine two possible extreme assumptions:assume \\(\\{\\theta_{}\\}\\) identical, case data can pooled, individual units ignored. typically presenting summary rates rate ratios whole study area.assume \\(\\{\\theta_{}\\}\\) identical, case data can pooled, individual units ignored. typically presenting summary rates rate ratios whole study area.extreme, assume \\(\\{\\theta_{}\\}\\) independent entirely unrelated. case, SMR area estimated independent data areas. saw SMR example , leads statistical instability numbers small.extreme, assume \\(\\{\\theta_{}\\}\\) independent entirely unrelated. case, SMR area estimated independent data areas. saw SMR example , leads statistical instability numbers small.third possible assumption lies somewhere two extremes. One assume \\(\\{\\theta_{}\\}\\) “similar” sense area labels convey additional information. known exchangeability, equivalent assuming \\(\\{\\theta_{}\\}\\) drawn common prior distribution unknown parameters.","code":""},{"path":"analyzing-your-data.html","id":"poisson-gamma-model","chapter":"6 Analyzing your data","heading":"6.6.2 Poisson gamma model","text":"classic example approach presented Clayton Kaldor (1987), developed Bayesian analysis Poisson likelihood model. model useful introduction idea hierarchical modelling disease rates, second stage distribution area variability analytically tractable helps build intuition smoothing works stabilize SMR estimates.first stage hierarchy, assume observed death counts area Poisson distributed:\n\\[ O_{} \\sim \\mathrm{Poisson}(\\theta_{} E_{}). \\]\nsecond stage, hierarchical prior placed \\(\\theta_{}\\):\\[ \\theta_{} \\sim \\mathrm{Gamma}(\\nu, \\alpha) .  \\]\nRecalling gamma distribution parameters \\(\\nu\\) \\(\\alpha\\) mean \\(\\nu/\\alpha\\), simply states expect distribution \\(\\{ \\theta_{}\\}\\) follow gamma distribution mean \\(\\nu/\\alpha\\) variance \\(\\nu/\\alpha^{2}\\). Since gamma distribution conjugate prior Poisson, posterior distribution \\(p(\\theta_{} | O_{},E_{})\\) also follows gamma distribution:\n\\[\n\\mathrm{Gamma}(\\nu + O_{}, \\alpha + E_{})\n\\]\nmean given \n\\[\n\\mathbb{E}(\\theta_{}|O_{},\\nu, \\alpha) = \\frac{\\nu+O_{}}{\\alpha + E_{}} = w_{}\\mbox{SMR}_{} + (1-w_{}) \\frac{\\nu}{\\alpha}\n\\]\n\n\\[w_{} = \\frac{E_{}}{\\alpha+E_{}}. \\]\nexpression \\(\\mathbb{E}(\\theta_{}|O_{},\\nu,\\alpha)\\) shows posterior mean relative risk \\(\\)th area weighted average observed SMR \\(\\)th area average relative risk (\\(\\nu/\\alpha\\)) areas. weight inversely proportional variance SMR. Accordingly, \\(E_{}\\) small (rare diseases small population counts), variance large, weight \\(w_{}\\) small posterior mean dominated prior mean, \\(\\nu/\\alpha\\). areas abundant data, posterior mean close observed \\(SMR_i = O_{}/E_{}\\). feature, whereby amount smoothing proportional amount information available particular area, known precision weighting. intuitive appeal , one observe lot information area (sample size small risk estimate unstable), one’s “best guess” concerning area’s mortality risk weighted towards little known prior knowledge, .e. risk , average, \\(\\nu/\\alpha\\). contrast, one observes lot information area (e.g. sample size large), one likely believe data say mortality risk particular area, thus “best guess” reasonably weighted towards observed SMR specific area.Empirical Bayes approach developed Clayton Kaldor (1987), \\(\\nu\\) \\(\\alpha\\) replaced estimates, \\(\\hat{\\nu}\\) \\(\\hat{\\alpha}\\), can calculated means iterative procedure using following two equations:\\[\n\\begin{align*}\n\\frac{\\hat{\\nu}}{\\hat{\\alpha}} & = \\frac{1}{n} \\sum_{}^{n} \\frac{O_i + \\hat{\\nu}}{E_i + \\hat{\\alpha}} = \\frac{1}{n} \\sum_i \\hat{\\theta}_i \\\\\n\\frac{\\hat{\\nu}}{\\hat{\\alpha}^2} & = \\frac{1}{n-1} \\sum_{}^{n} \\left(1 + \\frac{\\hat{\\alpha}}{E_i} \\right) \\left(\\hat{\\theta}_i - \\frac{\\hat{\\nu}}{\\hat{\\alpha}}\\right)^2\n\\end{align*}\n\\]\n\\(\\{\\hat{\\theta}_i\\}\\) empirical Bayes estimates. Together, two equations can used recursively compute \\(\\hat{\\nu}\\) \\(\\hat{\\alpha}\\). stage iteration, \\(\\{\\hat{\\theta}_i\\}\\) calculated current estimates \\(\\nu\\) \\(\\alpha\\), right hand sides two equations used provide new estimates \\(\\nu\\) \\(\\alpha\\) (Clayton Kaldor, 1987).interesting connection negative binomial regression marginal posterior distribution \\(O_i\\) (unconditional \\(\\theta_i\\)) negative binomial size \\(\\nu\\) probability \\(\\alpha/(E_i + \\alpha)\\).can apply algorithm observed expected death counts census tracts study area obtain empirical Bayes estimates census tract level SMRs, compare raw \\(SMR_i = O_i/E_i\\):caterpillar plots, observe empirial Bayes estimates substantially smoothed relative raw SMRs. observations extremes distribution huge confidence intervals smoothed towards mean, spread smoothed SMRs generally narrower raw SMRs.\nFigure 6.4: Caterpillar plots raw SMRs 95% CIs smoothed SMRs Poisson gamma model 95% credible intervals\nalso evident compare maps raw SMRs empirical Bayes SMRs, extreme low high SMRs smoothed towards middle values.\nFigure 6.5: Maps raw SMRs (left) smoothed SMRs Poisson gamma model\n","code":"\n# Use DCluster::empbaysmooth to fit the Poisson Gamma model as proposed by Clayton and Kaldor (1987)\npoisson_gamma <- DCluster::empbaysmooth(df_indirect_ordered$O, df_indirect_ordered$E)\n\n# Append these estimates to the dataset and also calculate naive empirical Bayes credible intervals\n# using the gamma distribution.\ndf_eb <- df_indirect_ordered %>%\n         mutate(nu1 = O + poisson_gamma$nu,\n                alpha1 = E + poisson_gamma$alpha,\n                empbayes_SMR = poisson_gamma$smthrr,\n                empbayes_CI95low = qgamma(0.025, nu1, alpha1),\n                empbayes_CI95up = qgamma(0.975, nu1, alpha1),\n                eb_sig = factor(case_when(\n                  empbayes_CI95low>1 & empbayes_CI95up>1 ~ 1,\n                  empbayes_CI95low<1 & empbayes_CI95up<1 ~ -1,\n                  TRUE ~ 0)),\n                raw_SMR = O/E, # recenter rawSMRs by adding intercept from intercept only model\n                raw_SMR_CI95low = pois.exact(x=O, pt=E, conf.level=0.95)[,4],\n                raw_SMR_CI95up = pois.exact(x=O, pt=E, conf.level=0.94)[,5],\n                raw_sig = factor(case_when(\n                  raw_SMR_CI95low>1 & raw_SMR_CI95up>1 ~ 1,\n                  raw_SMR_CI95low<1 & raw_SMR_CI95up<1 ~ -1,\n                  TRUE ~ 0))) %>%\n          mutate(empbayes_SMR = ifelse(empbayes_SMR==0, nu1/alpha1, empbayes_SMR))\n\n# caterpillar plot of the raw SMRs\nrawSMR_caterpillar_eb <- ggplot(df_eb %>% arrange(raw_SMR) %>% mutate(id_order = row_number()),\n       aes(x=id_order, y=raw_SMR, ymin=raw_SMR_CI95low, ymax=raw_SMR_CI95up, col=raw_sig)) +\n  geom_point() +\n  geom_errorbar() +\n  geom_hline(yintercept = 1, col=\"red\", linetype=\"dotted\") +\n  ylim(c(0,6)) +\n  labs(x=\"\", y=\"raw SMR\") +\n  scale_color_manual(name=\"\",\n                       values=c(\"#018571\", \"grey50\", \"#A6611A\"),\n                       labels=c(\"CI below 1\",\n                                \"CI crosses 1\",\n                                \"CI above 1\")) +\n  theme(legend.position=\"bottom\")\n\n# caterpillar plot of the empirical Bayes SMRs\nebSMR_caterpillar_eb <- ggplot(df_eb %>% arrange(empbayes_SMR) %>% mutate(id_order = row_number()),\n       aes(x=id_order, y=empbayes_SMR, ymin=empbayes_CI95low, ymax=empbayes_CI95up, col=eb_sig)) +\n  geom_point() +\n  geom_errorbar() +\n  geom_hline(yintercept = 1, col=\"red\", linetype=\"dotted\") +\n  ylim(c(0,6)) +\n  labs(x=\"\", y=\"empirical Bayes SMR\") +\n  scale_color_manual(name=\"\",\n                       values=c(\"#018571\", \"grey50\", \"#A6611A\"),\n                       labels=c(\"CI below 1\",\n                                \"CI crosses 1\",\n                                \"CI above 1\")) +\n  theme(legend.position=\"bottom\")\n\nrawSMR_caterpillar_eb + ebSMR_caterpillar_eb"},{"path":"analyzing-your-data.html","id":"poisson-log-normal-model","chapter":"6 Analyzing your data","heading":"6.6.3 Poisson log normal model","text":"gamma prior \\(\\{\\theta_{}\\}\\) mathematically convenient, can restrictive difficult incorporate observed covariates model. flexible model makes use normal prior \\(\\log(\\theta_{})\\) (Wakefield et al., 2000; Lawson et al., 2003):\n\\[\n\\begin{align*}\nO_{} & \\sim \\mathrm{Poisson}(\\theta_{} E_{}) \\\\\n\\log(\\theta_{}) & = \\alpha + v_{} \\\\\nv_{} & \\sim \\mathrm{Normal}(0, \\sigma^{2}_{v})\n\\end{align*}\n\\]\n\\(\\alpha\\) intercept term representing overall log relative risk disease whole study region compared reference rate \\(v_{}\\) residual log relative risk area \\(\\) compared average study region. , \\(\\{v_{}\\}\\) assumed arise normal distribution mean 0 variance \\(\\sigma^{2}_{v}\\). may recognize model generalized linear mixed model, “mixed” refers fact model contains “random effects” (\\(\\{v_{}\\}\\)), well accommodating “fixed” covariate effects, e.g.\n\\[\n\\log(\\theta_{}) = \\alpha + \\beta_{1} x_{1} + \\ldots + \\beta_{p} x_{p} + v_{} . \\]\ncase \\(\\{v_{}\\}\\) interpretable *residual area-specific effects conditional fixed covariates. Similarly, \\(\\beta\\)s interpretable covariate effects conditional area random effects.Fixed vs Random Effects: standard regression models, fixed random effects refer type statistical model used. using fixed effects analysis variance (ANOVA), assumptions made independent variable error distributions variable. Fixed effects estimated maximum likelihood (traditional beta estimates see regression models). appropriate trying generalize results values fixed variables used study (fixed variables “assumed measured without error … assumed values fixed variable one study values fixed variable another study”) (Newsom, 2019). however, researcher seeking make inferences beyond particular values independent variable, random effects model used. Random effects estimated shrinkage (partial pooling linear unbiased predictions). Random effects models accounting “additional expected random variation independent variable” instead value variable interest, random variables “assumed values drawn larger population values thus represent ” (Newsom, 2019; Gelman, 2005)noted , pointed Wolpert Ickstadt (1998), Poisson log normal model aggregate consistently. , one specifies log normal distribution relative risks combines two areas specifies log normal distribution relative risk combined area, distributions inconsistent (sum log normal distributions log normal). can understood form aggregation bias whereby risk relationships remain constant across levels aggregation (Wakefield et al., 2000). Nevertheless, normal second-stage distribution observed empirically provide good model log relative risks range aggregations, present advantages respect model flexibility ease computation.Unlike Poisson gamma model, analytically tractable closed-form solution posterior distributions. Instead, use INLA fit model.can see caterpillar plots raw SMRs vs. smoothed SMRs Poisson lognormal model , similar Poisson gamma model, Poisson lognormal model smooths extreme SMRs towards overall mean.\nFigure 6.6: Caterpillar plots raw SMRs 95% CIs smoothed SMRs Poisson lognormal model 95% credible intervals\n\nFigure 6.7: Maps raw SMRs (left) smoothed SMRs Poisson lognormal model\nestimate SMR area \\(\\) relative reference mortality rate (indirect age standardization) \\(\\exp(\\alpha + v_{})\\), \\(\\exp(v_{})\\) residual relative risk area \\(\\) relative mean study area. Note internal set reference rates indirect age standardization used, .e. based observed rates whole study area , \\(\\alpha\\) generally close zero.","code":"# Use INLA to fit Poisson gamma model\nmodel_form <- O ~ 1 + f(id_order, model=\"iid\")\nmodel_iid <- inla(model_form, family=\"poisson\", \n                data=df_indirect_ordered, E=E, # E points to the expected count field\n                control.predictor=list(compute=TRUE), # computes transformed posterior marginals\n                control.compute=list(dic=TRUE, waic=TRUE)) # computes DIC for model fit\nsummary(model_iid)"},{"path":"analyzing-your-data.html","id":"poisson-multilevel-model","chapter":"6 Analyzing your data","heading":"6.6.4 Poisson multilevel model","text":"Poisson gamma Poisson log normal models, smoothing global: random effects treated exchangeable come common distribution. result, area specific relative risks shrunk overall mean. kind global smoothing allow spatial correlation risks nearby areas, might expected local clustering spatial pattern risks. Local clustering risks may due risk factors shared within across census tracts: individuals share spatially varying risk factors expected similar outcomes. local clustering exists, violation assumption exchangeability. risk factors known observed, straightforward solution include covariates model. However, observational settings, one can rarely measure, even know , relevant risk factors. one suspects risk factors vary area, necessary consider ways allow local clustering models \\(\\{\\theta_{}\\}\\).order accommodate local smoothing, can specify Poisson multilevel model random effects census tract neighborhood levels. Greater Boston example, treat neighborhoods Boston roughly equivalent level city/town designation communities outside Boston make greater Boston study area.\\[\n\\begin{align*}\nO_{ij} & \\sim \\mathrm{Poisson}(\\theta_{ij} E_{ij}) \\\\\n\\log(\\theta_{ij}) & = \\alpha + v_{ij} + \\eta_j \\\\\nv_{ij} & \\sim \\mathrm{Normal}(0, \\sigma^{2}_{v})\\\\\n\\eta_{j} & \\sim \\mathrm{Normal}(0, \\sigma^{2}_{\\eta})\n\\end{align*}\n\\]can INLA using following code:\nFigure 6.8: Maps CT raw SMRs (left) smoothed SMRs Poisson multilevel model\n\nFigure 6.9: Map neighborhood level variation relative risk (Level 2 Poisson multilevel model)\n","code":"\nmodel_form <- O ~ 1 + f(id_order, model=\"iid\") + f(cityid, model=\"iid\")\nmodel_multi <- inla(model_form, family=\"poisson\", \n                data=df_indirect_ordered, E=E, # E points to the expected count field\n                control.predictor=list(compute=TRUE), # computes transformed posterior marginals\n                control.compute=list(dic=TRUE, waic=TRUE)) # computes DIC for model fit\nsummary(model_multi)"},{"path":"analyzing-your-data.html","id":"poisson-bym-model","chapter":"6 Analyzing your data","heading":"6.6.5 Poisson BYM model","text":"multilevel Poisson log normal model independent census tracts neighborhoods, \nneighborhood units specified priori. sense, neighborhood boundaries fixed, crossing boundary means one’s relative risk may jump quite lot. one wants account spatial correlation smoother manner? One way specify multivariate normal prior distribution area parameters spatially-structured covariance matrix. Many different ways proposed specify spatially structured multivariate normal distributions \\(\\log (\\theta_{})\\) (Wakefield et al., 2000; Banerjee et al., 2004).One particular form multivariate normal distribution commonly used intrinsic Gaussian conditional autoregressive (CAR) prior suggested Clayton Kaldor (1987) developed Besag et al. (1991). one popular ways dealing spatial autocorrelation. spatial structure formulated set conditional autoregressions, uses fact vector random variables multivariate normal distribution, distribution element vector conditional elements vector also normal, mean variance depend original multivariate mean covariance matrix.models, first stage model assumes observed counts Poisson distributed, additive model \\(\\log(\\theta_{})\\) can specified accommodating covariate effects:\n\\[\\begin{align*}\nO_{} & \\sim \\mathrm{Poisson}(\\theta_{} E_{}) \\\\\n\\log (\\theta_{}) & = \\alpha + u_{}\n\\end{align*}\\]\nInstead independent normal prior distribution census tract effects, one models\n\\[\nu_{}|u_{j, j \\neq } \\sim \\mathrm{Normal}(\\mu_{},\\tau^{2}_{u}/m_{})\n\\]\n\n\\[ \\mu_{} = \\frac{\\sum_{j} w_{ij} u_{j}}{\\sum_{j} w_{ij}},\n\\qquad \\sigma^{2}_{} = \\frac{\\tau^{2}_{u}}{\\sum_{j} w_{ij}} \\]\n\n\\[\nw_{ij} =\n\\begin{cases}\n1& \\text{$, j$ adjacent},\\\\\n0 & \\text{}\n\\end{cases}\n\\]\\(m_{}\\) number adjacent areas. understand , consider red highlighted census tract \\(\\) Figure 6.10. \\(\\partial_{}\\) set areas adjacent \\(\\) (shaded yellow figure), one sets \\(w_{ij}\\) 1 areas \\(j \\\\partial_{}\\) zero otherwise, prior\ndistribution \\(u_{}\\) conditional mean equal average neighbouring \\(u_{j}\\)’s variance inversely proportional number adjacent neighbours. effect smooth \\(u_{}\\) toward mean risk set neighbouring areas. Note \\(\\tau^{2}_{u}\\) variance (scaled \\(\\sum_{j \\\\partial_{}} {w_{ij}} = m_{}\\), .e. number neighbours); emphasize interpretable conditionally, labeled \\(\\tau^{2}_{u}\\) instead \\(\\sigma^{2}_{u}\\).\nFigure 6.10: Illustration CAR idea\nBesag, Yorke Mollié (1991) recommend combining CAR prior standard normal prior allow \nspatially unstructured latent covariates\nspatially correlated latent covariates. model, called BYM model convolution model literature, thus\\[\n\\begin{align*}\nO_{} & \\sim \\mbox{Poisson}(\\theta_{} E_{}) \\\\\n\\mbox{log}(\\theta_{}) & = \\beta_0 + u_{} + v_{} \\\\\nv_{} & \\sim \\mbox{Normal}(0,\\sigma^{2}_{v}) \\\\\nu_{}|u_{j, j \\neq } &  \\sim \\mbox{Normal}(\\mu_{},\\sigma^{2}_{u}/m_{})\n\\end{align*}\n\\], unstructured \\(v_{}\\) can thought capturing correlation within areas \nspatially structured \\(u_{}\\) capture spatial correlation across areas.Note, however, \\(\\sigma^{2}_{v}\\) (unstructured heterogeneity variance) \\(\\tau^{2}_{u}\\) (spatial variance) directly comparable: \\(\\sigma^{2}_{v}\\) reflects variability unstructured random effects areas, \\(\\tau^{2}_{u}\\) variance spatial effect area \\(\\), conditional values neighboring spatial effects. closed-form expression available -area variance spatial effects. However, Bayesian approach, marginal spatial variance \\(s^{2}_{u}\\) can estimated empirically posterior samples \\(\\{u_{}\\}\\):\n\\[ s^{2}_{u} = \\sum_{} (u_{}-\\bar{u})^2 / (n-1) \\]\n\\(\\bar{u}\\) average \\(\\{u_{}\\}\\).estimate marginal -area variance, one can also characterize relative contribution spatial vs. unstructured heterogeneity:\n\\[\n\\mathrm{frac_{spatial}} = s^{2}_{u} / (s^{2}_{u} + \\sigma^{2}_{v}) \\]\n\\(\\mathrm{frac_{spatial}}\\) close 1, spatial heterogeneity dominates. \\(\\mathrm{frac_{spatial}}\\) close 0, unstructured heterogeneity dominates (Best et al. 2005).aid interpretation variability posterior estimates \\(SMR_i = \\exp(u_i + v_i)\\), define quantity call QR90 (Best et al. 2005)\n\\[ QR_{90} = \\mbox{exp}(q_{95\\%} - q_{5\\%}) = \\mbox{ relative risk top bottom 5\\% areas}\n\\]\n\n\\[\n\\begin{align*}\nq_{5\\%} & = \\mbox{ log relative risk area ranked 5th percentile} \\\\\nq_{95\\%} & = \\mbox{ log relative risk area ranked 95th percentile}\n\\end{align*}\n\\]\nadvantage QR90 expresses variability SMRs relative risk scale, facilitates comparison disparities areas estimated inequities (also relative risk scale) attributable categories ABSMs.\nFigure 6.11: Depiction QR90: relative risk comparing 95th 5th quantiles random effects distribution\n\nFigure 6.12: Caterpillar plots raw SMRs 95% CIs smoothed SMRs Poisson BYM model 95% credible intervals\n\nFigure 6.13: Comparison premature mortality raw SMRs smoothed (BYM) SMRs\n","code":""},{"path":"analyzing-your-data.html","id":"estimating-absm-effects","chapter":"6 Analyzing your data","heading":"6.7 Estimating ABSM effects","text":"","code":""},{"path":"analyzing-your-data.html","id":"premature-mortality","chapter":"6 Analyzing your data","heading":"6.7.1 Premature mortality","text":"\nFigure 6.14: ABSMs vs. smoothed SMRs (BYM)\n\nFigure 6.15: Comparison premature mortality inequities ABSM categories across analytic methods models\n\nTable 6.1: Comparison estimated inequities premature mortality category CT ABSMs aggregated analysis Quasipoisson, Negative Binomial, BYM models\n\nFigure 6.16: Premature mortality: comparison smoothed SMRs (BYM) adjustment CT % poverty\n\nFigure 6.17: Premature mortality: comparison smoothed SMRs (BYM) adjustment CT Index Concentration Extremes (racialized economic segregation)\n\nFigure 6.18: Premature mortality: comparison smoothed SMRs (BYM) adjustment CT Social Vulnerability Index\n","code":""},{"path":"analyzing-your-data.html","id":"lung-cancer-mortality","chapter":"6 Analyzing your data","heading":"6.7.2 Lung cancer mortality","text":"\nFigure 6.19: Lung cancer mortality: CT ABSMs vs. smoothed SMRs (BYM)\n\nFigure 6.20: Comparison lung cancer mortality inequities CT ABSM categories across analytic methods models\n\nTable 6.2: Comparison estimated inequities lung cancer mortality category CT ABSMs aggregated analysis Quasipoisson, Negative Binomial, BYM models\n\nFigure 6.21: Lung cancer: comparison smoothed SMRs (BYM) adjustment CT % poverty\n\nFigure 6.22: Lung cancer: comparison smoothed SMRs (BYM) adjustment CT Index Concentration Extremes (racialized economic segregation)\n\nFigure 6.23: Lung cancer: comparison smoothed SMRs (BYM) adjustment CT Social Vulnerability Index\n","code":""},{"path":"analyzing-your-data.html","id":"intersectional-analysis-of-inequities-by-racialized-group-and-ct-absms","chapter":"6 Analyzing your data","heading":"6.8 Intersectional analysis of inequities by racialized group and CT ABSMs","text":"section, explore intersectional patterns inequities premature mortality racialized group CT ABSMs census tracts comprising Greater Boston study area. , focus joint patterning inequities membership racialized groups (captured individual-level membership racialized groups) residence census tracts characterized area-based social metrics indicative racialized societal inequities. accomplish , operationalize intersectional effects using methods analyzing statistical interactions (Knol VanderWeele 2012; Jackson et al. 2016; VanderWeele 2015). contrast analyses focus racial/ethnic disparities “controlling” area-based social metrics , conversely, disparities ABSM “controlling” racialized group membership, framework emphasizes importance describing joint patterning racialized social inequities.Drawing recommendations reporting interactions (Knol VanderWeele 2012), propose health disparities researchers interested reporting interactions three different ways:highlighting inequities racialized group within categories CT ABSMshighlighting inequities racialized group within categories CT ABSMshighlighting inequities CT ABSM within racialized groupshighlighting inequities CT ABSM within racialized groupshighlighting joint inequities racialized group CT ABSM relative common reference grouphighlighting joint inequities racialized group CT ABSM relative common reference groupMoreover, thorough reporting interaction analyses present measures effect measure modification additive multiplicative scales.","code":""},{"path":"analyzing-your-data.html","id":"aggregated-analysis","chapter":"6 Analyzing your data","heading":"6.8.1 Aggregated analysis","text":"begin computing age-standardized premature mortality rates racialized group CT ABSMs. Similar computed age-standardized rates CT ABSMs alone, aggregate age-specific death counts population person-time racialized group across census tracts within categories CT ABSM apply direct age-standardization. present theses estimated age-standardized premature mortality rates 95% confidence intervals Figure 6.24.Several patterns apparent visualization age-standardized rates. Firstly, gradients inequities CT ABSM apparent three ABSMs, premature mortality rates higher disadvantaged census tracts. gradient pronounced among non-Hispanic White population, steeper gradients tighter confidence limits. Among Black population, inequities across CT ABSM categories shallow, wider confidence limits advantaged categories reflecting smaller population sizes Black individuals living census tracts.Focusing Black/White inequities across CT ABSM categories, suggestion qualitative interaction. , Black/White inequities largest advantaged census tracts, disadvantaged census tracts, non-Hispanic Whites higher rates premature mortality Black populations living areas, particular quintile CT ICE SVI.\nFigure 6.24: Age-standardized premature mortality rates racialized group CT ABSM, Greater Boston, 2013-2017, computed aggregated method\nevaluate magnitude CT ABSM inequities within racialized group, present () age-standardized rate differences per 100,000 person-years (b) age-standardized incidence rate ratios Figure 6.25. additive (incidence rate difference) multiplicative scales (incidence rate ratio), ABSM gradients much steeper among non-Hispanic White population compared Black population. ABSM gradients among Black population attenuated towards null much wider confidence limits.\nFigure 6.25: ABSM gradients premature mortality racialized group (Blacks vs. Non-Hispanic Whites): age-standardized rate differences per 100,000 person-years (top row) rate ratios (bottom row) computed using aggregation method, Greater Boston, 2013-2017. plot, reference group advantaged racialized group-specific ABSM category.\nMeanwhile, Black/White inequities depicted Figure 6.26 confirm pattern Black/White crossover suggested Figure 6.24, non-Hispanic White premature mortality rates significantly elevated disadvantaged census tracts CT Index Concentration Extremes CT Social Vulnerability Index.\nFigure 6.26: Black/White inequities premature mortality category CT ABSM: age-standardized rate differences per 100,000 person-years (top row) rate ratios (bottom row) computed using aggregation method, Greater Boston, 2013-2017.\nvisualize joint pattern inequities additive (incidence rate difference) multiplicative (incidence rate ratio) scales relative common reference group Figure 6.27. general, gives insights similar visualization age-standardized incidence rates racialized group CT ABSM Figure 6.24 .\nFigure 6.27: Intersectional age-standardized incidence rate differences per 100,000 person-years (top row) age-standardized rate ratios (bottom row) racialized group category CT ABSM computed using aggregation method, Greater Boston, 2013-2017. Presented common reference group (non-Hispanic Whites advantaged categories CT ABSMs\nABSM gradients among Black population attenuated towards null much wider confidence limits. think might case?","code":""},{"path":"analyzing-your-data.html","id":"intersectional-inequities-as-estimated-by-non-spatial-multilevel-and-spatial-regression-models","chapter":"6 Analyzing your data","heading":"6.8.2 Intersectional inequities as estimated by non-spatial, multilevel, and spatial regression models","text":"Intersectional inequities racialized group CT ABSM can also estimated using regression models.\nFigure 6.28: Comparison inequities premature mortality racialized group CT ABSM estimated aggregated method, Poisson regression robust variance estimator, multilevel Poisson regression (census tracts city/town/neighborhoods), spatial Poisson regression BYM prior, Greater Boston, 2013-2017. , focus estimated incidence rate ratios common reference group (non-Hispanic Whites advantaged census tracts)\n\nTable 6.3: Comparison intersectional inequities premature mortality \nracialized group category CT ABSMs aggregated analysis, Poisson robust variance estimator, multilevel, BYM models. Note common reference group (non-Hispanic Whites advantaged CTs)\n\nFigure 6.29: Comparison CT ABSM inequities premature mortality within racialized group (Black Non-Hispanic White) estimated aggregated method, Poisson regression robust variance estimator, multilevel Poisson regression (census tracts city/town/neighborhoods), spatial Poisson regression BYM prior, Greater Boston, 2013-2017. plot, reference group advantaged ABSM category within racialized group.\nseen Figure 6.29, inequities premature mortality across CT ABSM categories much pronounced non-Hispanic White population, whereas gradient age-adjusted incidence rate ratios CT ABSM categories within Black population much shallower.\nFigure 6.30: Comparison Black vs. non-Hispanic White inequities premature mortality within category CT ABSM estimated aggregated method, Poisson regression robust variance estimator, multilevel Poisson regression (census tracts city/town/neighborhoods), spatial Poisson regression BYM prior, Greater Boston, 2013-2017\nseen Figure 6.30, Black-White inequities measured incidence rate ratios largest advantaged census tracts, whereas inequities racialized group smaller even reversed disadvantaged census tracts. , Non-Hispanic Whites living disadvantaged census tracts higher observed age-adjusted premature mortality rates Black populations living areas. think might case?Across methods, note confidence limits age-standardized rates, incidence rate differences, incidence rate ratios much wider aggregated method compared model-based estimates. reflects impact age-standardization, whereby age strata relatively less information may nevertheless contribute strongly age-standardized estimates age strata highly weighted standard population. result standard errors confidence limits aggregated method usually larger corresponding model-based estimates. Thus, aggregated method, commonly used, less power detect statistically significant inequities.","code":""},{"path":"analyzing-your-data.html","id":"references-1","chapter":"6 Analyzing your data","heading":"6.9 REFERENCES","text":"Banerjee , Dhillon , Ghosh J, Merugu S. information theoretic analysis maximum likelihood mixture estimation exponential families. Proceedings twenty-first international conference Machine learning 2004 Jul 4 (p. 8). https://dl.acm.org/doi/10.1145/1015330.1015431 Accessed June 4, 2022Besag J, York J, Mollié . Bayesian image restoration, two applications spatial statistics. Annals institute statistical mathematics. 1991 Mar;43(1):1-20.Best N, Richardson S, Thomson . comparison Bayesian spatial models disease mapping. Statistical methods medical research. 2005 Feb;14(1):35-59.Clayton D, Kaldor J. Empirical Bayes estimates age-standardized relative risks use disease mapping. Biometrics. 1987 Sep 1:671-81.Gelman, . don’t use term “fixed random effects.” Post Statistical Modeling, Causal Inference, Social Science. 01,25,2005. Available : https://statmodeling.stat.columbia.edu/2005/01/25/why_i_dont_use/Jackson JW, Williams DR, VanderWeele TJ. Disparities intersection marginalized groups. Soc Psychiatry Psychiatr Epidemiol. 2016 Oct;51(10):1349-1359. doi: 10.1007/s00127-016-1276-6. Epub 2016 Aug 16. PMID: 27531592; PMCID: PMC5350011.Knol MJ, VanderWeele TJ. Recommendations presenting analyses effect modification interaction. Int J Epidemiol. 2012 Apr;41(2):514-20. doi: 10.1093/ije/dyr218. Epub 2012 Jan 9. PMID: 22253321; PMCID: PMC3324457.Krieger N, Waterman PD, Chen JT, Rehkopf DH, Subramanian SV. Public Health Disparities Geocoding Project Monograph. Available June 30, 2004 : http://www.hsph.harvard.edu/thegeocodingprojectLawson AB, Browne WJ, Rodeiro CL. Disease mapping WinBUGS MLwiN. John Wiley & Sons; 2003 Sep 12.Nethery RC, Rushovich T, Peterson E, Chen JT, Waterman PD, Krieger N, Waller L, Coull BA. Comparing denominator sources real-time disease incidence modeling: American Community Survey WorldPop. SSM-Population Health. 2021 Jun 1;14:100786.Newsom, J. Distinguishing Random Fixed: Variables, Effects, Coefficients. Psy 526 Lecture Material: Multilevel Regression. Spring 2019. Available : http://www.web.pdx.edu/~newsomj/mlrclass/ho_randfixd.pdfPickle LW, White AA. Effects choice age‐adjustment method maps death rates. Statistics medicine. 1995 Mar 15;14(5‐7):615-27.United States Census Bureau. Population Housing Estimate Program (PEP). Updated 2022. Available : https://www.census.gov/programs-surveys/popest.htmlVanderWeele TJ. Explanation Causal Inference: Methods Mediation Interaction. New York, NY: Oxford University Press, 2015.Wakefield JC, Best NG, Waller L. Bayesian approaches diseasemapping, : Elliott, P., Wakefield, J.C., Best, N.G. Briggs, D.J. SpatialEpidemiology: Methods Applications, Oxford University Press, London, UK.pp 104–127, 2000.Wolpert RL, Ickstadt K. Poisson/gamma random field models spatial statistics. Biometrika. 1998 Jun 1;85(2):251-67.","code":""},{"path":"premature-mortality.html","id":"premature-mortality","chapter":"7 Case Study 1: Premature Mortality in Massachusetts (2013 - 2017)","heading":"7 Case Study 1: Premature Mortality in Massachusetts (2013 - 2017)","text":": Justin W. Morgan","code":""},{"path":"premature-mortality.html","id":"introduction","chapter":"7 Case Study 1: Premature Mortality in Massachusetts (2013 - 2017)","heading":"7.1 Introduction","text":"case study, outcome interest premature mortality (defined death occuring age 65). data, requested Massachusetts Registry Vital Records Statistics years 2013-2017 merged area-based social metrics (ABSMs) created American Community Survey (ACS) 5-Year Estimates. information ACA estimates metrics survey, please visit website. Vital Statistics registries require data residential address time death, can useful tools monitoring health health equity. data geocoded using Google Maps API.","code":""},{"path":"premature-mortality.html","id":"motivation-research-questions-and-learning-objectives","chapter":"7 Case Study 1: Premature Mortality in Massachusetts (2013 - 2017)","heading":"7.2 Motivation, Research Questions, and Learning Objectives","text":"goal case study develop familiarity methods exploring visualizing racial disparities health data. specific goals :Download merge health outcome ABSM dataVisualize map estimates ABSMs premature mortalityIdentify relationships racialized group, ABSMs, premature mortalityModel interaction effects racialized group ABSM.research questions seek answer throughout case study include:overall socioeconomic gradient premature mortality?racialized disparity premature mortality?ABSM interact individual level membership racialized groups? (.e., interactions socioeconomic position racialized groups, just socioeconomic inequities within racialized groups)","code":""},{"path":"premature-mortality.html","id":"downloading-and-wrangling-your-data","chapter":"7 Case Study 1: Premature Mortality in Massachusetts (2013 - 2017)","heading":"7.3 Downloading and Wrangling Your Data","text":"section, show download ACS data querying census API manipulate data format need rest analysis. case study investigates constructed variable: Index Concentration Extremes (high-income Non-Hispanic White people vs low-income people Color). See can replicate analysis different ACS variable!Note: outcome data shared due privacy restrictions - case study, receive pre-wrangled mortality dataset. Example code wrangling done included chapter’s appendix.","code":""},{"path":"premature-mortality.html","id":"dependencies","chapter":"7 Case Study 1: Premature Mortality in Massachusetts (2013 - 2017)","heading":"7.3.1 Dependencies","text":"need packages throughout case study:","code":"\n# \"mission critical\" packages\nlibrary(tidyverse)      # A collection of packages used for tidy data wrangling\nlibrary(readxl)         # A package for easy loading of Excel files\nlibrary(ggplot2)        # The most popular and flexible visualization package in R\nlibrary(tidycensus)     # A package to download data from the U.S. Census Bureau API\nlibrary(tigris)         # A package to download shapefiles from the U.S. Census Bureau\nlibrary(sf)             # A package with tools for simple (spatial) features\nlibrary(INLA)           # A package that allows for bayesian inference\ninla.setOption(inla.mode = \"experimental\") # An option to use the experimental (beta version)\nlibrary(spdep)          # A package to allow for spatial weighting in analyses\n# \"nice-to-have\" packages\nlibrary(cowplot)        # A ggplot2 add-on that will allow us to add sub plots\noptions(tigris_use_cache = TRUE) # An option within `tigris` to save what you download\nlibrary(viridis)        # A package including color-blind friendly color pallettes\nlibrary(Hmisc)          # A package containing useful functions for data analyses\nlibrary(fastDummies)    # A package that includes functions to create indicator variables\nlibrary(mapview)        # For interactive mapping, including topographical mapping\nlibrary(purrr)          # A programming toolkit for R\nlibrary(scales)         # An add-on package to help with scaling our maps appropriately\nlibrary(broom)          # A package that allows for extraction and wrangling of model output\n# If this code does not run for you, you may need to run install.package(\"package_name\")"},{"path":"premature-mortality.html","id":"your-health-outcome-data---premature-mortality","chapter":"7 Case Study 1: Premature Mortality in Massachusetts (2013 - 2017)","heading":"7.3.2 Your Health Outcome Data - Premature Mortality","text":"data aggregated individual observations death counts year, age, sex, racialized group, census tract, town. receive unrestricted mortality files government agencies research, likely encounter files one observation per death. geocoded individual-level observations, need aggregate geographic level interest analysis. data use also aggregated groups correspond common variables ACS, can aligned population counts used denominators rates calculated. Aggregation specific age groups racial categorizations allows us perform key aspects analysis, age-standardization data, stratification analysis racialized group.NOTE: variable town refers constructed variable. Ideally performing analysis might include two levels, smaller level (, census tracts) nested entirely within larger level (towns). Massachusetts though, several towns small populations smaller census tracts. towns, created crosswalk wherein neighboring small towns combined create larger “super towns”. super towns make one census tract, super town analysis now least one census tract “nested within ” multilevel analysis. interested attempting multilevel analysis, data may make good practice!","code":"\nma_mort_ct <- readRDS(\"your_file_path/ma_mort_ct.RDS\")"},{"path":"premature-mortality.html","id":"your-denominator-data-and-absm","chapter":"7 Case Study 1: Premature Mortality in Massachusetts (2013 - 2017)","heading":"7.3.3 Your Denominator Data and ABSM","text":"download population data (use denominators calculate mortality rates) ABSMs U.S. Census Bureau API using tidycensus package. requires registering Bureau API key. key redacted , can get Bureau. ABSM use analysis Index Concentration Extremes Racialized Economic Segregation (ICE), quantifies persons specified area concentrated top vs bottom specified societal distribution. distribution construct comparing high-income Non-Hispanic White people low-income people Color.","code":"\n# Population Denominators\nma_denominator <- vector(mode = \"list\", length = 5)     # Create an empty list to store our data in\nnames(ma_denominator) <- c(2013,2014,2015,2016,2017)    # Name the indices of the list for the data years\n# The following code creates a loop, and then runs the tidycensus package get_acs() function to draw down population counts for a list of variables (using their Census Bureau designations). You can see the full list of variables at https://api.census.gov/data/2019/acs/acs5/variables.html.\nfor (nm in names(ma_denominator)) {                   \n  ma_denominator[[nm]] <- get_acs(geography = \"tract\",\n                            variables = c(\"B01001_003\",\"B01001_004\",\"B01001_005\",\"B01001_006\",\n                                          \"B01001_007\",\"B01001_008\",\"B01001_009\",\"B01001_010\",\n                                          \"B01001_011\",\"B01001_012\",\"B01001_013\",\"B01001_014\",\n                                          \"B01001_015\",\"B01001_016\",\"B01001_017\",\"B01001_018\",\n                                          \"B01001_019\",\"B01001_027\",\"B01001_028\",\"B01001_029\",\n                                          \"B01001_030\",\"B01001_031\",\"B01001_032\",\"B01001_033\",\n                                          \"B01001_034\",\"B01001_035\",\"B01001_036\",\"B01001_037\",\n                                          \"B01001_038\",\"B01001_039\",\"B01001_040\",\"B01001_041\",\n                                          \"B01001_042\",\"B01001_043\",\n                                          \"B01001B_003\",\"B01001B_004\",\"B01001B_005\",\n                                          \"B01001B_006\",\"B01001B_007\",\"B01001B_008\",\n                                          \"B01001B_009\",\"B01001B_010\",\"B01001B_011\",\n                                          \"B01001B_012\",\"B01001B_013\",\"B01001B_018\",\n                                          \"B01001B_019\",\"B01001B_020\",\"B01001B_021\",\n                                          \"B01001B_022\",\"B01001B_023\",\"B01001B_024\",\n                                          \"B01001B_025\",\"B01001B_026\",\"B01001B_027\",\n                                          \"B01001B_028\",\n                                          \"B01001H_003\",\"B01001H_004\",\"B01001H_005\",\n                                          \"B01001H_006\",\"B01001H_007\",\"B01001H_008\",\n                                          \"B01001H_009\",\"B01001H_010\",\"B01001H_011\",\n                                          \"B01001H_012\",\"B01001H_013\",\"B01001H_018\",\n                                          \"B01001H_019\",\"B01001H_020\",\"B01001H_021\",\n                                          \"B01001H_022\",\"B01001H_023\",\"B01001H_024\",\n                                          \"B01001H_025\",\"B01001H_026\",\"B01001H_027\",\n                                          \"B01001H_028\"), \n                               year = as.numeric(nm) + 2,\n                               output = \"wide\",\n                               state = \"MA\",\n                               geometry = FALSE, # you can opt to download spatial information here\n                               key = \"your_key_here\",\n                               moe_level = 95,\n                               survey = \"acs5\") %>%\n    rowwise() %>%\n    # Here, we will transform those variables into variables that align better with our outcome data\n    mutate(year = as.numeric(nm),\n           GEOID10 = GEOID,\n           # Total Male Age Groups\n           `total_male_00-04` = B01001_003E,\n           `total_male_05-09` = B01001_004E,\n           `total_male_10-14` = B01001_005E,\n           `total_male_15-19` = B01001_006E + B01001_007E,\n           `total_male_20-24` = B01001_008E + B01001_009E + B01001_010E,\n           `total_male_25-29` = B01001_011E,\n           `total_male_30-34` = B01001_012E,\n           `total_male_35-39` = B01001_013E,\n           `total_male_40-44` = B01001_014E,\n           `total_male_45-49` = B01001_015E,\n           `total_male_50-54` = B01001_016E,\n           `total_male_55-59` = B01001_017E,\n           `total_male_60-64` = B01001_018E + B01001_019E,\n           total_male_total  = sum(c_across(starts_with(\"total_male\"))),\n           # Total Female Age Groups\n           `total_female_00-04` = B01001_027E,\n           `total_female_05-09` = B01001_028E,\n           `total_female_10-14` = B01001_029E,\n           `total_female_15-19` = B01001_030E + B01001_031E,\n           `total_female_20-24` = B01001_032E + B01001_033E + B01001_034E,\n           `total_female_25-29` = B01001_035E,\n           `total_female_30-34` = B01001_036E,\n           `total_female_35-39` = B01001_037E,\n           `total_female_40-44` = B01001_038E,\n           `total_female_45-49` = B01001_039E,\n           `total_female_50-54` = B01001_040E,\n           `total_female_55-59` = B01001_041E,\n           `total_female_60-64` = B01001_042E + B01001_043E,\n           total_female_total  = sum(c_across(starts_with(\"total_female\"))),\n           total_total_total   = total_male_total + total_female_total,\n           # Black Male Age Groups\n           `black_male_00-04` = B01001B_003E,\n           `black_male_05-09` = B01001B_004E,\n           `black_male_10-14` = B01001B_005E,\n           `black_male_15-19` = B01001B_006E + B01001B_007E,\n           `black_male_20-24` = B01001B_008E,\n           `black_male_25-29` = B01001B_009E,\n           `black_male_30-34` = B01001B_010E,\n           `black_male_35-44` = B01001B_011E,\n           `black_male_45-54` = B01001B_012E,\n           `black_male_55-64` = B01001B_013E,\n           black_male_total  = sum(c_across(starts_with(\"black_male\"))),\n           # Black Female Age Groups\n           `black_female_00-04` = B01001B_018E,\n           `black_female_05-09` = B01001B_019E,\n           `black_female_10-14` = B01001B_020E,\n           `black_female_15-19` = B01001B_021E + B01001B_022E,\n           `black_female_20-24` = B01001B_023E,\n           `black_female_25-29` = B01001B_024E,\n           `black_female_30-34` = B01001B_025E,\n           `black_female_35-44` = B01001B_026E,\n           `black_female_45-54` = B01001B_027E,\n           `black_female_55-64` = B01001B_028E,\n           black_female_total = sum(c_across(starts_with(\"black_female\"))),\n           black_total_total = black_male_total + black_female_total,\n           # Non-Hispanic White Male Age Groups\n           `white_male_00-04` = B01001H_003E,\n           `white_male_05-09` = B01001H_004E,\n           `white_male_10-14` = B01001H_005E,\n           `white_male_15-19` = B01001H_006E + B01001H_007E,\n           `white_male_20-24` = B01001H_008E,\n           `white_male_25-29` = B01001H_009E,\n           `white_male_30-34` = B01001H_010E,\n           `white_male_35-44` = B01001H_011E,\n           `white_male_45-54` = B01001H_012E,\n           `white_male_55-64` = B01001H_013E,\n           white_male_total  = sum(c_across(starts_with(\"white_male\"))),\n           # Non-Hispanic White Female Age Groups\n           `white_female_00-04` = B01001H_018E,\n           `white_female_05-09` = B01001H_019E,\n           `white_female_10-14` = B01001H_020E,\n           `white_female_15-19` = B01001H_021E + B01001H_022E,\n           `white_female_20-24` = B01001H_023E,\n           `white_female_25-29` = B01001H_024E,\n           `white_female_30-34` = B01001H_025E,\n           `white_female_35-44` = B01001H_026E,\n           `white_female_45-54` = B01001H_027E,\n           `white_female_55-64` = B01001H_028E,\n           white_female_total = sum(c_across(starts_with(\"white_female\"))),\n           white_total_total = white_male_total + white_female_total) %>%\n    ungroup() %>%\n    select(-starts_with(\"B0\")) %>%\n    pivot_longer(cols = c(starts_with(\"total\"),\n                          starts_with(\"black\"),\n                          starts_with(\"white\")),\n                 names_to  = c(\"race_group\", \"sex\", \"age_cat\"),\n                 names_sep = \"_\",\n                 values_to = \"population\") %>%\n    mutate(age_cat   = case_when(age_cat == \"00-04\" ~ \"0-4\",\n                                 age_cat == \"05-09\" ~ \"5-9\",\n                                 age_cat == \"total\" ~ \"Total\",\n                                 TRUE               ~ age_cat),\n           sex       = str_to_title(sex),\n           race_group = case_when(race_group == \"white\"  ~ \"Non-Hispanic White\",\n                                  race_group == \"black\"  ~ \"Black\",\n                                  race_group == \"total\"  ~ \"Total\"))\n}           \n# Combining all 5 years into one dataset\nma_denominator <- rbind(ma_denominator[[1]],\n                     ma_denominator[[2]],\n                     ma_denominator[[3]],\n                     ma_denominator[[4]],\n                     ma_denominator[[5]]) %>%\n  select(year, GEOID10, GEO_NAME = NAME, everything(), -GEOID)\n# saveRDS(ma_denominator, file = \"ma_denominator.RDS\")\n# Area Based Social Metric\nma_absm <- vector(mode = \"list\", length = 5)\nnames(ma_absm) <- c(2013,2014,2015,2016,2017)\nfor (nm in names(ma_absm)) {\n  ma_absm[[nm]] <- get_acs(geography = \"tract\",\n                           variables = c(\"B01003_001E\",\n                                         \"B19001_001E\",\n                                         \"B19001_002E\",\"B19001_003E\",\"B19001_004E\",\"B19001_005E\",\n                                         \"B19001_014E\",\"B19001_015E\",\"B19001_016E\",\"B19001_017E\",\n                                         \"B19001H_002E\",\"B19001H_003E\",\"B19001H_004E\",\"B19001H_005E\",\n                                         \"B19001H_014E\",\"B19001H_015E\",\"B19001H_016E\",\"B19001H_017E\"), \n                           year = as.numeric(nm) + 2,\n                           output = \"wide\",\n                           state = \"MA\",\n                           geometry = FALSE,\n                           key = \"4407a63721e192545e1e2a2fc7f6920477b10108\",\n                           moe_level = 95,\n                           survey = \"acs5\",\n                           cache_table = TRUE) %>%\n    # Transforming ACS variables into the ABSM's we want to use for our dataset\n    mutate(GEOID10 = GEOID,\n           pop_total = B01003_001E,\n           ICEwnhinc = ((B19001H_014E + B19001H_015E + B19001H_016E + B19001H_017E) - \n                        (B19001_002E + B19001_003E + B19001_004E + B19001_005E - \n                         B19001H_002E - B19001H_003E - B19001H_004E - B19001H_005E))/B19001_001E,           year = as.numeric(nm)) %>%\n    select(GEOID10, pop_total, ICEwnhinc, year)\n}\nma_absm <- rbind(ma_absm[[1]],\n                 ma_absm[[2]],\n                 ma_absm[[3]],\n                 ma_absm[[4]],\n                 ma_absm[[5]]) %>%\n  select(year, GEOID10, everything())\n# Weighting the ICE measure across all 5 years for visualization and analysis \nma_absm_sum <- ma_absm %>% \n  group_by(GEOID10) %>% \n  mutate(wt = pop_total / sum(pop_total, na.rm = TRUE)) %>% \n  summarise(ICEwnhinc        = mean(ICEwnhinc, wt = wt, na.rm = TRUE),\n            pop_total        = sum(pop_total, na.rm = TRUE)) %>% \n  mutate(ICE_qt=cut(ICEwnhinc, wtd.quantile(ICEwnhinc, weights=pop_total,\n                                                probs=c(0,0.2,0.4,0.6,0.8,1)),\n                        na.rm=TRUE))\n# saveRDS(ma_absm_sum, file = \"ma_absm_sum.RDS\")"},{"path":"premature-mortality.html","id":"approach","chapter":"7 Case Study 1: Premature Mortality in Massachusetts (2013 - 2017)","heading":"7.4 Approach","text":"Now data, let’s revisit questions interest:overall socioeconomic gradient premature mortality?racialized disparity premature mortality?ABSM interact individual level membership racialized groups? (.e., interactions socioeconomic position racialized groups, just socioeconomic inequities within racialized groups)Let’s first visualize overall socioeconomic gradient premature mortality.","code":""},{"path":"premature-mortality.html","id":"what-is-the-overall-socioeconomic-gradient-in-premature-mortality","chapter":"7 Case Study 1: Premature Mortality in Massachusetts (2013 - 2017)","heading":"7.4.1 What is the overall socioeconomic gradient in premature mortality?","text":"try see ABSM might impact premature mortality, let us first assess ICE measure distributed across space Massachusetts. can visualize using map. order map data, need add geometries dataset. import census tract shapefile produced government Massachusetts.code adds census tract geometries ICE data, creates map. several lines, referring different customization map. ggplot2 many options, can learn hereThis map showing us ICE measure distributed across state Massachusetts. particular iteration ICE compares high-income Non-Hispanic White people low-income people Color. ICE low, neighborhoods concentrated low-income people Color. 0, extreme concentration (, alternatively, polarization balanced). ICE higher, high concentration high-income Non-Hispanic White people.Next, visualize premature mortality data, using spatial models. order , first utilize indirect method age-standardize data. data age-standardized can calculate raw standardized mortality ratios (SMRs).ratios susceptible much discussed previous chapters regards infinity-magnitude rates, smaller sample sizes inducing extreme results. going eventually smooth data model, concerned state raw data. can see raw standardized mortality ratios extend zero infinity.SMR shows us ratio mortality given census tract compared expected census tract age_specific rates standard population. SMR greater 1 means area premature mortality higher expected. SMR less one means opposite.can also map relationship:can fit spatial model using INLA package “smooth” results fitting data Besag York Mollié (BYM) model. mentioned previous chapters, BYM model adjust estimates based surrounding areas - instead assuming neighboring geographies independent, model assume similar. order implement model R, first need create adjacency matrix, package knows census tracts neighborsNote code provides code calculate percentage variance data spatially correlated - case, 45%.can now visualize smoothed SMR. First, let’s look similar caterpillar plot raw SMR. plot looks slightly flatter, notably, infinity zero values pulled towards middle plot.map, can see color extreme values lighten.can see plot map still variation spatially smoothed SMR - bit less extreme areas raw SMR. get sense premature mortality varies respect ICE, look model adjusts ICE data see changes variation.Comparing map adjusted ICE measure smoothed map reveals concentration polarized racial-economic populations contribute impact. clearly seen less pronounced SMRs (directions) adjusting ICE areas southern Boston.","code":"\ntract_geometry <- st_read(\"CENSUS2010_BLK_BG_TRCT_SHP\",\n                       layer = \"CENSUS2010TRACTS_POLY\") %>% \n  mutate(id_order = row_number()) %>% \n  select(GEOID10, id_order)\nmap.ice <- ma_absm_sum %>% \n  left_join(tract_geometry, by = \"GEOID10\") %>% \n  ggplot(aes(geometry = geometry, fill=ICE_qt)) +\n    geom_sf(col=\"black\", size=0.1) +\n    labs(fill=\"Census Tract ICE\", \n         x=\"\", y=\"\", \n         title=expression(atop(\"Census Tract Level Index of Concentration at the Extremes\",\n                             \"Massachusetts, 5-Year ACS files from end-years 2015-2019\"))) +\n    scale_fill_brewer(palette=\"Oranges\", direction=-1, na.value=\"grey50\") +\n    theme_void() +\n    theme(axis.text.x=element_blank(), #remove x axis labels\n          axis.ticks.x=element_blank(), #remove x axis ticks\n          axis.text.y=element_blank(),  #remove y axis labels\n          axis.ticks.y=element_blank(),\n          legend.position = c(0.25, 0.25),\n          legend.key.size = unit(0.4, \"cm\"))\n# ggsave(\"your_file_path/map.ice.png\")\n# Calculate reference rates by age for MA overall\n# This will give us overall totals, and remove the race specific information we arent interested in right now\nma_total_pop <- ma_denominator %>%\n                filter(race_group == \"Total\") \n# This will align the mortality counts with those denominators so we can create total population expected death rates for each age group\nreference_rates <- ma_mort_ct %>%\n  filter(race_group == \"Total\",\n         sex != \"Total\",\n         age_cat != \"Total\") %>% \n  group_by(GEOID10, year, sex, age_cat) %>%\n  summarise(deaths=n()) %>%\n  left_join(ma_total_pop,\n             by=c(\"GEOID10\", \"year\", \"sex\", \"age_cat\")) %>%\n  mutate(age_cat = case_when(age_cat %in% c(\"35-39\",\"40-44\") ~ \"35-44\",\n                             age_cat %in% c(\"45-49\",\"50-54\") ~ \"45-54\",\n                             age_cat %in% c(\"55-59\",\"60-64\") ~ \"55-64\",\n                             TRUE ~ age_cat),\n         deaths = ifelse(is.na(deaths), 0, deaths)) %>%\n  group_by(age_cat) %>%\n  summarise(num = sum(deaths),\n            den = sum(population)) %>%\n  mutate(ref_rate = num/den) %>%\n  select(age_cat, ref_rate)\nma_mort_istd <- ma_mort_ct %>%\n  filter(race_group != \"Total\",\n         sex != \"Total\",\n         age_cat != \"Total\") %>% \n  mutate(race_group = ifelse(race_group == \"Non-Hispanic Black\", \"Black\", race_group)) %>% \n  left_join(ma_denominator, by = c(\"GEOID10\",\"year\",\"age_cat\",\"race_group\",\"sex\")) %>% \n  mutate(age_cat = case_when(age_cat %in% c(\"35-39\",\"40-44\") ~ \"35-44\",\n                             age_cat %in% c(\"45-49\",\"50-54\") ~ \"45-54\",\n                             age_cat %in% c(\"55-59\",\"60-64\") ~ \"55-64\",\n                             TRUE ~ age_cat)) %>%\n  group_by(GEOID10, super_town, age_cat) %>% \n  summarise(deaths = sum(deaths, na.rm = TRUE),\n            population = sum(population, na.rm = TRUE))%>% \n  right_join(reference_rates, by=\"age_cat\") %>%\n  mutate(expected = ref_rate*population) %>%\n  group_by(GEOID10, super_town) %>%\n  summarise(O = sum(deaths),\n            E = sum(expected),\n            raw_smr = O/E,\n            var_raw_smr = O/E^2,\n            raw_smr_lo95 = raw_smr - 1.96*sqrt(var_raw_smr),\n            raw_smr_up95 = raw_smr + 1.96*sqrt(var_raw_smr)) %>%\n  left_join(ma_absm_sum, by = \"GEOID10\") %>% \n  mutate(id = row_number()) %>%\n  dummy_cols(select_columns = \"ICE_qt\") %>%\n  rename(ICE_qt_1  = \"ICE_qt_(-0.689,0.0607]\",\n         ICE_qt_2  = \"ICE_qt_(0.0607,0.245]\", \n         ICE_qt_3  = \"ICE_qt_(0.245,0.362]\",  \n         ICE_qt_4  = \"ICE_qt_(0.362,0.465]\",  \n         ICE_qt_5  = \"ICE_qt_(0.465,0.727]\") \nplot.raw_smr <-  ma_mort_istd %>%\n  arrange(raw_smr) %>%\n  mutate(orderID = row_number()) %>% \n  ggplot(aes(x=orderID, y=raw_smr)) + \n    geom_errorbar(aes(ymin = raw_smr_lo95, ymax=raw_smr_up95), size = 0.1) +\n    geom_point(color = \"limegreen\", alpha = 0.8, size = 0.5) +\n    geom_hline(yintercept = 1, col=\"red\", linetype=\"dotted\") +\n    ylim(0,4) +\n    labs(title = \"Raw Standardized Mortality Ratio by Census Tract, with error\",\n         caption = expression(atop(\"Source: Massachusetts Mortality Data 2013-2017\", \n                                 \"5-Year ACS files from end-years 2015-2019\"))) +\n    xlab(\"Census Tract\") +\n    ylab(\"SMR\") +\n    theme_minimal()\n# ggsave(\"your_file_path/plot.raw_smr.png\")\n# Here you will see code for two maps. This block of code creates one state level map, and then a smaller, Boston area map to accompany it. This optional subsetting provides detail in an area of interest. The cowplot package combines the maps to display both in one space\nmap.raw_smr_state <- ma_mort_istd %>% \n  left_join(tract_geometry, by= \"GEOID10\") %>% \n  ggplot() +\n  geom_sf(mapping = aes(geometry=geometry, \n                        fill=raw_smr),\n          lwd = 0.1) +\n    scale_fill_distiller(palette = \"BrBG\",\n                         trans = scales::pseudo_log_trans(sigma=0.01),\n                         limits = exp(c(-1,1)*log(4)),\n                         breaks = c(0.25,0.5,1,2,4), oob=squish) +\n    labs(title = \"Raw Standardized Mortality Ratios (SMR)\",\n         caption = expression(atop(\"Source: Massachusetts Mortality Data 2013-2017\", \n                                   \"5-Year ACS files from end-years 2015-2019\")),\n         fill = \"SMR\", x=\"\", y=\"\") +\n    theme_void() +\n    theme(axis.text.x=element_blank(), #remove x axis labels\n          axis.ticks.x=element_blank(), #remove x axis ticks\n          axis.text.y=element_blank(),  #remove y axis labels\n          axis.ticks.y=element_blank(),\n          legend.position = c(0.25, 0.25),\n          legend.key.size = unit(0.4, \"cm\"))\nmap.raw_smr_boston <- ma_mort_istd %>% \n  left_join(tract_geometry, by= \"GEOID10\") %>% \n  filter(super_town %in% c(\"boston\",\"brookline\",\"chelsea\",\"cambridge\",\n                           \"everett\",\"hull\",\"malden\",\"medford\",\"newton\",\n                           \"somerville\",\"winthrop\",\"watertown\")) %>% \n  ggplot() +\n    geom_sf(mapping = aes(geometry = geometry,\n                          fill = raw_smr),\n            lwd = 0.01) +\n    scale_fill_distiller(palette = \"BrBG\",\n                         trans = scales::pseudo_log_trans(sigma=0.01),\n                         limits = exp(c(-1,1)*log(4)),\n                         breaks = c(0.25,0.5,1,2,4), oob=squish) +\n    labs(title = \"Boston\") +\n    theme_void() +\n    theme(strip.text.x = element_blank(),\n          legend.position = \"None\",\n          plot.title      = element_text(hjust = 0.5)) \nmap.raw_smr <- ggdraw() +\n  draw_plot(map.raw_smr_state , x = 0.00, y = 0.00, width = 0.80, height = 1.00) +\n  draw_plot(map.raw_smr_boston, x = 0.65, y = 0.50, width = 0.30, height = 0.30)\n# ggsave(\"your_file_path/map.raw_smr.png\")\n# We first want to make sure that our data file has our geometries in the same order as our shapefile - in other words, ensure that they are aligned. \nma_mort_istd_ordered <- ma_mort_istd %>% \n  mutate(intercept = 1) %>%\n  right_join(tract_geometry, by=\"GEOID10\")%>% \n  arrange(id_order) %>%\n  select(-geometry)\nn.tracts <- ma_mort_istd_ordered %>% \n  select(GEOID10) %>% \n  unique() %>% \n  nrow()\n# THis code will calculate the adjacency matrix \nW.nb <- poly2nb(tract_geometry , snap=0.001)\nW.list <- nb2listw(W.nb, style=\"B\", zero.policy = TRUE)\n# And this code will convert that matrix into a format INLA can understand\nnb2INLA(\"INLA_adj_mat\", W.nb) # this saves a file in the working directory\nINLA_adj_mat <- \"INLA_adj_mat\"\n# Intercept only (\"null\") BYM model \nmodel_form_0 <- O ~ 1 + f(id_order, model=\"bym2\", graph=INLA_adj_mat, scale.model=TRUE, constr=TRUE)\nmodel_0 <- inla(model_form_0, family=\"poisson\", \n                data=ma_mort_istd_ordered, E=E, # E points to the expected count field\n                control.predictor=list(compute=TRUE), # computes transformed posterior marginals\n                control.compute=list(dic=TRUE)) # computes DIC for model fit\n# Extract posterior (meaning, after the model is accounted for) means of the area effects\nnull_random <- model_0$summary.random$id_order$mean[1:n.tracts]\n# If we are interested in calculating the percentage of the variation that is spatially correlated, we have to do so empirically - if not, you can skip the next few lines. \nmat.marg <- matrix(NA, nrow=n.tracts, ncol=100000) #create empty matrix\nm <- model_0$marginals.random$id_order\nfor (i in 1:n.tracts){\n  #the first block of the random effects matrix contains area-specific effects (u + v), \n  #and the second block contains spatially structured residuals (u). \n  #So this is extracting from the second block of rows\n  u <- m[[n.tracts+i]] \n  mat.marg[i,] <- inla.rmarginal(100000, u) #randomly pick 100000 values from posterior distributions of area-specific spatially structures residuals\n}\n#Get empirical variance from 100000 obs\nvar.u <- apply(mat.marg, 2, var) \n#Get unstructured variance\nvar.v <- inla.rmarginal(100000,\n                        inla.tmarginal(function(x) 1/x, model_0$marginals.hyperpar$`Precision for id_order`))\n# Calculate spatially structured variance percentage\nperc.var.u <- mean(var.u/(var.u+var.v))\n# In addition to computing the spatially structured variance, you can also compute a QR90, comparing the 95th to the 5th quantile of the random effects distribution\nqr90 <- exp(diff(quantile(null_random, probs=c(0.05, 0.95))))\n# We will append the estimated effects back onto our dataset so that we can plot and map them.\nma_mort_istd_smoothed <- data.frame(data.frame(smooth_smr = null_random,\n                                               smooth_smr_lo95 = model_0$summary.random$id_order$`0.025quant`[1:n.tracts],\n                                               smooth_smr_up95 = model_0$summary.random$id_order$`0.975quan`[1:n.tracts])) %>%\n  mutate(id_order = row_number()) %>%\n  right_join(ma_mort_istd_ordered, by=\"id_order\") %>% \n  mutate(raw_smr = log(O/E)+0.046)\nplot.smr_smooth <- ma_mort_istd_smoothed %>%\n  ungroup() %>% \n  arrange(smooth_smr) %>% \n  mutate(orderID = row_number()) %>% \n  ggplot(aes(x=orderID, y=exp(smooth_smr))) + \n    geom_errorbar(aes(ymin = exp(smooth_smr_lo95), ymax=exp(smooth_smr_up95)), size = 0.1) +\n    geom_point(color = \"limegreen\", alpha = 0.8, size = 0.5) +\n    geom_hline(yintercept = 1, col=\"red\", linetype=\"dotted\") +\n    ylim(0,4) +\n    labs(title = \"Smoothed Standardized Mortality Ratio by Census Tract\",\n         caption = \"Source: Massachusetts Mortality Data 2013-2017, 5-Year ACS files from end-years 2015-2019\") +\n    xlab(\"Census Tract\") +\n    ylab(\"Smoothed SMR\") +\n    theme_minimal()\nplot.comp_smr <- plot_grid(plot.raw_smr, plot.smr_smooth,\n                           ncol = 2)\n# ggsave(\"your_file_path/plot.comp_smr.png\"\nmap.smooth_smr_state <- ma_mort_istd_smoothed %>% \n  left_join(tract_geometry, by= \"GEOID10\") %>% \n  ggplot() +\n  geom_sf(mapping = aes(geometry=geometry, \n                        fill= exp(smooth_smr)),\n          lwd = 0.1) +\n    scale_fill_distiller(palette = \"BrBG\",\n                         trans = scales::pseudo_log_trans(sigma=0.01),\n                         limits = exp(c(-1.01,1)*log(4)),\n                         breaks=c(0.25,0.5,1,2,4), oob=squish) +\n    labs(title = \"Smoothed Standardized Mortality Ratios (SMR)\",\n         caption = expression(atop(\"Source: Massachusetts Mortality Data 2013-2017\", \n                                   \"5-Year ACS files from end-years 2015-2019\")),\n         fill = \"Smoothed SMR\", x=\"\", y=\"\") +\n    theme_void() +\n    theme(axis.text.x=element_blank(), #remove x axis labels\n          axis.ticks.x=element_blank(), #remove x axis ticks\n          axis.text.y=element_blank(),  #remove y axis labels\n          axis.ticks.y=element_blank(),\n          legend.position = c(0.25, 0.25),\n          legend.key.size = unit(0.4, \"cm\"))\nmap.smooth_smr_boston <- ma_mort_istd_smoothed %>% \n  left_join(tract_geometry, by= \"GEOID10\") %>% \n  filter(super_town %in% c(\"boston\",\"brookline\",\"chelsea\",\"cambridge\",\n                           \"everett\",\"hull\",\"malden\",\"medford\",\"newton\",\n                           \"somerville\",\"winthrop\",\"watertown\")) %>% \n  ggplot() +\n    geom_sf(mapping = aes(geometry = geometry,\n                          fill = exp(smooth_smr)),\n            lwd = 0.1) +\n    scale_fill_distiller(palette = \"BrBG\",\n                         trans = scales::pseudo_log_trans(sigma=0.01),\n                         limits = exp(c(-1.01,1)*log(4)),\n                         breaks=c(0.25,0.5,1,2,4), oob=squish) +\n    labs(title = \"Boston\") +\n    theme_void() +\n    theme(strip.text.x = element_blank(),\n          legend.position = \"None\",\n          plot.title      = element_text(hjust = 0.5)) \nmap.smooth_smr <- ggdraw() +\n  draw_plot(map.smooth_smr_state , x = 0.00, y = 0.00, width = 0.80, height = 1.00) +\n  draw_plot(map.smooth_smr_boston, x = 0.65, y = 0.50, width = 0.30, height = 0.30)\nmap.comp_smr <- plot_grid(map.raw_smr, map.smooth_smr,\n                           ncol = 2)\n# ggsave(\"your_file_path/map.comp_smr.png\")\n#Code to include our ABSM\nmodel_form_1 <- O ~ 1 + f(id_order, model='bym2', graph=INLA_adj_mat, scale.model=TRUE, constr=TRUE) + ICE_qt_2 + ICE_qt_3 + ICE_qt_4 + ICE_qt_5 + ICE_qt_NA\nmodel_1 <- inla(model_form_1, family=\"poisson\",\n                data = ma_mort_istd_ordered,\n                E=E, # E points to the expected count field\n                control.predictor = list(compute=TRUE), # computes transformed posterior marginals\n                control.compute   = list(dic=TRUE)) # computes DIC for model fit\n# We can use this code to extract results of the model\nfixed_results <- model_1$summary.fixed \ndic_results <- summary(model_1$dic$dic)\n# the first block of the summary.random output are the area effects (u + v)\nrandom_results <- model_1$summary.random$id_order$mean[1:n.tracts]\n# and this code pulls out the residual results after adjusting for ICE\nrisk_residuals <- data.frame(ice_residuals = random_results) %>%\n  mutate(id_order = row_number())\n# append to dataset of other model effects\nma_mort_istd_adj <- ma_mort_istd_smoothed %>% \n  left_join(risk_residuals, by=\"id_order\")\n# Visualize map after adjusting for ICE\nmap.adj_smr_state <- ma_mort_istd_adj %>% \n  left_join(tract_geometry, by= c(\"GEOID10\",\"id_order\")) %>% \n  ggplot() +\n  geom_sf(mapping = aes(geometry = geometry, \n                        fill     = exp(ice_residuals)),\n          lwd = 0.1) +\n    scale_fill_distiller(palette = \"BrBG\",\n                         trans = scales::pseudo_log_trans(sigma=0.01),\n                         limits = exp(c(-1.01,1)*log(4)),\n                         breaks=c(0.25,0.5,1,2,4), oob=squish) +\n  labs(title = \"Residual Standardized Mortality Ratios after adjusting for ICE\",\n       caption = expression(atop(\"Source: Massachusetts Mortality Data 2013-2017\", \n                                 \"5-Year ACS files from end-years 2015-2019\",\n                                 \"National Air Toxin Assessment, 2017\")),\n       fill = \"Adjusted SMR\", x=\"\", y=\"\") +\n  theme_void() +\n  theme(axis.text.x=element_blank(), #remove x axis labels\n        axis.ticks.x=element_blank(), #remove x axis ticks\n        axis.text.y=element_blank(),  #remove y axis labels\n        axis.ticks.y=element_blank(),\n        legend.position = c(0.25, 0.25),\n        legend.key.size = unit(0.4, \"cm\"))\nmap.adj_smr_boston <- ma_mort_istd_adj %>% \n  left_join(tract_geometry, by= c(\"GEOID10\",\"id_order\")) %>% \n  filter(super_town %in% c(\"boston\",\"brookline\",\"chelsea\",\"cambridge\",\n                           \"everett\",\"hull\",\"malden\",\"medford\",\"newton\",\n                           \"somerville\",\"winthrop\",\"watertown\")) %>% \n  ggplot() +\n    geom_sf(mapping = aes(geometry = geometry,\n                          fill = exp(ice_residuals)),\n            lwd = 0.1) +\n    scale_fill_distiller(palette = \"BrBG\",\n                         trans = scales::pseudo_log_trans(sigma=0.01),\n                         limits = exp(c(-1.01,1)*log(4)),\n                         breaks=c(0.25,0.5,1,2,4), oob=squish) +\n    labs(title = \"Boston\") +\n    theme_void() +\n    theme(strip.text.x = element_blank(),\n          legend.position = \"None\",\n          plot.title      = element_text(hjust = 0.5)) \nmap.adj_smr <- ggdraw() +\n  draw_plot(map.adj_smr_state , x = 0.00, y = 0.00, width = 0.80, height = 1.00) +\n  draw_plot(map.adj_smr_boston, x = 0.65, y = 0.50, width = 0.30, height = 0.30)\n# ggsave(\"your_file_path/map.adj_smr.png\")\nmap.comp_smr2 <- plot_grid(map.smooth_smr, map.adj_smr,\n                           ncol = 2)\n# ggsave(\"your_file_path/map.comp_smr2.png\")"},{"path":"premature-mortality.html","id":"what-is-the-racialized-disparity-in-premature-mortality-overall","chapter":"7 Case Study 1: Premature Mortality in Massachusetts (2013 - 2017)","heading":"7.4.2 What is the racialized disparity in premature mortality overall?","text":"can describe inequities racialized groups much way describe inequities ABSM - aggregating death population data. already aggregated - now can stratify analysis compare White Non-Hispanic Black populations. Throughout example may refer groups White Black shorthand.NOTE: may noticed description White population includes “Non-Hispanic” designation, description Black population . quirk ACS data - population counts readily available Black Non-Hispanic population. thus slight mismatch numerator mortality data (Non-Hispanic Black deaths) denominator (Black population).need age-adjust data , differences premature mortality may due differential distribution ages racialized group seen :, case, direct age adjustment get standardized mortality ratios. require reference population, downloaded (wrangled) National Cancer Institute (https://seer.cancer.gov/stdpopulations/). preferential indirect age standardization, Non-Hispanic White population drive expected rates using method.Let’s now age adjust aggregated data racialized group can compare standardized mortality rates.mortality rates increase, also seems increase variance estimates. extreme rates seeing really product small sample sizes districts. can see demonstrated ordering results population size census tract.might expect, extremely variable mortality rates occurring smaller populations. maps look like?seeing really high rates census tracts Black population, smaller overall sample sizes. also seeing many census tracts black population. can also display two maps one, calculating Rate Difference, Rate Ratio. show crude Rate Ratio Age Standardized rates. Key interpreting map understanding rates influenced small sample sizes large potential errors previously visualized. ’s researchers communities interpret “real” effects see .map IRR, can see crude rate ratio Black Non-Hispanic White populations:can see crude map , areas Black deaths, variation IRR, high, concerning IRR values.","code":"\ntab_age_race <- ma_denominator %>%\n      filter(race_group %in% c(\"Non-Hispanic White\",\"Black\"),\n         sex != \"Total\",\n         age_cat != \"Total\") %>% \n  group_by(GEOID10, race_group, sex, age_cat) %>% \n  summarise(pop = sum(population, na.rm=T)) %>%\n  inner_join(ma_absm_sum, by = c(\"GEOID10\")) %>% \n  mutate(age_cat_broad = case_when(age_cat %in% c(\"0-4\",\"5-9\",\"10-14\",\"15-19\",\"20-24\") ~ \"0-24%\",\n                                   age_cat %in% c(\"25-29\",\"30-34\",\"35-39\",\"35-44\",\"40-44\") ~ \"25-44%\",\n                                   age_cat %in% c(\"45-49\",\"45-54\",\"50-54\",\"55-59\",\"55-64\",\"60-64\") ~ \"45-64%\")) %>%\n  group_by(race_group, age_cat_broad) %>%\n  summarise(pop = sum(pop, na.rm=T)) %>%\n  group_by(race_group) %>%\n  mutate(percentage = pop/sum(pop)) %>% \n  ggplot(aes(x=race_group, y=percentage, fill= age_cat_broad)) +\n    geom_bar(position=\"stack\", stat=\"identity\") +\n    scale_fill_viridis_d(option = \"A\") +\n    labs(title =\"Age Distribution by Racialized Group\",\n         fill = \"Age Category\",\n         caption = \"5-Year ACS files from end-years 2015-2019\") +\n    ylab(\"Percentage of Population\") +\n    xlab(\"Racialized Group\") +\n    theme(legend.position=\"bottom\") +\n    theme_minimal()\n    \n# ggsave(\"your_file_path/tab_age_race.png\")\nseer_std <- readRDS(\"data/07-premature-mortality/seer_std.RDS\")\nma_mort_dstd <- ma_mort_ct %>%\n  mutate(race_group = case_when(race_group == \"Non-Hispanic Black\" ~ \"Black\",\n                                TRUE ~ race_group)) %>% \n  inner_join(ma_denominator, by = c(\"year\",\"GEOID10\",\"race_group\",\"sex\",\"age_cat\")) %>%\n  filter(race_group %in% c(\"Non-Hispanic White\",\"Black\"),\n         age_cat != \"Total\") %>% \n  group_by(GEOID10,super_town, race_group, age_cat) %>% \n  summarise(num = sum(deaths, na.rm=T),\n            den = sum(population, na.rm=T))%>%\n  mutate(den = ifelse(den == 0, num, den)) %>% # this line takes areas with population counts as zero, and provides the count as the number of deaths. This is an approach to resolve when population data does not align with the outcome data\n  left_join(seer_std, by=\"age_cat\") %>%\n  mutate(rate_i = wt*num/den,\n         var_rate_i = (num*wt^2)/den^2) %>%\n  group_by(GEOID10, super_town, race_group) %>%\n  summarise(num = sum(num, na.rm=T),\n            den = sum(den, na.rm=T),\n            std_rate = sum(rate_i, na.rm=T),\n            var_std_rate = sum(var_rate_i, na.rm=T),\n            sumwt = sum(wt),\n            sumwt2 = sum(wt^2)) %>%\n  mutate(std_rate = std_rate / sumwt *1000,\n         var_std_rate = var_std_rate / sumwt2 *1000,\n         std_rate_lo95 = std_rate - 1.96*sqrt(var_std_rate),\n         std_rate_up95 = std_rate + 1.96*sqrt(var_std_rate)) %>%\n  ungroup()\n plot.agg_rates <- ma_mort_dstd %>% \n  filter(!is.na(std_rate)) %>% \n  select(GEOID10,race_group, contains(\"std_rate\")) %>% \n  arrange(std_rate) %>% \n  group_by(race_group) %>% \n  mutate(orderID = row_number()) %>%\n  ungroup() %>% \n  ggplot(aes(x=orderID, y=std_rate)) + \n    geom_errorbar(aes(ymin = std_rate_lo95, ymax=std_rate_up95), size = 0.1) +\n    geom_point(color = \"limegreen\", alpha = 0.8, size = 1) +\n    labs(title = \"Age-Standardized Premature Mortality Rate\",\n         caption = expression(atop(\"Source: Massachusetts Mortality Data 2013-2017\", \n                                   \"5-Year ACS files from end-years 2015-2019\"))) +\n    xlab(\"Census Tracts\") +\n    ylab(\"Mortality Rates per 1000\") +\n    facet_wrap(vars(race_group), ncol = 2, scales = \"free_x\") +\n    theme_minimal() +\n    theme(axis.ticks.x = element_blank(), \n          axis.text.x = element_blank())\n# ggsave(\"your_file_path/plot.agg_rates.png\")\nordered.agg_rates <- ma_mort_dstd %>% \n  filter(!is.na(std_rate)) %>% \n  select(GEOID10, race_group, contains(\"std_rate\"), den) %>% \n  ggplot(aes(x=den, y=std_rate)) + \n    geom_point(color = \"limegreen\", alpha = 0.8, size = 1) +\n    geom_errorbar(aes(ymin = std_rate_lo95, ymax=std_rate_up95), size = 0.1) +\n    labs(title = \"Age-Standardized Premature Mortality Rate\",\n         caption = expression(atop(\"Source: Massachusetts Mortality Data 2013-2017\", \n                                   \"5-Year ACS files from end-years 2015-2019\"))) +\n         xlab(\"Size of Census Tract\") +\n         ylab(\"Mortality Rates\") +\n    facet_wrap(vars(race_group), ncol = 2, scales = \"free_x\") +\n    theme_minimal() \n# ggsave(\"your_file_path/ordered.agg_rates.png\")\nplotlist <- vector(mode = \"list\", length = 2)\nnames(plotlist) <- c(\"Non-Hispanic White\",\"Black\")\nfor (plt in names(plotlist)) { # This loop allows us to make the same map twice, one for Non-Hispanic White, and one for Black\n  map.state <- ma_mort_dstd %>% \n    filter(race_group == plt) %>%\n    mutate(std_rate = ifelse(std_rate > 10, 10, std_rate)) %>% \n    left_join(tract_geometry, by= \"GEOID10\") %>% \n    ggplot() +\n      geom_sf(mapping = aes(geometry = geometry,\n                            fill = std_rate),\n              lwd = 0) +\n      scale_fill_distiller(palette = 'Blues', direction = 1, limits = c(0, 10)) + \n      labs(title = plt,\n           fill = \"Age Standardized Mortality Rate\", x=\"\", y=\"\") +\n      theme_void() +\n      theme(axis.text.x=element_blank(), #remove x axis labels\n            axis.ticks.x=element_blank(), #remove x axis ticks\n            axis.text.y=element_blank(),  #remove y axis labels\n            axis.ticks.y=element_blank(),\n            legend.position = c(0.25, 0.25),\n            legend.key.size = unit(0.2, \"cm\"))\n  \n  map.boston <- ma_mort_dstd %>% \n    filter(race_group == plt,\n           super_town %in% c(\"boston\",\"brookline\",\"chelsea\",\"cambridge\",\n                           \"everett\",\"hull\",\"malden\",\"medford\",\"newton\",\n                           \"somerville\",\"winthrop\",\"watertown\")) %>% \n    mutate(std_rate = ifelse(std_rate > 10, 10, std_rate)) %>% \n    left_join(tract_geometry, by= \"GEOID10\") %>% \n    ggplot() +\n      geom_sf(mapping = aes(geometry = geometry,\n                            fill = std_rate),\n              lwd = 0) +\n      scale_fill_distiller(palette = 'Blues', direction = 1, limits = c(0, 10)) + \n      labs(title = \"Boston\") +\n      theme_void() + \n      theme(strip.text.x = element_blank(),\n            legend.position = \"None\",\n            plot.title      = element_text(hjust = 0.5)) \n  \n  plotlist[[plt]] <- ggdraw() +\n    draw_plot(map.state , x = 0.00, y = 0.00, width = 0.80, height = 1.00) +\n    draw_plot(map.boston, x = 0.65, y = 0.50, width = 0.30, height = 0.30)\n  \n}\ntitle <- ggdraw() +\n  draw_label(\"Age Standardized Mortality Rates\", size = 12, fontface='bold', hjust = 0.2)\ncaption1 <- ggdraw() +\n  draw_label(\"Source: Massachusetts Mortality Data 2013-2017\", size = 8, hjust = 0.5) \ncaption2 <- ggdraw() +\n  draw_label(\"5-Year ACS files from end-years 2015-2019\", size = 8, hjust = 0.5) \nbyrace_std_rates <- plot_grid(plotlist = plotlist,\n                              ncol = 2)\n# ggsave(\"your_file_path/byrace_std_rates.png\")\nirr_data <- ma_mort_dstd %>% \n  select(GEOID10,super_town,race_group, std_rate, var_std_rate) %>% \n  pivot_wider(id_cols = c(GEOID10,super_town),\n              names_from = race_group,\n              values_from = c(std_rate, var_std_rate)) %>% \n  mutate(irr = ifelse(`std_rate_Non-Hispanic White` == 0, NA_real_, `std_rate_Black` / `std_rate_Non-Hispanic White`),\n         irr_var = `var_std_rate_Black` + `var_std_rate_Non-Hispanic White`,\n         irr_lo95 = irr - 1.96*sqrt(irr_var),\n         irr_up95 = irr + 1.96*sqrt(irr_var)) \nirr_plot <- irr_data %>% \n  arrange(irr) %>% \n  mutate(orderID = row_number()) %>%\n  ungroup() %>% \n  ggplot(aes(x=orderID, y=irr)) + \n    geom_point(color = \"limegreen\", alpha = 0.8, size = 1) +\n    geom_errorbar(aes(ymin = irr_lo95, ymax=irr_up95), size = 0.1) +\n    labs(title = expression(atop(\"Incidence Rate Ratio Comparing Black\", \n                                 \"and Non Hispanic White Populations\")),\n         caption = expression(atop(\"Source: Massachusetts Mortality Data 2013-2017\",\n                                   \"5-Year ACS files from end-years 2015-2019\"))) +\n         xlab(\"Census Tracts\") +\n         ylab(\"IRR\") +\n    theme_minimal() \n  \n# ggsave(\"your_file_path/irr_plot.png\")\nmap.state.irr <- irr_data %>% \n  mutate(irr = case_when(irr > 4  ~ 4,\n                         std_rate_Black == 0 ~ NA_real_, # Setting as missing any areas with no black deaths\n                         TRUE     ~ irr)) %>% \n  left_join(tract_geometry, by= \"GEOID10\") %>% \n  ggplot() +\n    geom_sf(mapping = aes(geometry = geometry,\n                          fill = irr),\n            lwd = 0.1) +\n    scale_fill_distiller(palette = \"BrBG\",\n                         trans = scales::pseudo_log_trans(sigma=0.01),\n                         limits = exp(c(-1,1)*log(4)),\n                         breaks = c(0.25,0.5,1,2,4), oob=squish,\n                         na.value = \"grey\") +\n    labs(title = expression(atop(\"Incidence Rate Ratio Comparing Non-Hispanic Black\", \n                                 \"and Non Hispanic White Populations\")),\n         caption = expression(atop(\"Source: Massachusetts Mortality Data 2013-2017\",\n                                   \"5-Year ACS files from end-years 2015-2019\")),\n         fill = \"IRR\", x=\"\", y=\"\") +\n    theme_void() +\n    theme(legend.position = c(0.25, 0.25),\n          legend.key.size = unit(0.3, \"cm\"),\n          plot.title      = element_text(hjust = 0.1),\n          plot.subtitle   = element_text(hjust = 0.1))\nmap.boston.irr <- irr_data %>% \n  filter(super_town %in% c(\"boston\",\"brookline\",\"chelsea\",\"cambridge\",\n                           \"everett\",\"hull\",\"malden\",\"medford\",\"newton\",\n                           \"somerville\",\"winthrop\",\"watertown\")) %>% \n  mutate(irr = case_when(irr > 4  ~ 4,\n                         std_rate_Black == 0 ~ NA_real_,\n                         TRUE     ~ irr)) %>% \n  left_join(tract_geometry, by= \"GEOID10\") %>% \n  ggplot() +\n    geom_sf(mapping = aes(geometry = geometry,\n                          fill = irr),\n            lwd = 0.1) +\n    scale_fill_distiller(palette = \"BrBG\",\n                         trans = scales::pseudo_log_trans(sigma=0.01),\n                         limits = exp(c(-1,1)*log(4)),\n                         breaks = c(0.25,0.5,1,2,4), oob=squish,\n                         na.value = \"grey\") +\n    labs(title = \"Boston\") +\n    theme_void() + \n    theme(strip.text.x = element_blank(),\n          legend.position = \"None\",\n          plot.title      = element_text(hjust = 0.5)) \nmap.irr <- ggdraw() +\n  draw_plot(map.state.irr , x = 0.00, y = 0.05, width = 0.80, height = 1.00) +\n  draw_plot(map.boston.irr, x = 0.65, y = 0.55, width = 0.30, height = 0.30)\n#ggsave(\"your_file_path/map.irr.png\")"},{"path":"premature-mortality.html","id":"what-are-the-associations-with-absm-by-racialized-group","chapter":"7 Case Study 1: Premature Mortality in Massachusetts (2013 - 2017)","heading":"7.4.3 What are the associations with ABSM by racialized group?","text":"order visualize differences ABSM - ICE - interacts racialized group, can use poisson model mortality rates, include interaction term racialized group poverty. can plot using `INLA’ alternatively use generalized linear model function. recreate indirect standardization - time race, order include variables model.can look results plots, get sense way ICE interacting race impact mortality Massachusetts.can see plots areas become heavily concentrated Non-Hispanic White, high-income individuals, impact racialized group Premature Mortality increases. populations heavily concentrated low-income People Color (POC), mortality rates Non-Hispanic White individuals higher Black individuals. end spectrum results insignificant (possibly Massachusetts less census tracts category) trend direction.look impact ICE within racial categories actually see little impact Black population, impact ICE significantly different concentrated low-income POC category. Non-Hispanic White population see larger differences reference, clear pattern Non-Hispanic White individuals fare much worse neighborhoods low-income POC.? learned differential impact neighborhood context individual racialized group membership?","code":"\n# Indirect standardization to prepare the data for spatial models\nma_mort_istd_byrace <- ma_mort_ct %>%\n  filter(race_group != \"Total\",\n         sex != \"Total\",\n         age_cat != \"Total\") %>% \n  mutate(race_group = ifelse(race_group == \"Non-Hispanic Black\", \"Black\", race_group)) %>% \n  filter(race_group %in% c(\"Non-Hispanic White\",\"Black\")) %>% \n  left_join(ma_denominator, by = c(\"GEOID10\",\"year\",\"age_cat\",\"race_group\",\"sex\")) %>% \n  mutate(age_cat = case_when(age_cat %in% c(\"35-39\",\"40-44\") ~ \"35-44\",\n                             age_cat %in% c(\"45-49\",\"50-54\") ~ \"45-54\",\n                             age_cat %in% c(\"55-59\",\"60-64\") ~ \"55-64\",\n                             TRUE ~ age_cat)) %>%\n  group_by(GEOID10, super_town, age_cat, race_group) %>% \n  summarise(deaths = sum(deaths, na.rm = TRUE),\n            population = sum(population, na.rm = TRUE))%>% \n  right_join(reference_rates, by=\"age_cat\") %>%\n  mutate(expected = ref_rate*population) %>%\n  group_by(GEOID10, super_town, race_group) %>%\n  summarise(O = sum(deaths),\n            E = sum(expected),\n            raw_smr = O/E,\n            var_raw_smr = O/E^2,\n            raw_smr_lo95 = raw_smr - 1.96*sqrt(var_raw_smr),\n            raw_smr_up95 = raw_smr + 1.96*sqrt(var_raw_smr)) %>%\n  left_join(ma_absm_sum, by = \"GEOID10\") %>% \n  mutate(id = row_number()) %>%\n  fastDummies::dummy_cols(select_columns= \"ICE_qt\") %>%\n  rename(ICE_qt_1  = \"ICE_qt_(-0.689,0.0607]\",\n         ICE_qt_2  = \"ICE_qt_(0.0607,0.245]\", \n         ICE_qt_3  = \"ICE_qt_(0.245,0.362]\",  \n         ICE_qt_4  = \"ICE_qt_(0.362,0.465]\",  \n         ICE_qt_5  = \"ICE_qt_(0.465,0.727]\") \n# Make sure that the data file has areas in the same order as the shape file\nma_mort_istd_byrace_ordered <- ma_mort_istd_byrace %>% \n  filter(!(O == 0 & E == 0)) %>% \n  mutate(intercept = 1) %>%\n  left_join(tract_geometry, by=\"GEOID10\") %>%\n  select(-geometry) %>% \n  arrange(id_order) %>%\n  mutate(race_group = factor(race_group, levels = c(\"Non-Hispanic White\", \"Black\")),\n         ICE_qt = factor(ICE_qt, levels=c(\"(0.465,0.727]\",\n                                          \"(0.362,0.465]\",\n                                          \"(0.245,0.362]\",\n                                          \"(0.0607,0.245]\",\n                                          \"(-0.689,0.0607]\")))\n# BYM model with ICE-race interaction\nmodel_form_2 <- O ~ 1 + f(id_order, model='bym2', graph=INLA_adj_mat, scale.model=TRUE, constr=TRUE) + factor(race_group)*factor(ICE_qt)\n#The following code outlines the linear combinations our model will make, so that we can calculate intersectional inequities\nlinear_combinations <- inla.make.lincombs(\n\"factor(race_group)Black\"                       = c(1,1,1,1,0,0,0,0,1,1,1,1),\n\"factor(ICE_qt)(0.362,0.465]\"                   = c(0,0,0,0,1,0,0,0,1,0,0,0),\n\"factor(ICE_qt)(0.245,0.362]\"                   = c(0,0,0,0,0,1,0,0,0,1,0,0),  \n\"factor(ICE_qt)(0.0607,0.245]\"                  = c(0,0,0,0,0,0,1,0,0,0,1,0),\n\"factor(ICE_qt)(-0.689,0.0607]\"                 = c(0,0,0,0,0,0,0,1,0,0,0,1),\n\"factor(race_group)Black:factor(ICE_qt)(0.362,0.465]\"   = c(1,0,0,0,1,0,0,0,1,0,0,0),\n\"factor(race_group)Black:factor(ICE_qt)(0.245,0.362]\"   = c(0,1,0,0,0,1,0,0,0,1,0,0),\n\"factor(race_group)Black:factor(ICE_qt)(0.0607,0.245]\"  = c(0,0,1,0,0,0,1,0,0,0,1,0),\n\"factor(race_group)Black:factor(ICE_qt)(-0.689,0.0607]\" = c(0,0,0,1,0,0,0,1,0,0,0,1))\nmodel_results_lc <- inla(model_form_2, \n                         family=\"poisson\",\n                         data=ma_mort_istd_byrace_ordered, E=E, \n                         control.predictor=list(compute=TRUE), \n                         control.compute=list(dic=TRUE, waic=TRUE),\n                         lincomb = linear_combinations)\n# Function to extract the fixed results\ntidy.inla.fixed <- function(x){\n  # x = model_inla\n  term_names <- rownames(x$summary.fixed)\n  tibble::as_tibble(x$summary.fixed) %>%\n    dplyr::mutate(terms = term_names) %>%\n    dplyr::rename(term = terms,\n                  estimate = mean,\n                  conf.low = `0.025quant`, \n                  conf.high = `0.975quant`) %>%\n    dplyr::select(term, estimate,\n                  conf.low, conf.high)\n}\n# Function to extract the linear combinations\ntidy.inla.lc <- function(x){\n  # x = model_inla\n  term_names <- rownames(x$summary.lincomb.derived)\n  tibble::as_tibble(x$summary.lincomb.derived) %>%\n    dplyr::mutate(terms = term_names) %>%\n    dplyr::rename(term = terms,\n                  estimate = mean,\n                  conf.low = `0.025quant`, \n                  conf.high = `0.975quant`) %>%\n    dplyr::select(term, estimate,\n                  conf.low, conf.high)\n}\ntidy_fixed_results <- tidy.inla.fixed(model_results_lc) %>%\n  filter(!term==\"(Intercept)\" & !stringr::str_detect(term, \":\")) %>%\n  separate(col=term, into=c(\"pre1\", \"term\"), sep='\\\\)') %>%\n  select(-pre1) %>%\n  mutate(race_group = case_when(term==\"Black\" ~ \"Black\",\n                                TRUE ~ \"Non-Hispanic White\"),\n         qval = case_when(!term==\"Black\" ~ term),\n         which = case_when(term==\"Black\" ~ \"byICE_qt\",\n                           TRUE ~ \"byRace\"),\n         twoway = case_when(term==\"Black\" ~ 1,\n                            TRUE ~ 1))\ntidy_lc_results <-  tidy.inla.lc(model_results_lc) %>% \n  mutate(race_group = \"Black\",\n         qval = factor(case_when(term %in% c(\"lc01\", \"lc05\", \"lc09\") ~ \"(0.362,0.465]\",\n                                 term %in% c(\"lc02\", \"lc06\", \"lc10\") ~ \"(0.245,0.362]\",\n                                 term %in% c(\"lc03\", \"lc07\", \"lc11\") ~ \"(0.0607,0.245]\",\n                                 term %in% c(\"lc04\", \"lc08\", \"lc12\") ~ \"(-0.689,0.0607]\"),\n                       levels=levels(ma_mort_istd_byrace_ordered$ICE_qt)),\n         which = factor(case_when(term %in% c(\"lc01\", \"lc02\", \"lc03\", \"lc04\") ~ \"byICE_qt\",\n                                  term %in% c(\"lc05\", \"lc06\", \"lc07\", \"lc08\") ~ \"byRace\")),\n         twoway = case_when(term %in% c(\"lc09\", \"lc10\", \"lc11\", \"lc12\") ~ 1 ,\n                            TRUE ~ 0))\ninteraction_effects_bym <- bind_rows(tidy_fixed_results, tidy_lc_results) %>%\n  arrange(which, race_group, qval) %>%\n  mutate(qval = ifelse(is.na(qval),\"(0.465,0.727]\",qval)) %>% \n  select(-term)\n# Plot black/white disparities within ICE categories\nrace_effect_within_ICEqt <- interaction_effects_bym %>%\n  filter(which == \"byICE_qt\") %>% \n  ggplot(aes(x=qval, \n             y=exp(estimate), \n             ymin=exp(conf.low), \n             ymax=exp(conf.high))) +\n  geom_point(position=position_dodge(width=0.5), size=3) +\n  geom_errorbar(width=0.2, position = position_dodge(width=0.5)) +\n  geom_hline(yintercept=1, linetype=\"dotted\") +\n  scale_color_brewer(palette=\"Set1\") +\n  labs(title = \"Impact of Race on Premature Mortality, across levels of ICE\",\n       caption = expression(atop(\"Source: Massachusetts Mortality Data 2013-2017\", \n                                 \"5-Year ACS files from end-years 2015-2019\")),\n       x = \"\", y = \"IRR comparing Black to White population\") +\n  theme_minimal() +\n  theme(legend.position=\"bottom\", \n        axis.text.x=element_text(angle=65, hjust=1, vjust=1))\n# Plot ABSM disparities within racial/ethnic categories, for aggregated method and Poisson robust\nICEqt_effect_within_race <- interaction_effects_bym %>% \n  filter(which==\"byRace\") %>% \n  ggplot(aes(x=qval, \n             y=exp(estimate), \n             ymin=exp(conf.low), \n             ymax=exp(conf.high),\n             color = race_group)) +\n  geom_point(size=3) +\n  geom_errorbar(width=0.2) +\n  geom_hline(yintercept=1, linetype=\"dotted\") +\n  labs(title = \"Impact of ICE on Premature Mortality, across racial categorizations\",\n       caption = expression(atop(\"Source: Massachusetts Mortality Data 2013-2017\", \n                                 \"5-Year ACS files from end-years 2015-2019\")),\n       x=\"\", \n       y=\"IRR comparing ICE category to reference\", \n       color = \"race_group\") +\n  theme_minimal() +\n  theme(legend.position=\"bottom\", \n        axis.text.x=element_text(angle=65, hjust=1, vjust=1))\ninteraction_plots <- plot_grid(race_effect_within_ICEqt, ICEqt_effect_within_race,\n                               ncol = 2)\n#ggsave(\"your_file_path/interaction_plots.png\")"},{"path":"premature-mortality.html","id":"appendix-wrangling-your-mortality-data","chapter":"7 Case Study 1: Premature Mortality in Massachusetts (2013 - 2017)","heading":"7.5 Appendix: Wrangling Your Mortality Data","text":"code (example) turn geocoded mortality file provided , merging census tracts towns data, aggregating groups align age racialized groups match denominator data.","code":"\n# Importing the raw mortality data, with the geocoded coordinates, converted into spatial feature (sf) data\nma_mort <- read_rds(\"ma_mort_geo.RDS\") %>% \n  st_as_sf(coords=c(\"lon\",\"lat\"), crs = 4269)\n# Importing a MA tract-level shapefile using tigris package\ntracts_sf <- tracts(state = \"25\", year = \"2010\")\n# Merging the Census Tracts to our crosswalk of towns so we know which ones need to be combined\nsupertowns <- read_excel(\"ma ct to supertinytowns.xlsx\", col_names = FALSE, col_types = \"text\") %>%\n  transmute(GEOID10 = ...1,\n            super_town = str_to_lower(...2))\ntracts_sf <- left_join(tracts_sf,supertowns, by = \"GEOID10\")\n# Merging to geocoded mortality and dropping geometries\n# The geometries (actual stored location information) of a shapefile can be very unwieldy and slow data wrangling commands. Since we only need them to visualize our maps, we will remove them for now\nma_mort_agg <- st_join(tracts_sf, ma_mort, left = FALSE) %>%\n  st_drop_geometry() %>%\n  arrange(id) %>%\n  mutate(super_town = case_when(super_town == \"stt1\"   ~ \"Super Town 1\",\n                                super_town == \"stt2\"   ~ \"Super Town 2\",\n                                super_town == \"stt3\"   ~ \"Super Town 3\",\n                                super_town == \"stt4\"   ~ \"Super Town 4\",\n                                super_town == \"stt5\"   ~ \"Super Town 5\",\n                                super_town == \"stt6\"   ~ \"Super Town 6\",\n                                super_town == \"stt7\"   ~ \"Super Town 7\",\n                                super_town == \"stt8\"   ~ \"Super Town 8\",\n                                super_town == \"stt9\"   ~ \"Super Town 9\",\n                                super_town == \"stt10\"  ~ \"Super Town 10\",\n                                super_town == \"stt11\"  ~ \"Super Town 11\",\n                                super_town == \"stt12\"  ~ \"Super Town 12\",\n                                super_town == \"stt13\"  ~ \"Super Town 13\",\n                                super_town == \"stt14\"  ~ \"Super Town 14\",\n                                super_town == \"stt15\"  ~ \"Super Town 15\",\n                                super_town == \"stt16\"  ~ \"Super Town 16\",\n                                super_town == \"stt17\"  ~ \"Super Town 17\",\n                                super_town == \"stt18\"  ~ \"Super Town 18\",\n                                super_town == \"stt19\"  ~ \"Super Town 19\",\n                                super_town == \"stt20\"  ~ \"Super Town 20\",\n                                super_town == \"stt21\"  ~ \"Super Town 21\",\n                                geo_town != super_town ~ super_town,\n                                TRUE                   ~ geo_town)) %>% \n  select(id, year, date,\n         age, sex, starts_with(\"race\"), hisp, immigrant, starts_with(\"educ\"),\n         contains(\"icd10\"),\n         address, zip, starts_with(\"geo\"), super_town, contains(\"type\"), north, south, east, west,\n         GEOID10)\n# Aggregating the data by census tract and our selected variables of interest\n# Dropping deaths that occurred after age 65\n# Creating Category Totals for each variable\nma_mort_ct <- ma_mort_agg %>%\n  mutate(age_cat = cut(age,\n                       breaks = c(-1,4,9,14,19,24,29,34,39,44,49,54,59,64),\n                       labels = c(\"0-4\",  \"5-9\",  \"10-14\",\"15-19\",\"20-24\",\n                                  \"25-29\",\"30-34\",\"35-39\",\"40-44\",\"45-49\",\n                                  \"50-54\",\"55-59\",\"60-64\"))) %>%\n    filter(age < 65,\n           race_group != \"Other\",\n           sex %in% c(\"Female\",\"Male\")) %>% \n  select(-c(age, race, immigrant, educ_years, contains(\"icd10\"),\n            contains(\"address\"), zip, geo_town, contains(\"type\"),\n            north, south, east, west, date, hisp)) %>%\n  group_by(year,\n           age_cat,\n           sex,\n           race_group,\n           super_town,\n           GEOID10) %>%\n  summarise(deaths = n()) %>%\n  ungroup() %>%\n  pivot_wider(id_cols = c(year, GEOID10, super_town),\n              names_from = c(\"race_group\",\"sex\",\"age_cat\"),\n              values_from = deaths,\n              names_sep = \"__\",\n              values_fill = 0) %>%\n  rowwise() %>%\n  mutate(`Black__Male__35-44` = `Black__Male__35-39` + `Black__Male__40-44`,\n         `Black__Male__45-54` = `Black__Male__45-49` + `Black__Male__50-54`,\n         `Black__Male__55-64` = `Black__Male__55-59` + `Black__Male__60-64`,\n         `Black__Female__35-44` = `Black__Female__35-39` + `Black__Female__40-44`,\n         `Black__Female__45-54` = `Black__Female__45-49` + `Black__Female__50-54`,\n         `Black__Female__55-64` = `Black__Female__55-59` + `Black__Female__60-64`,\n         `Non-Hispanic White__Male__35-44` = `Non-Hispanic White__Male__35-39` + `Non-Hispanic White__Male__40-44`,\n         `Non-Hispanic White__Male__45-54` = `Non-Hispanic White__Male__45-49` + `Non-Hispanic White__Male__50-54`,\n         `Non-Hispanic White__Male__55-64` = `Non-Hispanic White__Male__55-59` + `Non-Hispanic White__Male__60-64`,\n         `Non-Hispanic White__Female__35-44` = `Non-Hispanic White__Female__35-39` + `Non-Hispanic White__Female__40-44`,\n         `Non-Hispanic White__Female__45-54` = `Non-Hispanic White__Female__45-49` + `Non-Hispanic White__Female__50-54`,\n         `Non-Hispanic White__Female__55-64` = `Non-Hispanic White__Female__55-59` + `Non-Hispanic White__Female__60-64`) %>%\n  ungroup() %>%\n  pivot_longer(cols = `Black__Male__0-4`:`Non-Hispanic White__Female__55-64`,\n               names_to = c(\"race_group\",\"sex\",\"age_cat\"),\n               names_sep = \"__\",\n               values_to = \"deaths\")\n# saveRDS(ma_mort_ct, file = \"ma_mort_ct.RDS\")               "},{"path":"breast-cancer-mortality.html","id":"breast-cancer-mortality","chapter":"8 Case Study 2: Breast Cancer Mortality in Massachusetts","heading":"8 Case Study 2: Breast Cancer Mortality in Massachusetts","text":": Enjoli Hall","code":""},{"path":"breast-cancer-mortality.html","id":"overview","chapter":"8 Case Study 2: Breast Cancer Mortality in Massachusetts","heading":"8.1 Overview","text":"outcome interest breast cancer mortality. investigate disparities breast cancer mortality racialized/ethnic group socioeconomic position. primarily focus comparisons risk Black non-Hispanic White non-Hispanic populations, motivated longstanding health inequities groups sufficiently large population sizes Massachusetts support analyses. working mortality data Massachusetts Registry Vital Records Statistics [https://www.mass.gov/lists/death-data] years 2013-2017. Vital statistics disease registries states (submit data federal government) keep official enumeration deaths (Friedman et al., 2005; Hetzel, 1997; Krieger, 2019); death certificates require data residential address time death, can useful tools monitoring trends health health equity. data residential address individual allows us geocode observation physical social environment person lived. Death records also typically include demographic information deceased, including categories racialized groups conform 1997 US Office Management Budget (OMB) standards classification federal data “race ethnicity” (OMB, 1997). explore social membership racialized groups area-based social metrics (ABSMs) might associated breast cancer mortality across Massachusetts. , pair mortality data demographic socioeconomic data U.S. Census Bureau’s 5-year American Community Survey (ACS) files.","code":""},{"path":"breast-cancer-mortality.html","id":"background-and-significance","chapter":"8 Case Study 2: Breast Cancer Mortality in Massachusetts","heading":"8.2 Background and Significance","text":"Breast cancer commonly diagnosed cancer worldwide one leading causes cancer death. United States, breast cancer incidence mortality rates vary widely across geographic regions racialized groups. example, breast cancer deaths United States declined overall 42 percent last 30 years, persistent mortality gap Black women White women. Breast cancer incidence rates among Black White women close, mortality rates markedly different, Black women 41 percent higher death rate breast cancer compared White women United States (Breast Cancer Research Foundation, 2022).Nevertheless, routine stratification presentation cancer data “race” absence socioeconomic data occupation, educational level, income, perpetuates view “race”—wrongly construed biological variable—explains racialized inequities breast cancer mortality cancer outcomes. Hidden view ways economic forms discrimination inequality might drive “racial”/ethnic inequities breast cancer mortality. Rather taking either/approach analyzing interpreting cancer data “race/ethnicity” socioeconomic position, important public health researchers stratify cancer data “race/ethnicity” socioeconomic position neither sufficient capture membership racialized groups class relations, separately together, affect health populations. However, U.S. public health surveillance systems typically include data relating health status socioeconomic position individual records. One possible solution gaps combine data health surveillance systems socioeconomic data derived U.S. Census analyze breast cancer mortality relation area-based socioeconomic measures domains socioeconomic position income poverty, thereby permitting calculation population-based breast cancer mortality rates stratified area-based socioeconomic position making visible socioeconomic gradients breast cancer mortality.can also monitor racialized socioeconomic cancer health inequities using conventional individual- area-level socioeconomic measures also measures racialized economic segregation polarization neighborhood, city town, regional levels; latter measures bring focus full range concentrations privilege deprivation area. 20 years ago, Williams Collins (2001) explained racial residential segregation acts fundamental cause racial disparities health exposing Black communities less healthy neighborhood housing conditions, fewer economic educational opportunities, lower quality health care resources compared White communities. Since time, hundreds empirical studies examined segregation key driver patterns population health health inequities, relatively studies examined cancer outcomes. Together, studies suggest residential segregation can generate racialized economic inequities across cancer continuum, part producing differential access medical care unequal exposures social environmental cancer risks.","code":""},{"path":"breast-cancer-mortality.html","id":"motivation-research-questions-and-learning-objectives-1","chapter":"8 Case Study 2: Breast Cancer Mortality in Massachusetts","heading":"8.3 Motivation, Research Questions, and Learning Objectives","text":"goal case study develop familiarity methods exploring visualizing racialized socioeconomic inequities health. specific goals :Download merge different datasets analysisDownload merge different datasets analysisVisualize map estimates area-based social metrics breast cancer mortalityVisualize map estimates area-based social metrics breast cancer mortalityIdentify relationships racialized group, area-based social metrics, breast cancer mortalityIdentify relationships racialized group, area-based social metrics, breast cancer mortalityModel space may impact inequities breast cancer mortalityModel space may impact inequities breast cancer mortalityThe research questions seek answer throughout case study include:overall socioeconomic gradient breast cancer mortality? (hint: can visualize spatial model)overall socioeconomic gradient breast cancer mortality? (hint: can visualize spatial model)racialized disparity breast cancer mortality overall? (hint: can visualize stratified aggregate analyses)racialized disparity breast cancer mortality overall? (hint: can visualize stratified aggregate analyses)area-based socioeconomic measures interact individual-level membership racialized groups affect patterns breast cancer mortality (.e., interactions socioeconomic position racialized groups, just socioeconomic inequities within racialized groups)? (hint: can explore Poisson model using interaction term)area-based socioeconomic measures interact individual-level membership racialized groups affect patterns breast cancer mortality (.e., interactions socioeconomic position racialized groups, just socioeconomic inequities within racialized groups)? (hint: can explore Poisson model using interaction term)","code":""},{"path":"breast-cancer-mortality.html","id":"getting-and-wrangling-your-data","chapter":"8 Case Study 2: Breast Cancer Mortality in Massachusetts","heading":"8.4 Getting and Wrangling Your Data","text":"providing datasets use throughout case studies wrangled reshaped use, also provide code show go process . can look whole datasets RStudio using view() command, look summaries datasets simply typing dataset name console window. can skip ahead “Approach” section follow along case study without issue.","code":""},{"path":"breast-cancer-mortality.html","id":"running-rstudio-and-setting-up-your-working-directory","chapter":"8 Case Study 2: Breast Cancer Mortality in Massachusetts","heading":"8.4.1 Running RStudio and Setting Up Your Working Directory","text":"Download Breast Cancer Mortality project folder [https://hu-.sharepoint.com/:f:/g/personal/denajavadi_g_harvard_edu/EpyUmRZB-hBGvdygLgRLTmMByYr_lmlP6pqY09f_bz8QBg?e=tDxIIg] contains data files geographic shapefiles well maps figures use case study. Save folder Desktop another easily accessible location computer. Note: edit change file names code corresponds file names folder.Next, need open RStudio create new R Project file work case study. Create new R Project file selecting File > New Project… menu bar. Select New Directory popup window. Next, select New Project. Pick meaningful name project folder, .e. Directory Name. Ensure project folder created right place. can change subdirectory clicking Browse…. subdirectory place Breast Cancer Mortality folder files just downloaded saved computer. Lastly, tick Open new session. open R Project new RStudio window. happy choices, can click Create Project. open new R Session, can start working case study. make sure project files case study properly loaded avoid potential errors running code, navigate project folder files Files/Plots/Packages/Help window bottom-right corner RStudio window go double-click file names open run files project.","code":""},{"path":"breast-cancer-mortality.html","id":"dependencies-1","chapter":"8 Case Study 2: Breast Cancer Mortality in Massachusetts","heading":"8.4.2 Dependencies","text":"Run lines code load R packages need throughout case study. code run , may need run install.packages(“package_name”).","code":"\n# Libraries - if this code does not run for you, you may need to run install.package(\"package_name\")\nlibrary(knitr)\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(cowplot)\nlibrary(tidycensus)\nlibrary(tigris)\noptions(tigris_use_cache = TRUE)\nlibrary(sf)\nlibrary(spdep)\nlibrary(viridis) \nlibrary(Hmisc)\nlibrary(fastDummies)\nlibrary(lme4)\nlibrary(INLA)\nlibrary(broom)"},{"path":"breast-cancer-mortality.html","id":"health-surveillance-data","chapter":"8 Case Study 2: Breast Cancer Mortality in Massachusetts","heading":"8.4.3 Health Surveillance Data","text":"Data breast cancer deaths obtained Massachusetts Department Public Health. mortality record included data decedent’s age, gender, racialized group (using U.S. census categories), residential address time death coded cause death following International Classification Diseases 10th Revision (ICD-10). employed R software Google Maps API geocode residential address case latitude longitude, used assign census tract city/town geocodes.mortality data aggregated individual observations death counts age, racialized group, census tract, city/town. get unrestricted mortality files government sources research, likely receive files one observation per death. geocoded observations, need aggregate level interest analysis. requires aggregation census tract level, also age groups can appropriate age standardization, sex groups racialized groups can stratify analyses groups, towns can explore second areal level analysis.code (example) turn raw mortality file provided , merging census tracts towns data, aggregating groups. vast majority area-based social metrics come U.S. Census, detailed level analysis available census tract (however case study, analyzing health outcome relatively small numbers cases, need perform analysis larger geography city/town instead census tract ensure sufficient number cases, especially inclusive different racialized groups, units analysis). can use tigris package download census tract geometries sf package link geocoded observations appropriate census tracts.Ideally performing analysis might include two levels smaller level (e.g., census tracts) nested entirely within larger level (e.g., towns). Massachusetts though, many towns small populations smaller census tracts. towns, created crosswalk wherein small towns combined create larger super towns. super towns make one census tract, super town analysis now least one census tract nested within . Please note analysis focusing breast cancer mortality women, designated MA cancer registry records.","code":"\n# the raw mortality data, with the geocoded coordinates converted into spatial feature (sf) data\nma_mort <- read_rds(\"data/08-breast-cancer-mortality/ma_mort_geo.RDS\") %>% \n  st_as_sf(coords=c(\"lon\",\"lat\"), crs = 4269)\n\n# Importing tract shapefile using tigris package\ntracts_sf <- tracts(state = \"25\", year = \"2010\")\n\n# Linking Census Tracts and Super Towns File\nsupertowns <- read_excel(\"data/08-breast-cancer-mortality/ma ct to supertinytowns.xlsx\", col_names = FALSE, col_types = \"text\") %>%\n  transmute(GEOID10 = ...1,\n            super_town = str_to_lower(...2)) %>% \n  mutate(super_town = case_when(GEOID10   == \"25023500101\" ~ \"hull\",\n                                super_town == \"stt1\"       ~ \"Super Town 1\",\n                                super_town == \"stt2\"       ~ \"Super Town 2\",\n                                super_town == \"stt3\"       ~ \"Super Town 3\",\n                                super_town == \"stt4\"       ~ \"Super Town 4\",\n                                super_town == \"stt5\"       ~ \"Super Town 5\",\n                                super_town == \"stt6\"       ~ \"Super Town 6\",\n                                super_town == \"stt7\"       ~ \"Super Town 7\",\n                                super_town == \"stt8\"       ~ \"Super Town 8\",\n                                super_town == \"stt9\"       ~ \"Super Town 9\",\n                                super_town == \"stt10\"      ~ \"Super Town 10\",\n                                super_town == \"stt11\"      ~ \"Super Town 11\",\n                                super_town == \"stt12\"      ~ \"Super Town 12\",\n                                super_town == \"stt13\"      ~ \"Super Town 13\",\n                                super_town == \"stt14\"      ~ \"Super Town 14\",\n                                super_town == \"stt15\"      ~ \"Super Town 15\",\n                                super_town == \"stt16\"      ~ \"Super Town 16\",\n                                super_town == \"stt17\"      ~ \"Super Town 17\",\n                                super_town == \"stt18\"      ~ \"Super Town 18\",\n                                super_town == \"stt19\"      ~ \"Super Town 19\",\n                                super_town == \"stt20\"      ~ \"Super Town 20\",\n                                super_town == \"stt21\"      ~ \"Super Town 21\",\n                                TRUE                       ~ super_town))\n\n\n\ntracts_sf <- inner_join(tracts_sf,supertowns, by = \"GEOID10\")\n\n#Creating a supertown-level map\ntown_geometry <- st_read(\"citytown_shp\",\n                         layer = \"TOWNSSURVEY_POLYM\") %>% \n  transmute(town = str_to_lower(TOWN),\n            geometry = geometry,\n            super_town = case_when(town %in% c(\"charlemont\", \"colrain\", \"hawley\", \"heath\", \"monroe\", \"rowe\")            ~ \"Super Town 1\",\n                                   town %in% c(\"cummington\", \"middlefield\", \"plainfield\", \"worthington\")                ~ \"Super Town 2\",\n                                   town %in% c(\"monterey\", \"tyringham\")                                                 ~ \"Super Town 3\",\n                                   town %in% c(\"hardwick\", \"new braintree\")                                             ~ \"Super Town 4\",\n                                   town %in% c(\"bernardston\", \"gill\", \"leyden\")                                         ~ \"Super Town 5\",\n                                   town %in% c(\"blandford\", \"chester\", \"granville\", \"montgomery\", \"russell\", \"tolland\") ~ \"Super Town 6\",\n                                   town %in% c(\"sunderland\", \"whateley\")                                                ~ \"Super Town 7\",\n                                   town %in% c(\"becket\", \"washington\")                                                  ~ \"Super Town 8\",\n                                   town %in% c(\"holland\", \"wales\")                                                      ~ \"Super Town 9\",\n                                   town %in% c(\"peru\", \"windsor\")                                                       ~ \"Super Town 10\",\n                                   town %in% c(\"goshen\", \"williamsburg\")                                                ~ \"Super Town 11\",\n                                   town %in% c(\"hancock\", \"new ashford\", \"richmond\")                                    ~ \"Super Town 12\",\n                                   town %in% c(\"leverett\", \"new salem\", \"shutesbury\")                                   ~ \"Super Town 13\",\n                                   town %in% c(\"sandisfield\", \"otis\")                                                   ~ \"Super Town 14\",\n                                   town %in% c(\"erving\", \"warwick\", \"wendell\")                                          ~ \"Super Town 15\",\n                                   town %in% c(\"buckland\", \"shelburne\")                                                 ~ \"Super Town 16\",\n                                   town %in% c(\"alford\", \"egremont\", \"mount washington\")                                ~ \"Super Town 17\",\n                                   town %in% c(\"ashfield\", \"conway\")                                                    ~ \"Super Town 18\",\n                                   town %in% c(\"aquinnah\", \"chilmark\", \"gosnold\", \"west tisbury\")                       ~ \"Super Town 19\",\n                                   town %in% c(\"petersham\", \"phillipston\")                                              ~ \"Super Town 20\",\n                                   town %in% c(\"florida\", \"savoy\")                                                      ~ \"Super Town 21\",\n                                   TRUE ~ town)) %>%\n  group_by(super_town) %>% \n  mutate(geometry = st_combine(geometry)) %>% \n  select(super_town, geometry) %>% \n  unique()\n\n# Merging to geocoded mortality and dropping geometries\n# The geometries of a shapefile can be very unweildy and slow data wrangling commands. Since we only need them to map, we will remove them for now\nma_mort_ct <- st_join(tracts_sf, ma_mort, left = FALSE) %>%\n  st_drop_geometry() %>%\n  arrange(id) %>%\n  select(id, year, date,\n         age, sex, starts_with(\"race\"), hisp, immigrant, starts_with(\"educ\"),\n         contains(\"icd10\"),\n         address, zip, starts_with(\"geo\"), super_town, contains(\"type\"), north, south, east, west,\n         GEOID10)\n\n# Aggregating the data by our selected variables of interest\n#Filtering to only breast cancer deaths\nma_mort_bc <- ma_mort_ct %>%\n  filter(str_detect(icd10, \"C50\"),\n         sex == \"Female\") %>% \n  mutate(age_cat = cut(age,\n                       breaks = c(-1,4,9,14,19,24,29,34,39,44,49,54,59,64,69,74,79,84,200),\n                       labels = c(\"0-4\",  \"5-9\",  \"10-14\",\"15-19\",\"20-24\",\n                                  \"25-29\",\"30-34\",\"35-39\",\"40-44\",\"45-49\",\n                                  \"50-54\",\"55-59\",\"60-64\",\"65-69\",\"70-74\",\n                                  \"75-79\",\"80-84\",\"85+\")),\n         race_group = racegroup) %>%\n  select(-c(age, race, immigrant, educ_years, contains(\"icd10\"),\n            contains(\"address\"), zip, geo_town, contains(\"type\"),\n            north, south, east, west, date, hisp)) %>%\n  group_by(year,\n           age_cat,\n           sex,\n           race_group,\n           super_town,\n           GEOID10) %>%\n  summarise(deaths = n()) %>%\n  ungroup() \n\ntown_ma_mort_bc <- ma_mort_bc %>% \n  group_by(super_town, year, age_cat, sex, race_group) %>% \n  summarise(deaths = sum(deaths, na.rm = TRUE)) %>% \n  ungroup()## # A tibble: 6 × 6\n##   super_town  year age_cat sex    race_group         deaths\n##   <chr>      <dbl> <fct>   <fct>  <fct>               <int>\n## 1 abington    2013 70-74   Female Non-Hispanic White      1\n## 2 abington    2014 75-79   Female Non-Hispanic White      1\n## 3 abington    2016 40-44   Female Non-Hispanic White      2\n## 4 abington    2016 50-54   Female Non-Hispanic White      1\n## 5 abington    2016 85+     Female Non-Hispanic White      1\n## 6 abington    2017 70-74   Female Non-Hispanic White      1"},{"path":"breast-cancer-mortality.html","id":"population-denominator-data-and-area-based-social-metric-data","chapter":"8 Case Study 2: Breast Cancer Mortality in Massachusetts","heading":"8.4.4 Population Denominator Data and Area-Based Social Metric Data","text":"download population data use denominators rates area-based social metrics (ABSMs) U.S. Census Bureau using tidycensus package. requires registering U.S. Census API key. key redacted , can get https://api.census.gov/data/key_signup.html. case study, using 2015-2019 American Community Survey (ACS) 5-year estimates counts population estimates constructing ABSMs.ABSMs use analysis percentage population poverty (information Census Bureau measures poverty, see https://www.census.gov/topics/income-poverty/poverty/guidance/poverty-measures.html) Index Concentration Extremes (ICE) racialized economic segregation (.e., race/ethnicity + income), measures extent area’s population concentrated extremes deprivation privilege. ICE measure racialized economic segregation scaled -1 1: value -1 means 100% population concentrated deprived group (analysis, conceptualized Black non-Hispanic population low-income households), value 1 means 100% population concentrated privileged group (analysis, conceptualized White non-Hispanic population high-income households). information formula ICE measure construction specific ICE variables, see Krieger et al., 2016. area-based social metrics (poverty racialized economic segregation) calculated city/town level.","code":"\n# Population Denominators\nma_demo_acs_bc <- vector(mode = \"list\", length = 5)\nnames(ma_demo_acs_bc) <- c(2013,2014,2015,2016,2017)\nfor (nm in names(ma_demo_acs_bc)) {\n  ma_demo_acs_bc[[nm]] <- get_acs(geography = \"tract\",\n                               # These are the myriad variables that make up the variables we intend to use\n                               variables = c(\"B01001_027\",\"B01001_028\",\"B01001_029\",\n                                             \"B01001_030\",\"B01001_031\",\"B01001_032\",\"B01001_033\",\n                                             \"B01001_034\",\"B01001_035\",\"B01001_036\",\"B01001_037\",\n                                             \"B01001_038\",\"B01001_039\",\"B01001_040\",\"B01001_041\",\n                                             \"B01001_042\",\"B01001_043\",\"B01001_044\",\"B01001_045\",\n                                             \"B01001_046\",\"B01001_047\", \"B01001_048\",\"B01001_049\",\n                                             \"B01001B_017\",\"B01001B_018\",\"B01001B_019\",\n                                             \"B01001B_020\",\"B01001B_021\",\"B01001B_022\",\"B01001B_023\",\n                                             \"B01001B_024\",\"B01001B_025\",\"B01001B_026\",\"B01001B_027\",\n                                             \"B01001B_028\",\"B01001B_029\",\"B01001B_030\",\"B01001B_031\",\n                                             \"B01001C_017\",\"B01001C_018\",\"B01001C_019\",\n                                             \"B01001C_020\",\"B01001C_021\",\"B01001C_022\",\"B01001C_023\",\n                                             \"B01001C_024\",\"B01001C_025\",\"B01001C_026\",\"B01001C_027\",\n                                             \"B01001C_028\",\"B01001C_029\",\"B01001C_030\",\"B01001C_031\",\n                                             \"B01001D_017\",\"B01001D_018\",\"B01001D_019\",\n                                             \"B01001D_020\",\"B01001D_021\",\"B01001D_022\",\"B01001D_023\",\n                                             \"B01001D_024\",\"B01001D_025\",\"B01001D_026\",\"B01001D_027\",\n                                             \"B01001D_028\",\"B01001D_029\",\"B01001D_030\",\"B01001D_031\",\n                                             \"B01001E_017\",\"B01001E_018\",\"B01001E_019\",\n                                             \"B01001E_020\",\"B01001E_021\",\"B01001E_022\",\"B01001E_023\",\n                                             \"B01001E_024\",\"B01001E_025\",\"B01001E_026\",\"B01001E_027\",\n                                             \"B01001E_028\",\"B01001E_029\",\"B01001E_030\",\"B01001E_031\",\n                                             \"B01001H_017\",\"B01001H_018\",\"B01001H_019\",\n                                             \"B01001H_020\",\"B01001H_021\",\"B01001H_022\",\"B01001H_023\",\n                                             \"B01001H_024\",\"B01001H_025\",\"B01001H_026\",\"B01001H_027\",\n                                             \"B01001H_028\",\"B01001H_029\",\"B01001H_030\",\"B01001H_031\",\n                                             \"B01001I_017\",\"B01001I_018\",\"B01001I_019\",\n                                             \"B01001I_020\",\"B01001I_021\",\"B01001I_022\",\"B01001I_023\",\n                                             \"B01001I_024\",\"B01001I_025\",\"B01001I_026\",\"B01001I_027\",\n                                             \"B01001I_028\",\"B01001I_029\",\"B01001I_030\",\"B01001I_031\"), \n                               year = as.numeric(nm) + 2,\n                               output = \"wide\",\n                               state = \"MA\",\n                               geometry = FALSE,\n                               moe_level = 95,\n                               survey = \"acs5\") %>%\n    rowwise() %>%\n    # Here, we will transform those variables into variables that align better with our outcome data\n    mutate(year = as.numeric(nm),\n           GEOID10 = GEOID,\n           # Total Female Age Groups\n           `total_00-04` = B01001_027E,\n           `total_05-09` = B01001_028E,\n           `total_10-14` = B01001_029E,\n           `total_15-19` = B01001_030E + B01001_031E,\n           `total_20-24` = B01001_032E + B01001_033E + B01001_034E,\n           `total_25-29` = B01001_035E,\n           `total_30-34` = B01001_036E,\n           `total_35-39` = B01001_037E,\n           `total_40-44` = B01001_038E,\n           `total_45-49` = B01001_039E,\n           `total_50-54` = B01001_040E,\n           `total_55-59` = B01001_041E,\n           `total_60-64` = B01001_042E + B01001_043E,\n           `total_65-69` = B01001_044E + B01001_045E,\n           `total_70-74` = B01001_046E,\n           `total_75-79` = B01001_047E,\n           `total_80-84` = B01001_048E,\n           `total_85+`   = B01001_049E,\n           total_total   = sum(c_across(starts_with(\"total\"))),\n           # Black Female Age Groups\n           `black_00-04` = B01001B_018E,\n           `black_05-09` = B01001B_019E,\n           `black_10-14` = B01001B_020E,\n           `black_15-19` = B01001B_021E + B01001B_022E,\n           `black_20-24` = B01001B_023E,\n           `black_25-29` = B01001B_024E,\n           `black_30-34` = B01001B_025E,\n           `black_35-44` = B01001B_026E,\n           `black_45-54` = B01001B_027E,\n           `black_55-64` = B01001B_028E,\n           `black_65-74` = B01001B_029E,\n           `black_75-84` = B01001B_030E,\n           `black_85+` = B01001B_031E,\n           black_total = sum(c_across(starts_with(\"black\"))),\n           # Native Female Age Groups\n           `native_00-04` = B01001C_018E,\n           `native_05-09` = B01001C_019E,\n           `native_10-14` = B01001C_020E,\n           `native_15-19` = B01001C_021E + B01001C_022E,\n           `native_20-24` = B01001C_023E,\n           `native_25-29` = B01001C_024E,\n           `native_30-34` = B01001C_025E,\n           `native_35-44` = B01001C_026E,\n           `native_45-54` = B01001C_027E,\n           `native_55-64` = B01001C_028E,\n           `native_65-74` = B01001C_029E,\n           `native_75-84` = B01001C_030E,\n           `native_85+` = B01001C_031E,\n           native_total = sum(c_across(starts_with(\"native\"))),\n           # Asian PI Female Age Groups\n           `asian_00-04` = B01001D_018E,\n           `asian_05-09` = B01001D_019E,\n           `asian_10-14` = B01001D_020E,\n           `asian_15-19` = B01001D_021E + B01001D_022E,\n           `asian_20-24` = B01001D_023E,\n           `asian_25-29` = B01001D_024E,\n           `asian_30-34` = B01001D_025E,\n           `asian_35-44` = B01001D_026E,\n           `asian_45-54` = B01001D_027E,\n           `asian_55-64` = B01001D_028E,\n           `asian_65-74` = B01001D_029E,\n           `asian_75-84` = B01001D_030E,\n           `asian_85+` = B01001D_031E,\n           asian_total = sum(c_across(starts_with(\"asian\"))),\n           # White Female Age Groups\n           `white_00-04` = B01001H_018E,\n           `white_05-09` = B01001H_019E,\n           `white_10-14` = B01001H_020E,\n           `white_15-19` = B01001H_021E + B01001H_022E,\n           `white_20-24` = B01001H_023E,\n           `white_25-29` = B01001H_024E,\n           `white_30-34` = B01001H_025E,\n           `white_35-44` = B01001H_026E,\n           `white_45-54` = B01001H_027E,\n           `white_55-64` = B01001H_028E,\n           `white_65-74` = B01001H_029E,\n           `white_75-84` = B01001H_030E,\n           `white_85+` = B01001H_031E,\n           white_total = sum(c_across(starts_with(\"white\"))),\n           # Hispanic Female Age Groups\n           `hisp_00-04` = B01001I_018E,\n           `hisp_05-09` = B01001I_019E,\n           `hisp_10-14` = B01001I_020E,\n           `hisp_15-19` = B01001I_021E + B01001I_022E,\n           `hisp_20-24` = B01001I_023E,\n           `hisp_25-29` = B01001I_024E,\n           `hisp_30-34` = B01001I_025E,\n           `hisp_35-44` = B01001I_026E,\n           `hisp_45-54` = B01001I_027E,\n           `hisp_55-64` = B01001I_028E,\n           `hisp_65-74` = B01001I_029E,\n           `hisp_75-84` = B01001I_030E,\n           `hisp_85+` = B01001I_031E,\n           hisp_total = sum(c_across(starts_with(\"hisp\")))) %>% \n    ungroup() %>%\n    select(-starts_with(\"B0\")) %>%\n    pivot_longer(cols      = c(starts_with(\"total\"),\n                               starts_with(\"hisp\"),\n                               starts_with(\"black\"),\n                               starts_with(\"native\"),\n                               starts_with(\"white\"),\n                               starts_with(\"asian\")),\n                 names_to  = c(\"race_group\", \"age_cat\"),\n                 names_sep = \"_\",\n                 values_to = \"population\") %>%\n    mutate(age_cat   = case_when(age_cat == \"00-04\" ~ \"0-4\",\n                                 age_cat == \"05-09\" ~ \"5-9\",\n                                 age_cat == \"total\" ~ \"Total\",\n                                 TRUE               ~ age_cat),\n           sex       = \"Female\",\n           race_group = case_when(race_group == \"white\"  ~ \"Non-Hispanic White\",\n                                 race_group == \"black\"  ~ \"Non-Hispanic Black\",\n                                 race_group == \"native\" ~ \"Non-Hispanic Native American, Alaskan Native, Other\",\n                                 race_group == \"asian\"  ~ \"Non-Hispanic Asian or Pacific Islander\",\n                                 race_group == \"hisp\"   ~ \"Hispanic\",\n                                 race_group == \"total\"  ~ \"Total\"))\n}\n# Combining all 5 years into one dataset\nma_demo_acs_bc <- rbind(ma_demo_acs_bc[[1]],\n                     ma_demo_acs_bc[[2]],\n                     ma_demo_acs_bc[[3]],\n                     ma_demo_acs_bc[[4]],\n                     ma_demo_acs_bc[[5]]) %>%\n  select(year, GEOID10, GEO_NAME = NAME, everything(), -GEOID)\n\nsaveRDS(ma_demo_acs_bc, file = \"ma_demo_acs_bc.rds\")\n\n#Agreggating to Super Town level\ntown_ma_demo_acs_bc <- ma_demo_acs_bc %>% \n  left_join(supertowns , by = \"GEOID10\") %>% \n  group_by(super_town, year, age_cat, sex, race_group) %>% \n  summarise(population = sum(population, na.rm = TRUE)) %>% \n  ungroup()## # A tibble: 6 × 6\n##   super_town  year age_cat sex    race_group                                          population\n##   <chr>      <dbl> <chr>   <chr>  <chr>                                                    <dbl>\n## 1 abington    2013 0-4     Female Hispanic                                                    45\n## 2 abington    2013 0-4     Female Non-Hispanic Asian or Pacific Islander                       0\n## 3 abington    2013 0-4     Female Non-Hispanic Black                                          45\n## 4 abington    2013 0-4     Female Non-Hispanic Native American, Alaskan Native, Other          0\n## 5 abington    2013 0-4     Female Non-Hispanic White                                         456\n## 6 abington    2013 0-4     Female Total                                                      529\n# Area-Based Social Metrics\nma_absm_acs <- vector(mode = \"list\", length = 5)\nnames(ma_absm_acs) <- c(2013,2014,2015,2016,2017)\nfor (nm in names(ma_absm_acs)) {\n  ma_absm_acs[[nm]] <- get_acs(geography = \"tract\",\n                               variables = c(\"B01003_001E\",\"B02001_001E\",\"B02001_002E\",\"B02001_003E\",\n                                             \"B02001_004E\",\"B02001_005E\",\"B02001_006E\",\"B02001_007E\",\n                                             \"B02001_008E\",\"B02001_009E\",\"B02001_010E\",\"B03001_001E\",\n                                             \"B03001_003\",\"B01001H_001E\"), \n                               year = as.numeric(nm) + 2,\n                               output = \"wide\",\n                               state = \"MA\",\n                               geometry = FALSE,\n                               moe_level = 95,\n                               survey = \"acs5\",\n                               cache_table = TRUE) %>%\n    # Transforming ACS variables into the ABSMs we want to use for our dataset\n    mutate(GEOID10 = GEOID,\n           percBlack = B02001_003E/B02001_001E,\n           percHisp = B03001_003E/B03001_001E,\n           pop_total = B01003_001E,\n           pop_white = B02001_002E,\n           pop_black = B02001_003E,\n           pop_amind = B02001_004E,\n           pop_api = B02001_005E + B02001_006E,\n           pop_hisp = B03001_003E,\n           pop_wnh = B01001H_001E,\n           percColor = (B01003_001E - B01001H_001E)/B01003_001E,\n           year = as.numeric(nm)) %>%\n    select(GEOID10, percBlack, percHisp, percColor, pop_total,\n           pop_white, pop_black, pop_amind, pop_api,\n           pop_hisp, pop_wnh, year)\n  \n  # Downloading tables from the ACS in addition to those variables\n\n    raw.white <- get_acs(geography=\"tract\", table=\"B19001A\", state=\"MA\", year= as.numeric(nm) + 2, \n                         output=\"wide\", cache_table=TRUE)\n    raw.black <- get_acs(geography=\"tract\", table=\"B19001B\", state=\"MA\", year= as.numeric(nm) + 2, \n                         output=\"wide\", cache_table=TRUE)\n    raw.all   <- get_acs(geography=\"tract\", table=\"B19001\",  state=\"MA\", year= as.numeric(nm) + 2, \n                         output=\"wide\", cache_table=TRUE)\n    raw.pov   <- get_acs(geography=\"tract\", table=\"B17001\",  state=\"MA\", year= as.numeric(nm) + 2, \n                         output=\"wide\", cache_table=TRUE)\n    raw.crowd <- get_acs(geography=\"tract\", table=\"B25014\",  state=\"MA\", year= as.numeric(nm) + 2, \n                         output=\"wide\", cache_table=TRUE)\n    raw.wnh   <- get_acs(geography=\"tract\", table=\"B19001H\", state=\"MA\", year= as.numeric(nm) + 2, \n                         output=\"wide\", cache_table=TRUE)\n    \n    d.merge <- left_join(raw.white, raw.black, by=\"GEOID\") %>%\n      left_join(raw.wnh, by=\"GEOID\") %>%\n      left_join(raw.all, by=\"GEOID\") %>%\n      left_join(raw.pov, by=\"GEOID\") %>%\n      left_join(raw.crowd, by=\"GEOID\") %>%\n      select(c(\"GEOID\",\"B19001_001E\",\"B19001_002E\",\"B19001_003E\",\"B19001_004E\",\"B19001_005E\",\n               \"B19001_014E\",\"B19001_015E\",\"B19001_016E\",\"B19001_017E\",\"B19001A_014E\",\n               \"B19001A_015E\",\"B19001A_016E\",\"B19001A_017E\",\"B19001B_002E\",\"B19001B_003E\",\n               \"B19001B_004E\",\"B19001B_005E\",\"B19001H_002E\",\"B19001H_003E\",\"B19001H_004E\",\n               \"B19001H_005E\",\"B19001H_014E\",\"B19001H_015E\",\"B19001H_016E\",\"B19001H_017E\",\n               \"B17001_001E\",\"B17001_002E\",\"B25014_001E\", \"B25014_005E\",\"B25014_006E\",\n               \"B25014_007E\",\"B25014_011E\",\"B25014_012E\",\"B25014_013E\")) %>% \n      mutate(ICEwbinc=((B19001A_014E + B19001A_015E + B19001A_016E + B19001A_017E) -\n                         (B19001B_002E + B19001B_003E + B19001B_004E + B19001B_005E))/B19001_001E,\n             ICEwnhinc=((B19001H_014E + B19001H_015E + B19001H_016E + B19001H_017E) -\n                          (B19001_002E + B19001_003E + B19001_004E + B19001_005E -\n                             B19001H_002E - B19001H_003E - B19001H_004E - B19001H_005E))/B19001_001E,\n             ice_wnh_highinc= (B19001H_014E + B19001H_015E + B19001H_016E + B19001H_017E)/B19001_001E,\n             ice_poc_lowinc = (B19001_002E + B19001_003E + B19001_004E + B19001_005E -\n                                 B19001H_002E - B19001H_003E - B19001H_004E - B19001H_005E)/B19001_001E,\n             ice_wnh_lowinc = (B19001H_002E + B19001H_003E + B19001H_004E + B19001H_005E)/B19001_001E,\n             ice_poc_highinc = (B19001_014E + B19001_015E + B19001_016E + B19001_017E -\n                                  B19001H_014E - B19001H_015E - B19001H_016E - B19001H_017E)/B19001_001E,\n             tractPov = B17001_002E/B17001_001E,\n             tractCrowd = (B25014_005E + B25014_006E + B25014_007E +\n                             B25014_011E + B25014_012E + B25014_013E) / B25014_001E,\n             tractSevereCrowd = (B25014_006E + B25014_007E +\n                                   B25014_012E + B25014_013E) / B25014_001E,\n             year = as.numeric(nm)) %>%\n     select(GEOID10 = GEOID, ICEwbinc, ICEwnhinc, tractPov, tractCrowd, tractSevereCrowd, year,\n            ice_wnh_highinc, ice_wnh_lowinc, ice_poc_lowinc, ice_poc_highinc)\n    \n    ma_absm_acs[[nm]] <- left_join(ma_absm_acs[[nm]], d.merge, by=c(\"GEOID\",\"year\"))\n}\n\nma_absm_acs <- rbind(ma_absm_acs[[1]],\n                     ma_absm_acs[[2]],\n                     ma_absm_acs[[3]],\n                     ma_absm_acs[[4]],\n                     ma_absm_acs[[5]]) %>%\n  select(year, GEOID10, everything())\n\n# Creating a dataset for visualization and analysis from the weighted ABSM over all 5 years\nma_absm_sum <- ma_absm_acs %>% \n  group_by(GEOID10) %>% \n  mutate(wt = pop_total / sum(pop_total, na.rm = TRUE)) %>% \n  summarise(percBlack        = mean(percBlack, wt = wt, na.rm = TRUE),\n            percHisp         = mean(percHisp,  wt = wt, na.rm = TRUE),\n            percColor        = mean(percColor, wt = wt, na.rm = TRUE),\n            ICEwbinc         = mean(ICEwbinc, wt = wt, na.rm = TRUE),\n            ICEwnhinc        = mean(ICEwnhinc, wt = wt, na.rm = TRUE),\n            tractPov         = mean(tractPov, wt = wt, na.rm = TRUE),\n            tractCrowd       = mean(tractCrowd, wt = wt, na.rm = TRUE),\n            tractSevereCrowd = mean(tractSevereCrowd, wt = wt, na.rm = TRUE),\n            ice_wnh_highinc  = mean(ice_wnh_highinc, wt = wt, na.rm = TRUE),\n            ice_wnh_lowinc   = mean(ice_wnh_lowinc, wt = wt, na.rm = TRUE),\n            ice_poc_lowinc   = mean(ice_poc_lowinc, wt = wt, na.rm = TRUE),\n            ice_poc_highinc  = mean(ice_poc_highinc, wt = wt, na.rm = TRUE),\n            pop_total        = sum(pop_total, na.rm = TRUE)) %>% \n  mutate(pov_cat=factor(case_when(0<=tractPov  & tractPov<0.05 ~ \"0-4.9%\",\n                                   0.05<=tractPov & tractPov<0.10 ~ \"5-9.9%\",\n                                   0.10<=tractPov & tractPov<0.2 ~ \"10-19.9%\",\n                                   0.20<=tractPov & tractPov<=1 ~ \"20-100%\",\n                                   TRUE ~ NA_character_), \n                         levels=c(\"0-4.9%\",\"5-9.9%\",\"10-19.9%\",\"20-100%\")),\n         pov_qt=cut(tractPov, wtd.quantile(tractPov, weights=pop_total,\n                                            probs=c(0,0.2,0.4,0.6,0.8,1)),\n                     na.rm=TRUE, \n                     include.lowest=T),\n         ICE_qt=cut(ICEwnhinc, wtd.quantile(ICEwnhinc, weights=pop_total,\n                                                probs=c(0,0.2,0.4,0.6,0.8,1)),\n                        na.rm=TRUE, \n                        include.lowest=T),\n         perc_Color_qt = cut(percColor, wtd.quantile(percColor, weights=pop_total,\n                                                  probs=c(0,0.2,0.4,0.6,0.8,1)),\n                          na.rm=TRUE, \n                          include.lowest=T),\n         perc_BLACK_qt = cut(percBlack, wtd.quantile(percBlack, weights=pop_total,\n                                                  probs=c(0,0.2,0.4,0.6,0.8,1)),\n                          na.rm=TRUE, \n                          include.lowest=T))\n\ntown_ma_absm_sum <- ma_absm_acs %>% \n  left_join(supertowns , by = \"GEOID10\") %>% \n  group_by(super_town) %>% \n  mutate(wt = pop_total / sum(pop_total, na.rm = TRUE)) %>% \n  summarise(percBlack        = mean(percBlack, wt = wt, na.rm = TRUE),\n            percHisp         = mean(percHisp,  wt = wt, na.rm = TRUE),\n            percColor        = mean(percColor, wt = wt, na.rm = TRUE),\n            ICEwbinc         = mean(ICEwbinc, wt = wt, na.rm = TRUE),\n            ICEwnhinc        = mean(ICEwnhinc, wt = wt, na.rm = TRUE),\n            tractPov         = mean(tractPov, wt = wt, na.rm = TRUE),\n            tractCrowd       = mean(tractCrowd, wt = wt, na.rm = TRUE),\n            tractSevereCrowd = mean(tractSevereCrowd, wt = wt, na.rm = TRUE),\n            ice_wnh_highinc  = mean(ice_wnh_highinc, wt = wt, na.rm = TRUE),\n            ice_wnh_lowinc   = mean(ice_wnh_lowinc, wt = wt, na.rm = TRUE),\n            ice_poc_lowinc   = mean(ice_poc_lowinc, wt = wt, na.rm = TRUE),\n            ice_poc_highinc  = mean(ice_poc_highinc, wt = wt, na.rm = TRUE),\n            pop_total        = sum(pop_total, na.rm = TRUE)) %>% \n  mutate(pov_cat=factor(case_when(0<=tractPov  & tractPov<0.05 ~ \"0-4.9%\",\n                                   0.05<=tractPov & tractPov<0.10 ~ \"5-9.9%\",\n                                   0.10<=tractPov & tractPov<0.2 ~ \"10-19.9%\",\n                                   0.20<=tractPov & tractPov<=1 ~ \"20-100%\",\n                                   TRUE ~ NA_character_), \n                         levels=c(\"0-4.9%\",\"5-9.9%\",\"10-19.9%\",\"20-100%\")),\n         pov_qt=cut(tractPov, wtd.quantile(tractPov, weights=pop_total,\n                                            probs=c(0,0.2,0.4,0.6,0.8,1)),\n                     na.rm=TRUE, \n                     include.lowest=T),\n         ICE_qt=cut(ICEwnhinc, wtd.quantile(ICEwnhinc, weights=pop_total,\n                                                probs=c(0,0.2,0.4,0.6,0.8,1)),\n                        na.rm=TRUE, \n                        include.lowest=T),\n         perc_Color_qt = cut(percColor, wtd.quantile(percColor, weights=pop_total,\n                                                  probs=c(0,0.2,0.4,0.6,0.8,1)),\n                          na.rm=TRUE, \n                          include.lowest=T),\n         perc_BLACK_qt = cut(percBlack, wtd.quantile(percBlack, weights=pop_total,\n                                                  probs=c(0,0.2,0.4,0.6,0.8,1)),\n                          na.rm=TRUE, \n                          include.lowest=T)) %>% \n  ungroup()## # A tibble: 6 × 19\n##   super_town percBlack percHisp percColor ICEwbinc ICEwnhinc tractPov tractCrowd tractSevereCrowd\n##   <chr>          <dbl>    <dbl>     <dbl>    <dbl>     <dbl>    <dbl>      <dbl>            <dbl>\n## 1 abington    0.0243     0.0232    0.101     0.378     0.373   0.0384    0.0120          0.00224 \n## 2 acton       0.0155     0.0236    0.295     0.465     0.448   0.0307    0.00933         0.00130 \n## 3 acushnet    0.000574   0.0331    0.0926    0.309     0.303   0.0546    0.0380          0.00675 \n## 4 adams       0.00715    0.0121    0.0379    0.183     0.179   0.104     0.0103          0.00124 \n## 5 agawam      0.0155     0.0609    0.117     0.300     0.271   0.0822    0.0126          0.00606 \n## 6 amesbury    0.00675    0.0239    0.0668    0.368     0.356   0.0552    0.0111          0.000323\n## # … with 10 more variables: ice_wnh_highinc <dbl>, ice_wnh_lowinc <dbl>, ice_poc_lowinc <dbl>,\n## #   ice_poc_highinc <dbl>, pop_total <dbl>, pov_cat <fct>, pov_qt <fct>, ICE_qt <fct>, perc_Color_qt <fct>,\n## #   perc_BLACK_qt <fct>"},{"path":"breast-cancer-mortality.html","id":"approach-1","chapter":"8 Case Study 2: Breast Cancer Mortality in Massachusetts","heading":"8.5 Approach","text":"Now data, let’s revisit questions interest:\n1. overall socioeconomic gradient breast cancer mortality?racialized disparity breast cancer mortality overall?racialized disparity breast cancer mortality overall?area-based socioeconomic measures interact individual-level membership racialized groups affect patterns breast cancer mortality (.e., interactions socioeconomic position racialized groups, just socioeconomic inequities within racialized groups)?area-based socioeconomic measures interact individual-level membership racialized groups affect patterns breast cancer mortality (.e., interactions socioeconomic position racialized groups, just socioeconomic inequities within racialized groups)?","code":""},{"path":"breast-cancer-mortality.html","id":"question-1-what-is-the-overall-socioeconomic-gradient-in-breast-cancer-mortality","chapter":"8 Case Study 2: Breast Cancer Mortality in Massachusetts","heading":"8.5.1 Question 1: What is the overall socioeconomic gradient in breast cancer mortality?","text":"First, let’s just look spatial distribution area-based socioeconomic measure poverty constructed: percentage city/town population poverty. map percentage population poverty Massachusetts city/town poverty quantile years 2013-2017.Now, let’s look spatial distribution area-based socioeconomic measure constructed: ICE measure racialized economic segregation. Compare map city/town ICE (racialized group + income) quantile first map city/town poverty quantile. notice differences? find one area-based socioeconomic measures visually clear compelling , ?Next, look breast cancer mortality varies across ICE measure racialized economic segregation. Using 2015-2019 American Community Survey (ACS) 5-year estimates counts, compute age- sex-standardized 2013-2017 breast cancer mortality rates (per 1000) MA city/town ICE quantile using direct method. method require reference population, downloaded (wrangled) National Cancer Institute. (https://seer.cancer.gov/stdpopulations/).can see clear socioeconomic gradient breast cancer mortality rates city/town level Massachusetts years 2013-2017. interpret socioeconomic gradient? expected, ?","code":"\nma_poverty_map <- town_ma_absm_sum %>% \n  left_join(town_geometry, by= \"super_town\") %>% \n  ggplot() +\n  geom_sf(mapping = aes(geometry=geometry, \n                        fill=pov_qt),\n          lwd = 0.1,\n          color = \"black\") +\n  scale_fill_viridis_d() +\n  labs(title = expression(atop(\"Percentage of population below poverty\", \"by MA city/town, 2013-2017\")),\n       caption = \"5-year ACS files from end-years 2015-2019\",\n       fill = \"Poverty quantile\", x=\"\", y=\"\") +\n  theme_void() +\n  theme(axis.text.x=element_blank(), #remove x axis labels\n        axis.ticks.x=element_blank(), #remove x axis ticks\n        axis.text.y=element_blank(),  #remove y axis labels\n        axis.ticks.y=element_blank(),\n        legend.position = c(0.25, 0.25),\n        legend.key.size = unit(0.4, \"cm\"))\n\nggsave(\"ma_poverty_map.png\")\nma_ice_map <- town_ma_absm_sum %>% \n  left_join(town_geometry, by= \"super_town\") %>% \n  ggplot() +\n  geom_sf(mapping = aes(geometry=geometry, \n                        fill=ICE_qt),\n          lwd = 0.1,\n          color = \"black\") +\n  scale_fill_viridis_d(option = \"E\") +\n  labs(title = expression(atop(\"Index of Concentration at the Extremes for Racialized Economic Segregation\", \"by MA city/town, 2013-2017\")),\n       caption = \"5-year ACS files from end-years 2015-2019\",\n       fill = \"ICE quantile\", x=\"\", y=\"\") +\n  theme_void() +\n  theme(axis.text.x=element_blank(), #remove x axis labels\n        axis.ticks.x=element_blank(), #remove x axis ticks\n        axis.text.y=element_blank(),  #remove y axis labels\n        axis.ticks.y=element_blank(),\n        legend.position = c(0.25, 0.25),\n        legend.key.size = unit(0.4, \"cm\"))\n\nggsave(\"ma_ice_map.png\")## # A tibble: 13 × 3\n##    age_cat    pop     wt\n##    <chr>    <dbl>  <dbl>\n##  1 0-4      69135 0.0691\n##  2 10-14    73032 0.0730\n##  3 15-19    72169 0.0722\n##  4 20-24    66478 0.0665\n##  5 25-29    64529 0.0645\n##  6 30-34    71044 0.0710\n##  7 35-44   162613 0.163 \n##  8 45-54   134834 0.135 \n##  9 5-9      72533 0.0725\n## 10 55-64    87247 0.0872\n## 11 65-74    66037 0.0660\n## 12 75-84    44841 0.0448\n## 13 85+      15508 0.0155\nma_mort_ice_qt <- town_ma_mort_bc %>%\n  inner_join(town_ma_demo_acs_bc, by = c(\"year\",\"super_town\",\"race_group\",\"sex\",\"age_cat\")) %>% \n  left_join(town_ma_absm_sum, by= \"super_town\") %>% \n  filter(age_cat != \"Total\") %>% \n  group_by(ICE_qt, age_cat) %>% \n  summarise(num = sum(deaths, na.rm=T),\n            den = sum(population, na.rm=T))%>%\n  mutate(den = den + 0.001) %>% \n  left_join(seer_std, by=\"age_cat\") %>%\n  mutate(rate_i = wt*num/den,\n         var_rate_i = (num*wt^2)/den^2) %>%\n  group_by(ICE_qt) %>%\n  summarise(num = sum(num, na.rm=T),\n            den = sum(den, na.rm=T),\n            std_rate = sum(rate_i, na.rm=T),\n            var_std_rate = sum(var_rate_i, na.rm=T),\n            sumwt = sum(wt),\n            sumwt2 = sum(wt^2)) %>%\n  mutate(std_rate = std_rate / sumwt *1000,\n         var_std_rate = var_std_rate / sumwt2 *1000,\n         std_rate_lo95 = std_rate - 1.96*sqrt(var_std_rate),\n         std_rate_up95 = std_rate + 1.96*sqrt(var_std_rate)) %>%\n  ungroup() %>% \n  ggplot(aes(x=ICE_qt, y=std_rate)) +\n    geom_bar(stat=\"identity\", fill = \"limegreen\",color = \"black\") +\n    labs(title = expression(atop(\"Age-standardized breast cancer mortality rate by MA city/town\",\"ICE for racialized group & income quantile, 2013-2017\")),\n         caption = expression(atop(\"Source: Massachusetts Mortality Data 2013-2017\", \n                                   \"5-year ACS files from end-years 2015-2019\"))) +\n    ylab(\"Breast cancer mortality per 1000\") +\n    xlab(\"ICE quantile\") +\n    theme(legend.position=\"bottom\") +\n    theme_minimal()\n\n\nggsave(\"ma_mort_ice_qt.png\")"},{"path":"breast-cancer-mortality.html","id":"question-2-what-is-the-racialized-disparity-in-breast-cancer-mortality-overall","chapter":"8 Case Study 2: Breast Cancer Mortality in Massachusetts","heading":"8.5.2 Question 2: What is the racialized disparity in breast cancer mortality overall?","text":"can describe inequities racialized groups much way describe inequities ABSMs: aggregating death population data. ’ve already got data aggregated, can stratify analysis compare Black non-Hispanic White non-Hispanic populations. Note: Throughout case study may refer groups Black White shorthand (see footnote preface regarding capitalization conventions used throughout manual).first question, age-adjust aggregated data racialized group compare standardized mortality rates Black White populations. can see lots variation mortality rates increase - causing ?explore variation, reminder, age-adjust aggregated data racialized group can compare standardized mortality rates, differences breast cancer mortality two groups may due differential distribution ages racialized group. can see age distributions racialized group quite different. describe differences?Now, return question see lots variation mortality rates. might expect, extremely variable mortality rates occurring small populations. maps look like?seeing really extreme rates Black population, smaller overall sample sizes. also seeing many cities towns recorded breast cancer deaths Black White populations, particularly case Black population (likely cities towns small resident Black populations due racialized economic segregation). else observe?can also display two maps one, calculating rate difference, rate ratio. calculate display incidence rate ratio comparing age-standardized mortality rates Black non-Hispanic White non-Hispanic populations, visualized start section.can also map like maps.Note boundaries cities towns completely blank. areas recorded breast cancer deaths Black White women don’t get visualized literally data. Cities towns visualized, white “empty,” data White women. , see handful cities towns recorded breast cancer deaths Black women, cities towns get visualized white “empty” IRR can’t calculated. Key understanding rates influenced small sample sizes large potential errors previously visualized. ’s researchers communities interpret “real” effects see .","code":"\nma_mort_stratified <- town_ma_mort_bc %>%\n  inner_join(town_ma_demo_acs_bc, by = c(\"year\",\"super_town\",\"race_group\",\"sex\",\"age_cat\"))%>% \n  filter(race_group %in% c(\"Non-Hispanic White\",\"Non-Hispanic Black\"),\n         age_cat != \"Total\") %>% \n  group_by(super_town, race_group, age_cat) %>% \n  summarise(num = sum(deaths, na.rm=T),\n            den = sum(population, na.rm=T))%>%\n  mutate(den = den + 0.001) %>% \n  left_join(seer_std, by=\"age_cat\") %>%\n  mutate(rate_i = wt*num/den,\n         var_rate_i = (num*wt^2)/den^2) %>%\n  group_by(super_town, race_group) %>%\n  summarise(num = sum(num, na.rm=T),\n            den = sum(den, na.rm=T),\n            std_rate = sum(rate_i, na.rm=T),\n            var_std_rate = sum(var_rate_i, na.rm=T),\n            sumwt = sum(wt),\n            sumwt2 = sum(wt^2)) %>%\n  mutate(std_rate = std_rate / sumwt *1000,\n         var_std_rate = var_std_rate / sumwt2 *1000,\n         std_rate_lo95 = std_rate - 1.96*sqrt(var_std_rate),\n         std_rate_up95 = std_rate + 1.96*sqrt(var_std_rate)) %>%\n  ungroup()\n\nbc_dotplot_agg_rates <- ma_mort_stratified %>% \n  filter(!is.na(std_rate),\n         den > 0.01) %>% \n  select(super_town,race_group, contains(\"std_rate\")) %>% \n  arrange(std_rate) %>% \n  group_by(race_group) %>% \n  mutate(orderID = row_number()) %>%\n  ungroup() %>% \n  ggplot(aes(x=orderID, y=std_rate)) + \n    geom_point(color = \"limegreen\", alpha = 0.8, size = 1) +\n    geom_errorbar(aes(ymin = std_rate_lo95, ymax=std_rate_up95), size = 0.1) +\n    labs(title = \"Age-standardized breast cancer mortality rate\",\n         caption = expression(atop(\"Source: Massachusetts Mortality Data 2013-2017\", \n                                   \"5-year ACS files from end-years 2015-2019\"))) +\n    xlab(\"City/towns\") +\n    ylab(\"Mortality rates\") +\n    facet_wrap(vars(race_group), ncol = 2, scales = \"free_x\") +\n    theme_minimal() +\n    theme(axis.ticks.x = element_blank(), \n          axis.text.x = element_blank())\n\nggsave(\"bc_dotplot_agg_rates.png\")\nbc_tab_age_race <- town_ma_demo_acs_bc %>%\n      filter(race_group %in% c(\"Non-Hispanic White\",\"Non-Hispanic Black\"),\n         sex == \"Female\",\n         age_cat != \"Total\") %>% \n  group_by(super_town, race_group, sex, age_cat) %>% \n  summarise(pop = sum(population, na.rm=T)) %>%\n  inner_join(town_ma_absm_sum, by = c(\"super_town\")) %>% \n  mutate(age_cat_broad = case_when(age_cat %in% c(\"0-4\",\"5-9\",\"10-14\",\"15-19\",\"20-24\") ~ \"0-24\",\n                                   age_cat %in% c(\"25-29\",\"30-34\",\"35-39\",\"35-44\",\"40-44\") ~ \"25-44\",\n                                   age_cat %in% c(\"45-49\",\"45-54\",\"50-54\",\"55-59\",\"55-64\",\"60-64\") ~ \"45-64\",\n                                   age_cat %in% c(\"65-69\",\"65-74\",\"70-74\",\"75-59\",\"75-84\",\"80-84\",\"85+\") ~ \"65+\")) %>%\n  group_by(race_group, age_cat_broad) %>%\n  summarise(pop = sum(pop, na.rm=T)) %>%\n  group_by(race_group) %>%\n  mutate(percentage = pop/sum(pop)) %>% \n  ggplot(aes(x=race_group, y=percentage, fill= age_cat_broad)) +\n    geom_bar(position=\"stack\", stat=\"identity\") +\n    scale_fill_viridis_d(option = \"A\") +\n    labs(title =\"Age distribution by racialized group\",\n         fill = \"Age category\",\n         caption = \"5-year ACS files from end-years 2015-2019\") +\n    ylab(\"Percentage of population\") +\n    xlab(\"Racialized group\") +\n    theme(legend.position=\"bottom\") +\n    theme_minimal()\n    \nggsave(\"bc_tab_age_race.png\")\ninclude_graphics(\"images/08-breast-cancer-mortality/bc_tab_age_race.png\")\nplotlist <- vector(mode = \"list\", length = 2)\nnames(plotlist) <- c(\"Non-Hispanic White\",\"Non-Hispanic Black\")\nfor (plt in names(plotlist)) {\n\n  map.state <- ma_mort_stratified %>% \n    filter(race_group == plt) %>%\n    mutate(std_rate = ifelse(std_rate > 10, 10, std_rate)) %>% \n    left_join(town_geometry, by= \"super_town\") %>% \n    ggplot() +\n      geom_sf(mapping = aes(geometry = geometry,\n                            fill = std_rate),\n              lwd = 0.1,\n              color = \"black\") +\n      scale_fill_viridis(option = \"B\", limits = c (0, 10)) +\n      labs(title = plt,\n           fill = \"Age Standardized Mortality Rate\", x=\"\", y=\"\") +\n      theme_void() +\n      theme(axis.text.x=element_blank(), #remove x axis labels\n            axis.ticks.x=element_blank(), #remove x axis ticks\n            axis.text.y=element_blank(),  #remove y axis labels\n            axis.ticks.y=element_blank(),\n            legend.position = c(0.25, 0.25),\n            legend.key.size = unit(0.2, \"cm\"))\n  \n  map.boston <- ma_mort_stratified %>% \n    filter(race_group == plt,\n           super_town == \"boston\") %>% \n    mutate(std_rate = ifelse(std_rate > 10, 10, std_rate)) %>% \n    left_join(town_geometry, by= \"super_town\") %>% \n    ggplot() +\n      geom_sf(mapping = aes(geometry = geometry,\n                            fill = std_rate),\n              lwd = 0) +\n      scale_fill_viridis(option = \"B\", limits = c (0, 10)) +\n      labs(title = \"Boston\") +\n      theme_void() + \n      theme(strip.text.x = element_blank(),\n            legend.position = \"None\",\n            plot.title      = element_text(hjust = 0.5)) \n  \n  plotlist[[plt]] <- ggdraw() +\n    draw_plot(map.state , x = 0.00, y = 0.00, width = 0.80, height = 1.00) +\n    draw_plot(map.boston, x = 0.65, y = 0.50, width = 0.30, height = 0.30)\n  \n}\n\ntitle <- ggdraw() +\n  draw_label(\"Age-standardized breast cancer mortality rates\", size = 12, fontface='bold', hjust = 0.2)\ncaption1 <- ggdraw() +\n  draw_label(\"Source: Massachusetts Mortality Data 2013-2017\", size = 8, hjust = 0.5) \ncaption2 <- ggdraw() +\n  draw_label(\"5-year ACS files from end-years 2015-2019\", size = 8, hjust = 0.5) \nplots <- plot_grid(plotlist = plotlist,\n                  ncol = 1)\n\nbc_byrace_std_rates <- plot_grid(title, plots, caption1, caption2,\n          rel_heights = c(0.05, 1.0, 0.05, 0.05),\n          ncol = 1)\n\n\nggsave(\"bc_byrace_std_rates.png\")\nirr_data <- ma_mort_stratified %>% \n  select(super_town,race_group, std_rate, var_std_rate) %>% \n  pivot_wider(id_cols = super_town,\n              names_from = race_group,\n              values_from = c(std_rate, var_std_rate)) %>% \n  mutate(irr = ifelse(`std_rate_Non-Hispanic White` == 0, NA_real_, `std_rate_Non-Hispanic Black` / `std_rate_Non-Hispanic White`),\n         irr_var = `var_std_rate_Non-Hispanic Black` + `var_std_rate_Non-Hispanic White`,\n         irr_lo95 = irr - 1.96*sqrt(irr_var),\n         irr_up95 = irr + 1.96*sqrt(irr_var)) \n\nirr_plot <- irr_data %>% \n  arrange(irr) %>% \n  filter(!is.na(irr)) %>% \n  mutate(orderID = row_number()) %>%\n  ungroup() %>% \n  ggplot(aes(x=orderID, y=irr)) + \n    geom_point(color = \"limegreen\", alpha = 0.8, size = 1) +\n    geom_errorbar(aes(ymin = irr_lo95, ymax=irr_up95), size = 0.1) +\n    labs(title = expression(atop(\"Incidence Rate Ratio Comparing Non-Hispanic Black\", \n                                 \"and Non Hispanic White Populations\")),\n         caption = expression(atop(\"Source: Massachusetts Mortality Data 2013-2017\",\n                                   \"5-Year ACS files from end-years 2015-2019\"))) +\n         xlab(\"Census Tracts\") +\n         ylab(\"Incidence Rate Ratio\") +\n    theme_minimal() \n  \nirr_plot\nmap.irr.bc <- irr_data %>% \n  mutate(irr = case_when(irr > 4  ~ 4,\n                         TRUE     ~ irr)) %>% \n  left_join(town_geometry, by= \"super_town\") %>% \n  ggplot() +\n    geom_sf(mapping = aes(geometry = geometry,\n                          fill = irr),\n            lwd = 0.1) +\n  scale_fill_viridis(option = \"E\",\n                     trans = scales::pseudo_log_trans(sigma=0.01),\n                     limits = exp(c(-1,1)*log(4)),\n                     breaks=c(0.25, 0.5,1,2,4),\n                     na.value = \"white\") +\n  labs(title = expression(atop(\"Incidence Rate Ratio Comparing Non-Hispanic Black\", \n                                 \"and Non Hispanic White Populations\")),\n       caption = expression(atop(\"Source: Massachusetts Mortality Data 2013-2017\", \n                                 \"5-Year ACS files from end-years 2015-2019\")),\n       fill = \"IRR\", x=\"\", y=\"\") +\n  theme_void() +\n    theme(legend.position = c(0.25, 0.25),\n          legend.key.size = unit(0.3, \"cm\"),\n          plot.title      = element_text(hjust = 0.1),\n          plot.subtitle   = element_text(hjust = 0.1))\n\nmap.boston.irr <- irr_data %>% \n  filter(super_town == \"boston\") %>% \n  mutate(irr = case_when(irr > 4  ~ 4,\n                         irr == 0 ~ 0.0000001,\n                         TRUE     ~ irr)) %>% \n  left_join(town_geometry, by= \"super_town\") %>% \n  ggplot() +\n    geom_sf(mapping = aes(geometry = geometry,\n                          fill = irr),\n            lwd = 0.1) +\n  scale_fill_viridis(option = \"E\",\n                     trans = scales::pseudo_log_trans(sigma=0.01),\n                     limits = exp(c(-1,1)*log(4)),\n                     breaks=c(0.25, 0.5,1,2,4),\n                     na.value = \"white\") +\n    labs(title = \"Boston\") +\n    theme_void() + \n    theme(strip.text.x = element_blank(),\n          legend.position = \"None\",\n          plot.title      = element_text(hjust = 0.5)) \n\nggsave(\"map.irr.bc.png\")"},{"path":"breast-cancer-mortality.html","id":"question-3-how-do-area-based-socioeconomic-measures-interact-with-individual-level-membership-in-racialized-groups-to-affect-patterns-of-breast-cancer-mortality-i.e.-interactions-between-socioeconomic-position-and-racialized-groups-not-just-socioeconomic-inequities-within-racialized-groups","chapter":"8 Case Study 2: Breast Cancer Mortality in Massachusetts","heading":"8.5.3 Question 3: How do area-based socioeconomic measures interact with individual-level membership in racialized groups to affect patterns of breast cancer mortality (i.e., interactions between socioeconomic position and racialized groups, not just socioeconomic inequities within racialized groups)?","text":"course, now can map value well.adjusted model allow us see poverty rates impacting incidence rate ratio.now map.understand statistically city/town poverty level impacting relationship racialized groups breast cancer mortality, can compare models without poverty. can see impact social membership racialized group reduced presence city/town level poverty.","code":"\npoisson_data <- town_ma_mort_bc %>%\n  inner_join(town_ma_demo_acs_bc, by = c(\"year\",\"super_town\",\"race_group\",\"sex\",\"age_cat\")) %>%\n  filter(race_group %in% c(\"Non-Hispanic White\",\"Non-Hispanic Black\"),\n         age_cat != \"Total\") %>% \n  group_by(super_town, race_group, age_cat) %>% \n  summarise(num = sum(deaths, na.rm=T),\n            den = sum(population, na.rm=T))%>%\n  mutate(den = den + 0.001,\n         race_group = factor(race_group)) %>%\n  inner_join(town_ma_absm_sum, by = c(\"super_town\")) %>% \n  fastDummies::dummy_cols(select_columns=c(\"pov_cat\", \"ICE_qt\")) %>%\n  rename(pov_cat_1 = \"pov_cat_0-4.9%\",\n         pov_cat_2 = \"pov_cat_5-9.9%\",\n         pov_cat_3 = \"pov_cat_10-19.9%\",\n         pov_cat_4 = \"pov_cat_20-100%\",\n         ICE_qt_1 = \"ICE_qt_[-0.272,0.0426]\",\n         ICE_qt_2 = \"ICE_qt_(0.0426,0.237]\" , \n         ICE_qt_3 = \"ICE_qt_(0.237,0.337]\" ,  \n         ICE_qt_4 = \"ICE_qt_(0.337,0.45]\" ,  \n         ICE_qt_5 = \"ICE_qt_(0.45,0.687]\") \n\n# Null poisson model\nmodel0 <- glm(num ~ race_group + factor(age_cat) + offset(log(den)),\n                        family=poisson(link=log),\n                        data=poisson_data)\n\nsummary.model0 <- summary(model0)\nsaveRDS(summary.model0, file = \"poisson0_bc.rds\")\n\n\npoisson_data$fit <- as_tibble(predict(model0)) %>% \n  transmute(fit = exp(value))\n\npoisson_irr <- poisson_data %>%\n  group_by(super_town, race_group) %>% \n  summarise(num = sum(fit, na.rm=T),\n            den = sum(den, na.rm=T)) %>% \n  mutate(fit_rate = num/den * 1000) %>% \n  pivot_wider(id_cols = super_town,\n              names_from = race_group,\n              values_from = fit_rate) %>% \n  mutate(fit_irr = ifelse(`Non-Hispanic White` == 0, NA_real_, `Non-Hispanic Black` / `Non-Hispanic White`))\n\npoisson_irr_plot <- poisson_irr %>% \n  ungroup() %>% \n  arrange(fit_irr) %>% \n  mutate(orderID = row_number())%>%\n  ggplot(aes(x=orderID, y=fit_irr)) + \n    geom_point(color = \"limegreen\", alpha = 0.8, size = 1) +\n    labs(title = expression(atop(\"Incidence Rate Ratio Comparing Non-Hispanic Black\", \n                                 \"and Non Hispanic White Populations\")),\n         caption = expression(atop(\"Source: Massachusetts Mortality Data 2013-2017\",\n                                   \"5-Year ACS files from end-years 2015-2019\"))) +\n         xlab(\"Census Tracts\") +\n         ylab(\"IRR\") +\n    theme_minimal() \n\nggsave(\"poisson_irr_plot_bc.png\")\n# Adjusted Poisson Model\nmodel1 <- glm(num ~ race_group + factor(age_cat) + (race_group * factor(pov_cat, exclude = NULL)) + offset(log(den)),\n                        family=poisson(link=log),\n                        data=poisson_data)\n\nsummary.model1 <- summary(model1)\nsaveRDS(summary.model1, file = \"poisson1_bc.rds\")\n\npoisson_data$adj_fit <- as_tibble(predict(model1)) %>% \n  transmute(adj_fit = exp(value))\n\nadj_poisson_irr <- poisson_data %>%\n  group_by(super_town, race_group) %>% \n  summarise(num = sum(adj_fit, na.rm=T),\n            den = sum(den, na.rm=T)) %>% \n  mutate(adj_fit_rate = num/den * 1000) %>% \n  pivot_wider(id_cols = super_town,\n              names_from = race_group,\n              values_from = adj_fit_rate) %>% \n  mutate(adj_fit_irr = ifelse(`Non-Hispanic White` == 0, NA_real_, `Non-Hispanic Black` / `Non-Hispanic White`))\n\nadj_poisson_irr_plot <- adj_poisson_irr %>% \n  ungroup() %>% \n  arrange(adj_fit_irr) %>% \n  mutate(orderID = row_number())%>%\n  ggplot(aes(x=orderID, y=adj_fit_irr)) + \n    geom_point(color = \"limegreen\", alpha = 0.8, size = 1) +\n    labs(title = expression(atop(\"Poverty Adjusted Incidence Rate Ratio\", \n                                 \"Comparing Non-Hispanic Black and Non Hispanic White Populations\")),\n         caption = expression(atop(\"Source: Massachusetts Mortality Data 2013-2017\",\n                                   \"5-Year ACS files from end-years 2015-2019\"))) +\n         xlab(\"Census Tracts\") +\n         ylab(\"IRR\") +\n    theme_minimal() \n\nggsave(\"adj_poisson_irr_plot_bc.png\")\nadj_map.state.p.irr <- adj_poisson_irr %>% \n  mutate(adj_fit_irr = case_when(adj_fit_irr > 4  ~ 4,\n                         TRUE     ~ adj_fit_irr)) %>% \n  left_join(town_geometry, by= \"super_town\") %>% \n  ggplot() +\n    geom_sf(mapping = aes(geometry = geometry,\n                          fill = adj_fit_irr),\n            lwd = 0.1) +\n  scale_fill_viridis(option = \"E\",\n                     trans = scales::pseudo_log_trans(sigma=0.01),\n                     limits = exp(c(-1,1)*log(4)),\n                     breaks=c(0.25, 0.5,1,2,4),\n                     na.value = \"white\") +\n  labs(title = expression(atop(\"Poverty Adjusted Incidence Rate Ratio\", \n                                 \"Comparing Non-Hispanic Black and Non Hispanic White Populations\")),\n       caption = expression(atop(\"Source: Massachusetts Mortality Data 2013-2017\", \n                                 \"5-Year ACS files from end-years 2015-2019\")),\n       fill = \"IRR\", x=\"\", y=\"\") +\n  theme_void() +\n    theme(legend.position = c(0.25, 0.25),\n          legend.key.size = unit(0.3, \"cm\"),\n          plot.title      = element_text(hjust = 0.1),\n          plot.subtitle   = element_text(hjust = 0.1))\n\nggsave(\"adj_map.p.irr_bc.png\")## \n## Call:\n## glm(formula = num ~ race_group + factor(age_cat) + offset(log(den)), \n##     family = poisson(link = log), data = poisson_data)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -3.4188  -0.0709   0.7356   1.3605   7.0549  \n## \n## Coefficients:\n##                              Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)                   -6.1048     1.0000  -6.105 1.03e-09 ***\n## race_groupNon-Hispanic Black   0.3307     0.1640   2.016   0.0438 *  \n## factor(age_cat)25-29          -2.7375     1.0541  -2.597   0.0094 ** \n## factor(age_cat)30-34          -1.2184     1.0240  -1.190   0.2341    \n## factor(age_cat)85+             0.2629     1.0006   0.263   0.7928    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 962.55  on 250  degrees of freedom\n## Residual deviance: 649.01  on 246  degrees of freedom\n## AIC: 1360.4\n## \n## Number of Fisher Scoring iterations: 8## \n## Call:\n## glm(formula = num ~ race_group + factor(age_cat) + (race_group * \n##     factor(pov_cat, exclude = NULL)) + offset(log(den)), family = poisson(link = log), \n##     data = poisson_data)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -4.5916  -0.2185   0.4568   1.2308   7.0101  \n## \n## Coefficients:\n##                                                                      Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)                                                          -6.00791    1.00397  -5.984 2.17e-09 ***\n## race_groupNon-Hispanic Black                                          3.60000    0.74960   4.803 1.57e-06 ***\n## factor(age_cat)25-29                                                 -2.21154    1.05829  -2.090 0.036641 *  \n## factor(age_cat)30-34                                                 -1.13313    1.02978  -1.100 0.271174    \n## factor(age_cat)85+                                                    0.42021    1.00158   0.420 0.674820    \n## factor(pov_cat, exclude = NULL)5-9.9%                                -0.09688    0.08916  -1.087 0.277233    \n## factor(pov_cat, exclude = NULL)10-19.9%                              -0.36987    0.10031  -3.687 0.000227 ***\n## factor(pov_cat, exclude = NULL)20-100%                               -0.67203    0.11095  -6.057 1.39e-09 ***\n## race_groupNon-Hispanic Black:factor(pov_cat, exclude = NULL)5-9.9%    2.64245    0.84169   3.139 0.001692 ** \n## race_groupNon-Hispanic Black:factor(pov_cat, exclude = NULL)10-19.9% -2.01362    0.80967  -2.487 0.012884 *  \n## race_groupNon-Hispanic Black:factor(pov_cat, exclude = NULL)20-100%  -3.30795    0.77136  -4.288 1.80e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 962.55  on 250  degrees of freedom\n## Residual deviance: 505.50  on 240  degrees of freedom\n## AIC: 1228.9\n## \n## Number of Fisher Scoring iterations: 10"},{"path":"breast-cancer-mortality.html","id":"references-2","chapter":"8 Case Study 2: Breast Cancer Mortality in Massachusetts","heading":"8.5.4 References","text":"Breast Cancer Research Foundation. (2022). Black women breast cancer: disparities persist end . https://www.bcrf.org/blog/black-women--breast-cancer--disparities-persist---end-/; accessed June 14, 2022.Friedman D, Hunter E, Parrish R (eds). (2005). Health statistics. New York: Oxford University Press.Hetzel . (1997). History organization vital statistics systems. Bethesda, MA: National Center Health Statistics. https://www.cdc.gov/nchs/data/misc/usvss.pdf; accessed June 14, 2022.Krieger N. (2019). US Census People’s Health: Public Health Engagement Enslavement “Indians Taxed” Census Tracts Health Equity (1790-2018). J Public Health. 2019 Aug;109(8):1092-1100. doi: 10.2105/AJPH.2019.305017. Epub 2019 Jun 20.Krieger N, Singh, N, Waterman, PD. (2016). Metrics monitoring cancer inequities: residential segregation, Index Concentration Extremes (ICE), breast cancer estrogen receptor status (USA, 1992-2012). Cancer Causes Control. 2016 Sep;27(9):1139-51. doi: 10.1007/s10552-016-0793-7. Epub 2016 Aug 8.US Office Management Budget. Revisions Standards Classification Federal Data Race Ethnicity. Federal Register 1997; 62(210):58782-58790. https://www.govinfo.gov/content/pkg/FR-1997-10-30/pdf/97-28653.pdf; accessed June 14, 2022.Williams DR, Collins C. (2001). Racial residential segregation: fundamental cause racial disparities health. Public Health Rep. Sep-Oct 2001;116(5):404-16. doi: 10.1093/phr/116.5.404.","code":""},{"path":"cook-county-covid.html","id":"cook-county-covid","chapter":"9 Case Study 3: COVID-19 Mortality in Cook County (March 2020 - March 2022)","heading":"9 Case Study 3: COVID-19 Mortality in Cook County (March 2020 - March 2022)","text":": Sudipta Saha, Christian Testa","code":""},{"path":"cook-county-covid.html","id":"introduction-1","chapter":"9 Case Study 3: COVID-19 Mortality in Cook County (March 2020 - March 2022)","heading":"9.1 Introduction","text":"case study, outcome interest COVID-19 mortality Cook County. impact COVID-19 marked inequities racialized/ethnic group socioeconomic position. investigate disparities COVID-19 mortality, focus comparing risks Black Hispanic non-Hispanic, White non-Hispanic, Hispanic populations.Cook County Medical Examiner made dataset COVID-19 related deaths publicly available following intent:Cook County Government created Medical Examiner COVID-19 Dashboard provide direct, transparent access critical information COVID-19 deaths County public health agencies, medical professionals, first responders, journalists, policymakers residents. Medical Examiner’s Office encourages visitors use data explore trends, identify areas concern take appropriate action. data can utilized identify communities severely impacted virus can inform proactive public policy.Read : https://datacatalog.cookcountyil.gov/stories/s/ttk4-trbuData Source: https://datacatalog.cookcountyil.gov/Public-Safety/Medical-Examiner-Case-Archive-COVID-19-Related-Dea/3trz-enysThe death records dataset includes individual-level fields “Race” “Latino”, obtain social membership racialized/ethnic groups. data set also includes residential ZIP Codes, can linked US Census ZIP Code Tabulation Areas (ZCTAs), can use link area-based social measures (ABSMs) American Community Survey (ACS) 2015-19. explore membership racialized/ethnic groups ABSMs associated COVID-19 mortality.","code":""},{"path":"cook-county-covid.html","id":"motivation-research-questions-and-learning-objectives-2","chapter":"9 Case Study 3: COVID-19 Mortality in Cook County (March 2020 - March 2022)","heading":"9.2 Motivation, Research Questions, and Learning Objectives","text":"case study focuses following research questions:differences COVID-19 mortality rates racialized group accounting age?differences COVID-19 mortality rates racialized group accounting age?gradients overall COVID-19 mortality relation ABSMs measuring: racialized group composition, ICE racialized economic segregation, percent population living poverty line, percent people living crowded housing, median income?gradients overall COVID-19 mortality relation ABSMs measuring: racialized group composition, ICE racialized economic segregation, percent population living poverty line, percent people living crowded housing, median income?gradients COVID-19 mortality racialized group relation ABSMs?gradients COVID-19 mortality racialized group relation ABSMs?spatial variation COVID-19 mortality racialized group?spatial variation COVID-19 mortality racialized group?today’s case example using data, ’ll show can use tidycensus package download relevant area based population estimates sociodemographic measures ACS.’ll show cleaned data merged data ACS Cleaning Data section. However, ’re also providing cleaned dataset allow pick follow along Visualizing Data section onwards.","code":""},{"path":"cook-county-covid.html","id":"approach-2","chapter":"9 Case Study 3: COVID-19 Mortality in Cook County (March 2020 - March 2022)","heading":"9.3 Approach","text":"look overall differences COVID-19 mortality racialized group, model aggregate mortality rates across Cook County racialized groupTo look overall differences COVID-19 mortality racialized group, model aggregate mortality rates across Cook County racialized groupTo look gradients relation ABSMs, model overall ZCTA-level mortality rates ABSM interest.look gradients relation ABSMs, model overall ZCTA-level mortality rates ABSM interest.look gradients relation ABSMs racialized group, model ZCTA-level mortality separately racialized group ABSM.look gradients relation ABSMs racialized group, model ZCTA-level mortality separately racialized group ABSM.explore spatial variation use spatial models.explore spatial variation use spatial models.","code":""},{"path":"cook-county-covid.html","id":"dependencies-2","chapter":"9 Case Study 3: COVID-19 Mortality in Cook County (March 2020 - March 2022)","heading":"9.4 Dependencies","text":"packages need run code case example. copy run code load dependencies, can jump ahead Visualizing Data Section want.","code":"\n# mission critical packages\nlibrary(tidycensus) \nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tigris)\nlibrary(mapview)\nlibrary(INLA)\nlibrary(spdep)\n\n# nice to have packages\nlibrary(magrittr) # for the %<>% and %$% pipe\nlibrary(janitor)\nlibrary(purrr)\nlibrary(Hmisc)\nlibrary(epitools)\nlibrary(leaflet)\nlibrary(scales)"},{"path":"cook-county-covid.html","id":"cleaning-the-data","chapter":"9 Case Study 3: COVID-19 Mortality in Cook County (March 2020 - March 2022)","heading":"9.5 Cleaning the Data","text":"","code":""},{"path":"cook-county-covid.html","id":"denominator-data","chapter":"9 Case Study 3: COVID-19 Mortality in Cook County (March 2020 - March 2022)","heading":"9.5.1 Denominator Data","text":"use tidycensus download relevant variables 2015-19 ACS dataset. complete list variables can viewed online : https://api.census.gov/data/2019/acs/acs5/variables.html. need U.S. Census API key download data, can obtain : https://api.census.gov/data/key_signup.html. key personal use , redacted .looking range different ABSMs, also require age-stratified populations racialized group. can browse variables identify tables contain variables need.can helpful identify patterns variable names efficiently query Census API. code see approach require us type variable names. Can think efficient ways query API?Since Cook County Medical Examiner Case Archive data includes records deaths decedents’ residential ZCTAs inside well outside Cook County, Illinois, want restrict dataset deaths county residence Cook County. issue ZCTAs neatly nested within county borders - often part ZCTA can lie outside county.work issue ZCTAs crossing county borders? calculated percentage area ZCTA also falls within Cook County borders, retained 90% overlap (arbitrary threshold). use different threshold? also deal issues parts Lake Michigan (bordering Cook County’s east coast) included shapefiles others.Note unit geography analysis US census-defined Zip Code Tabulation Area (ZCTA), related identical US Postal Service ZIP Code (unit mail delivery, reflecting postal carrier routes, given spatial area can encompass several ZIP Codes). create ZCTAs, US Census assigns census block frequently occurring ZIP Code within block. technical information ZCTAs, see: https://www.census.gov/programs-surveys/geography/guidance/geo-areas/zctas.html. means ZCTAs zip codes may cover area exactly. discussion pitfalls keep mind using ZCTAs linked residential Zip Code can found paper https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1447194/ .Now obtained ZCTAs keep study, can now focus prepare demographic denominator data.","code":"\n# census_api_key(\"Your API KEY goes here\")\n\n# get population sizes from ACS -------------------------------------------\n\n#Get a list of the variable names\nacs_vars <- tidycensus::load_variables(2019, dataset = 'acs5')\n\n\n# the racialized group and age-group stratified population estimates from ACS are\n# in the B01001 table.\n# \n# B01001_001 through _049 are the sex/gender overall population size estimates,\n# and B01001A through B01001I are the \"race/ethnicity\" specific tables.  For the\n# \"race/ethnicity\" specific tables the age-groups suffixes range from _001 to _031\n# \n# in the following three steps, we programmatically construct the population\n# size variables that we want to retrieve from the ACS since otherwise there are\n# a lot of them to type out.\nrace_chars <-\n  c(\n    white = 'H', # In the 2015-19 ACS data, a suffix of H indicates non-Hispanic White, but be careful because these suffixes may change from year to year!\n    black = 'B',\n    hispanic_or_latino = 'I'\n  )\n\n# for each of H, B, and I, construct B01001*_001 through _031. If you look at the variable list at the link above, you will see that the variable names constructed below correspond to total and age-sex specific population estimates for each racialized/ethnic group.\nsex_race_age_vars <-\n  paste0(rep(paste0('B01001', race_chars, '_0'), each = 31),\n         stringr::str_pad(\n           1:31,\n           width = 2,\n           side = 'left',\n           pad = '0'\n         ))\n\n# this adds on the sex/gender overall population estimates (e.g.B01001_001 is the total male population)\nsex_race_age_vars %<>% c(.,\n                         paste0(\n                           'B01001_0',\n                           stringr::str_pad(\n                             1:49,\n                             width = 2,\n                             side = 'left',\n                             pad = '0'\n                           )\n                         ))\n\n# The API gives us data whose labels are all in one string, with information separated by '!!'. We split the label to make it more useful.\nacs_vars %<>%\n  tidyr::separate(label,\n           into = c('estimate', 'total', 'gender', 'age', 'subgroup'),\n           sep = '!!')\n\n# clean label values (remove unnecessary colon characters and leading/trailing spaces)\nacs_vars %<>% mutate_at(.vars = vars(estimate, total, gender, age, subgroup),\n                        ~ gsub(\":\", \"\", .) %>% stringr::str_trim())\n\n# select only what we need\nacs_vars %<>% dplyr::select(name, total, gender, age, concept)\n\n# Now that we have the variables names, we use it to get sex, racialized group (incl. hispanic or latino), and age stratified population\n# estimates\npopsizes <-\n  tidycensus::get_acs(\n    geography = 'zip code tabulation area',\n    state = 'IL',\n    year = 2019,\n    geometry = TRUE, # This obtains the geometry / shapes of each zip-code tabulation area (ZCTA)\n    variables = sex_race_age_vars,\n    output = 'tidy',\n    cache_table = T #this will enable quicker loading if we load the data again in the future\n  )\n\n# join in our variable names\npopsizes %<>% left_join(acs_vars, by = c('variable' = 'name'))\n\npopsizes %<>% janitor::clean_names() # clean column names\npopsizes %<>% dplyr::select(-total) # remove total column that only contains 'total'\n\n# While we set geometry = T so that we got the geometry data, we don't need this as we clean the data. It makes things slow, so we take out the geometry for use later\nzip_geometry <- popsizes %>% dplyr::select(geoid) %>% unique()\n# ggplot(zip_geometry) + geom_sf() # zip codes of illinois\n\n# remove geometry before data reshaping\npopsizes %<>% sf::st_drop_geometry()\n\nif ('sf' %in% class(popsizes)) {\n  stop(\"popsizes data has geometry, this can cause errors in the group_by and summarize steps using dplyr.\")\n}\n\n# aggregate by sex/gender\npopsizes %<>%\n  group_by(geoid, name, age, concept) %>%\n  dplyr::summarize(\n  estimate = sum(estimate),\n  # here we're using the tidycensus::moe_sum function to create margin of error estimates for \n  # sums of population size estimates\n  moe = tidycensus::moe_sum(moe = moe, estimate = estimate)\n  )\n# identify relevant zip codes ---------------------------------------------\n\n# we originally downloaded the county shapefiles using tigris, but \n# we found that those contained water area we wanted to remove. \n# \n# we tried using the tigris::erase_water function but it took a very \n# long time to run for us, so we found that the alternative was easier\n# to implement:  downloading a county shapefile directly from the census \n# and using that one which came already with the water areas removed. \n# \n# We got our county shapefile for Cook county in what follows from here: \n# https://www.census.gov/geographies/mapping-files/time-series/geo/carto-boundary-file.html \n\n########## Change to file path where the shape file is downloaded ###########\ncounties <- read_sf('./data/09-cook-county-covid/cb_2018_us_county_5m/cb_2018_us_county_5m.shp')\n\n# get the map for cook county\ncook_county <- counties %>% filter(\n  # get the 5-digit FIPS or GEOID for Cook County programmatically by filtering\n  # the tigris::fips_codes dataframe, or if you happen to know it's 17031 \n  # you could code it explicitly instead\n  GEOID == tigris::fips_codes %>% \n    filter(county == 'Cook County', state == 'IL') %$% \n    paste0(state_code, county_code))\n\n# get cook county sf object from tigris that is water inclusive for filtering. Since the ZIP geometry can include bits of water, we want to use the Cook County boundaries that include Lake Michigan. Otherwise some coastal ZIP codes may have less than 90% overlap and get dropped.\nIL_counties <- tigris::counties(state = 'IL')\ncook_county_incl_water <- IL_counties %>% \n  filter(\n    COUNTYFP == {\n      tigris::fips_codes %>% \n        filter(state == 'IL', county == 'Cook County') %>% \n        pull(county_code)\n    }\n  )\n\n# Find the zip codes intersecting cook county. In the output of st_intersects, zips that intersect with cook county have length greater than 0. So here we keep Zips with any overlap\nzips_with_overlap <- zip_geometry[st_intersects(zip_geometry, cook_county_incl_water) %>% \n                                    map_lgl( ~ length(.) > 0),]\n\n# figure showing cook county and all ZCTAs intersecting it \nggplot() + \n  geom_sf(data = cook_county_incl_water, color = 'cadetblue', fill = 'cadetblue') + \n  geom_sf(data = zips_with_overlap, color = 'orange', fill = 'white', alpha = .4) + \n  theme_bw()\n\n# interactive file \nleaflet() %>% \n  addTiles() %>% #Adds the basemap\n  addPolygons(data = cook_county_incl_water, weight = .5) %>% #Adds the Cook county boundary\n  addPolygons(data = zips_with_overlap, color = 'orange', weight = 2, label = ~geoid) # Adds Zip boundaries\n\n# calculate each zip code total area\nzips_with_overlap$total_area <- st_area(zips_with_overlap)\n\n# calculate the intersection of cook county and each zip code tabulation area\nzips_with_overlap_intersection <- st_intersection(zips_with_overlap, cook_county_incl_water) #this returns polygons where county and ZCTA intersect\nzips_with_overlap_intersection %<>% dplyr::select(geoid)\n\n# calculate the area of each intersection\nzips_with_overlap_intersection$overlapping_area_w_cook_cty <- st_area(zips_with_overlap_intersection)\n\n# drop geometry so we can merge this back in\nzips_with_overlap_intersection %<>% sf::st_drop_geometry()\n\n# add in overlap calculation\nzips_with_overlap %<>% left_join(\n  zips_with_overlap_intersection %>% dplyr::select(geoid, overlapping_area_w_cook_cty),\n  by = c('geoid')\n)\n\n# calculate the proportion of area for that zip code within cook county\nzips_with_overlap %<>% mutate(\n  prop_area_overlap = as.numeric(overlapping_area_w_cook_cty / total_area))\n\n# filter for zip codes with 90% land area inside cook county\nzips_with_over_90pct_overlap <- zips_with_overlap %>%\n  filter(prop_area_overlap >= .9)\n\n# Visualize Cook County and ZIPs with over 90 percent overlap\nggplot() + \n  geom_sf(data = cook_county, aes(color = \"Cook County\"), fill = 'white') + #Plots Cook county boundary\n  geom_sf(\n    data = zips_with_over_90pct_overlap, #plots zip codes\n    mapping = aes(color = \"Zip Code Tabulation Areas\"),\n    size = .5,\n    alpha = .6 #Sets transparency so we can see both layers\n  ) + \n  scale_color_manual(\n    values = c(\"Cook County\" = \"#E69F00\", \"Zip Code Tabulation Areas\" = \"#56B4E9\") #manually set colors\n  ) + \n  # theme_bw() + \n  guides(color = guide_legend(override.aes = list(fill = c('white', 'grey95')))) +\n  scale_x_continuous(breaks = c(-88.3, -88.1, -87.9, -87.7, -87.5)) +   \n  ggtitle(\"Cook County and Zip Code Tabulation Areas 90% Contained Within\")\n\n# save the plot\n# ggsave(here(\"images/09-cook-county-covid/cook_county_and_zip_codes.png\"), width = 10, height = 6)\n# group population sizes together by age groups and racialized group and zip code\nzips_with_population_estimates_by_race_ethnicity_and_age <-\n  popsizes %>%\n  filter(! is.na(age) & concept != 'SEX BY AGE') %>% # filter for overall population\n  filter(geoid %in% zips_with_over_90pct_overlap$geoid) %$% #Filter to keep only the zips selected above\n  left_join(zips_with_over_90pct_overlap, .) #join with the zip dataframe to add the geometry column\n\n# Change names of the Census racialized groups\nzips_with_population_estimates_by_race_ethnicity_and_age %<>% mutate(\n  race_ethnicity = case_when(\n    concept ==  \"SEX BY AGE (BLACK OR AFRICAN AMERICAN ALONE)\" ~ \"Black, Hispanic or non-Hispanic\",\n    concept ==  \"SEX BY AGE (HISPANIC OR LATINO)\" ~ \"Hispanic\",\n    concept ==  \"SEX BY AGE (WHITE ALONE, NOT HISPANIC OR LATINO)\" ~ \"White, non-Hispanic\"\n  ))\n\n\n# remove totals (across age groups) observations\nzips_with_population_estimates_by_race_ethnicity_and_age %<>% filter(! is.na(age))\nzips_with_population_estimates_by_race_ethnicity_and_age %<>% filter(! is.na(race_ethnicity))\n\n# aggregate into larger age groups\nzips_with_population_estimates_by_race_ethnicity_and_age %<>% mutate(\n  age_group = case_when(\n    age %in% c(\n      \"Under 5 years\",\n      \"5 to 9 years\",\n      \"10 to 14 years\",\n      \"15 to 17 years\",\n      \"18 and 19 years\",\n      \"20 to 24 years\"\n    ) ~ \"Under 25\",\n    age %in% c(\n      \"25 to 29 years\",\n      \"30 to 34 years\"\n    ) ~ \"25 to 34 years\",\n    age == \"35 to 44 years\" ~ \"35 to 44 years\",\n    age == \"45 to 54 years\" ~ \"45 to 54 years\",\n    age == \"55 to 64 years\" ~ \"55 to 64 years\",\n    age == \"65 to 74 years\" ~ \"65 to 74 years\",\n    age == \"75 to 84 years\" ~ \"75 to 84 years\",\n    age == \"85 years and over\" ~ \"85 years and over\"\n  ))\n\n# sum age group population sizes by racialized group\nzips_with_population_estimates_by_race_ethnicity_and_age %<>% group_by(geoid, age_group, race_ethnicity) %>%\n  dplyr::summarize(\n    estimate = sum(estimate, na.rm=T),\n    moe = moe_sum(moe, estimate)\n  )\n\n# check how large the population sizes are by racialized group and age\n# in each ZCTA\nzips_with_population_estimates_by_race_ethnicity_and_age %>% \n  ggplot(\n    aes(x = estimate) #estimate is the column name for the population size\n  ) + \n  geom_histogram() + \n  facet_grid(race_ethnicity~age_group) + #creates grid of individual plots for each combination of racialized group and age group\n  scale_x_log10() + #x axis in log scale since distribution of population counts is skewed\n  xlab(\"Population Estimate\") +\n  ggtitle(\"Population Sizes by Racialized Group and Age Group in ZCTAs\") +\n  theme(axis.text.x = element_text(angle = 75, hjust = 1)) #rotate x-axis labels\n  \n\n# ggsave(here(\"images/09-cook-county-covid/population_count_histogram.png\"), height = 8, width = 10)\n\n# check how many have estimates less than 10\nzips_with_population_estimates_by_race_ethnicity_and_age %>% \n  group_by(estimate < 10) %>% count()"},{"path":"cook-county-covid.html","id":"case-data","chapter":"9 Case Study 3: COVID-19 Mortality in Cook County (March 2020 - March 2022)","heading":"9.5.2 Case Data","text":"Now ’re ready load case data, add population size estimates racialized group (White non-Hispanic, Black non-Hispanic Hispanic, Hispanic) age-group ZIP code.can now add ABSMs interested data. ’ll add Index Concentration Extremes Racialized Economic Segregation, proportion population poverty line, median income ZCTA.added discretized (cut, .e. categorical) versions different ABSMs. allows model fit nonlinear responses increasing levels covariates. Alternative approaches involve fitting models smoothing splines variables instead, aren’t including approaches . used Illinois-wide distribution ABSMs ZCTA level create quantiles variables except poverty. poverty, use pre-specified cutpoints.set cutpoints continuous ABSMs based state-wide distribution ZCTAs. might affect interpretation? might research question context affect decide ABSM cutpoints ?might selection cutpoints affect visualizations? might cutpoints change depending want communicate map?remaining necessary cleaning steps perform ’re ready model data.make sure factor variables appropriate reference levels set. Typically set reference level privileged group can frame results “_____ group X times mortality rate reference group.”","code":"\n# read in your data. This has been donwloaded from the Cook County Medical Examiner's office website, as mentioned above. https://datacatalog.cookcountyil.gov/Public-Safety/Medical-Examiner-Case-Archive-COVID-19-Related-Dea/3trz-enys\n\ncook_county_deaths <- \n  readr::read_csv(\"./data/09-cook-county-covid/Medical_Examiner_Case_Archive_-_COVID-19_Related_Deaths.csv\")\n\n# merge racialized group and age-group specific denominators ----------------\n\n# use janitor::clean_names to standardize column name syntax into snake_case\ncook_county_deaths %<>% janitor::clean_names()\n\n# check distribution of deaths if needed\n#ggplot(cook_county_deaths, aes(x = age)) + geom_bar()\n#ggplot(cook_county_deaths, aes(x = race)) + geom_bar() \n#ggplot(cook_county_deaths, aes(x = latino)) + geom_bar() + facet_wrap(~race)\n#ggplot(cook_county_deaths, aes(x = latino)) + geom_bar()\n\n# code racialized groups into the following: \n#  Black (Hispanic or non-Hispanic)\n#  Hispanic and non-Hispanic\n#  White non-Hispanic\ncook_county_deaths %<>%\n  mutate(\n    black = race == 'Black',\n    white_nh = (! latino) & race == 'White',\n    hispanic = latino\n  )\n\n# we restrict our time-period to during March 2020 to March 2022 -- noting the \n# information on the dataset stories web page: \n# \n#  https://datacatalog.cookcountyil.gov/stories/s/ttk4-trbu\n# \n#  Effective April 1, 2022, the Cook County Medical Examiner’s Office no longer\n#  takes jurisdiction over hospital, nursing home or hospice COVID-19 deaths\n#  unless there is another factor that falls within the Office’s jurisdiction.\n# \ncook_county_deaths %<>% filter(\n  lubridate::mdy_hms(date_of_death) >= lubridate::mdy(\"03/01/2020\") & #convert date in character format to a date format (POSIXct). This allows evaluation of conditions such as date greater than x, etc.\n    lubridate::mdy_hms(date_of_death) <= lubridate::mdy(\"04/01/2022\")\n)\n\n# Truncate Zip Codes to 5 Characters\ncook_county_deaths %<>% mutate(residence_zip = stringr::str_extract(residence_zip, \"^[0-9]{5}\"))\n\n# Categorize Age Group\ncook_county_deaths %<>% mutate(\n    age_group = case_when(\n      age <= 24 ~ \"Under 25\",\n      age >= 25 & age <= 35 ~ \"25 to 34 years\",\n      age >= 35 & age <= 44 ~ \"35 to 44 years\",\n      age >= 45 & age <= 54 ~ \"45 to 54 years\",\n      age >= 55 & age <= 64 ~ \"55 to 64 years\",\n      age >= 65 & age <= 74 ~ \"65 to 74 years\",\n      age >= 75 & age <= 84 ~ \"75 to 84 years\",\n      age >= 85 ~ \"85 years and over\",\n      TRUE ~ NA_character_\n    )\n  )\n\n#Convert to factor so that when plotting age groups are plotted in order\ncook_county_deaths$age_group %<>% factor(\n  levels = c(\n   \"Under 25\",\n   \"25 to 34 years\",\n   \"30 to 34 years\",\n   \"35 to 44 years\",\n   \"45 to 54 years\",\n   \"55 to 64 years\",\n   \"65 to 74 years\",\n   \"75 to 84 years\",\n   \"85 years and over\")\n  ) \n\n# separate out deaths into the racialized group categories we're interested in \n# \n# note that these do have overlap: since the ACS variables do not include \n# age tables for the Black non-Hispanic group, we are using the Black \n# Hispanic or non-Hispanic population by age tables, and so there is overlap\n# between them and the Hispanic group counts.\ndeaths_by_race_ethnicity_group <- \n  list(\n    black = cook_county_deaths %>% filter(black),\n    hispanic = cook_county_deaths %>% filter(hispanic),\n    white_nh = cook_county_deaths %>% filter(white_nh)\n  ) \n\n# Assign labels for the racialized groups \nrace_ethnicity_groups <- c(\n  \"Black, Hispanic or non-Hispanic\",\n  \"Hispanic\",\n  \"White, non-Hispanic\"\n)\n\n# for each racialized group, tabulate the number of deaths by age group and \n# residence zip code. \n# \n# here, purrr::map is applying a function which does that tabulation to each of\n# the data frames in the deaths_by_race_ethnicity_group list\ndeaths_by_race_ethnicity_group <- \n  purrr::map(1:3, function(i) {\n    group_by(deaths_by_race_ethnicity_group[[i]], residence_zip, age_group) %>% \n        count(name = 'deaths') %>% \n      mutate(race_ethnicity = race_ethnicity_groups[[i]])\n  })\n\n# bind the tables for each of the racialized groups together\ndeaths_by_race_ethnicity_group %<>% bind_rows()\n\n# check how many of the deaths proportionally were NA\nggplot(deaths_by_race_ethnicity_group, aes(x = age_group, y = deaths)) + \n  geom_col() + \n  facet_wrap(~race_ethnicity) + \n  ylab(\"Count of Deaths\") + \n  theme(axis.text.x = element_text(angle = 75, hjust = 1))\n\n# remove missing age, missing deaths in racialized groups\ndeaths_by_race_ethnicity_group %<>% filter(! is.na(age_group))\ndeaths_by_race_ethnicity_group %<>% filter(! is.na(race_ethnicity))\n\n# check distribution of deaths by age group\ndeaths_by_race_ethnicity_group %>% \n  group_by(age_group) %>% \n  count() \n\n# check distribution of deaths by racial/ethnic group\ndeaths_by_race_ethnicity_group %>% \n  group_by(race_ethnicity) %>% \n  count() \n\n# merge the denominators and deaths together\ndf <-\n  zips_with_population_estimates_by_race_ethnicity_and_age %>% left_join(\n  deaths_by_race_ethnicity_group,\n  by = c('geoid' = 'residence_zip', 'age_group' = 'age_group', 'race_ethnicity' = 'race_ethnicity'))\n\n# make age_group a factor variable\ndf$age_group %<>% factor(\n  levels = c(\n    'Under 25',\n    '25 to 34 years',\n    '35 to 44 years',\n    '45 to 54 years',\n    '55 to 64 years',\n    '65 to 74 years',\n    '75 to 84 years',\n    '85 years and over'\n  )\n)\n\n# add person-time\nobservation_time_in_years <- as.integer(lubridate::mdy(\"04-01-2022\") - lubridate::mdy(\"03-01-2020\")) / 365\ndf %<>% mutate(person_time = estimate * observation_time_in_years)\ndf %<>% mutate(deaths = ifelse(is.na(deaths), 0, deaths))\ndf %<>% mutate(mortality_per100k_py = deaths / person_time * 1e5)\n\ndf %>% \n  group_by(age_group, race_ethnicity) %>% \n  dplyr::summarize(deaths = sum(deaths)) %>%\n  ggplot(aes(x = age_group, y = deaths)) + \n  geom_col() + \n  facet_wrap(~race_ethnicity) + \n  xlab(\"Age Group\") + \n  ylab(\"Count of Deaths\") + \n  theme(axis.text.x = element_text(angle = 75, hjust = 1)) + \n  ggtitle(\"Age Distribution of COVID-19 Deaths in Cook County, IL\")\n\n# ggsave(here(\"images/09-cook-county-covid/age_distribution_deaths.png\"), width = 8, height = 6)\n# add area based socioeconomic measures  ----------------------------------\n\n# get zip code rates for\n#   - poverty\n#   - ICEraceinc\n#   - median income\n\n# We create a data dictionary for ABSMS. The first column indicates the total variable code, the second the variable name, and the third the description.\nabsms_dictionary <- tibble::tribble(\n  ~var, ~varname, ~description,\n  # total population\n  \"B01001_001\",  \"total_popsize\", \"total population estimate\", \n  \n  # racial composition \n  'B01003_001',  \"race_ethnicity_total\", \"race_ethnicity_total\",\n  \n  # ICEraceinc\n  \"B19001_001\",  'hhinc_total',   \"total population for household income estimates\",\n  \"B19001A_002\", 'hhinc_w_1',     \"white n.h. pop with household income <$10k\",\n  \"B19001A_003\", 'hhinc_w_2',     \"white n.h. pop with household income $10k-14 999k\",\n  \"B19001A_004\", 'hhinc_w_3',     \"white n.h. pop with household income $15k-19 999k\",\n  \"B19001A_005\", 'hhinc_w_4',     \"white n.h. pop with household income $20k-24 999k\",\n  \"B19001A_014\", 'hhinc_w_5',     \"white n.h. pop with household income $100 000 to $124 999\",\n  \"B19001A_015\", 'hhinc_w_6',     \"white n.h. pop with household income $125k-149 999k\",\n  \"B19001A_016\", 'hhinc_w_7',     \"white n.h. pop with household income $150k-199 999k\",\n  \"B19001A_017\", 'hhinc_w_8',     \"white n.h. pop with household income $196k+\",\n  \"B19001_002\",  'hhinc_total_1', \"total pop with household income <$10k\",\n  \"B19001_003\",  'hhinc_total_2', \"total pop with household income $10k-14 999k\",\n  \"B19001_004\",  'hhinc_total_3', \"total pop with household income $15k-19 999k\",\n  \"B19001_005\",  'hhinc_total_4', \"total pop with household income $20k-24 999k\",\n\n  # poverty\n  \"B05010_002\",  'in_poverty',    \"population with household income < poverty line\",\n  \"B05010_001\",  'total_pop_for_poverty_estimates',  \"total population for poverty estimates\",\n\n  # median income\n  \"B06011_001\",  'median_income',  \"median income estimate for total population\",\n  \n  # crowded housing\n  \"B25014_005\",  'owner_occupied_crowding1', 'owner occupied, 1 to 1.5 per room',\n  \"B25014_006\",  'owner_occupied_crowding2', 'owner occupied, 1.51 to 2 per room',\n  \"B25014_007\",  'owner_occupied_crowding3', 'owner occupied, 2.01 or more per room',\n  \"B25014_011\",  'renter_occupied_crowding1', 'owner occupied, 1 to 1.5 per room',\n  \"B25014_012\",  'renter_occupied_crowding2', 'owner occupied, 1.51 to 2 per room',\n  \"B25014_013\",  'renter_occupied_crowding3', 'owner occupied, 2.01 or more per room',\n  \"B25014_001\",  'crowding_total',            'total for crowding (occupants per room)',\n  \n  \"B01001I_001\",  'total_hispanic',           'total hispanic population estimate',\n  \"B01001B_001\",  'total_black',              'total black, hispanic or non-hispanic estimate',\n  \"B01001H_001\",  'total_white_nh',           'total white, non-hispanic population estimate'\n)\n\n# We create a function which takes as a argument a vector of zip codes, and queries the Census API using those to obtain our ABSM variables for those ZCTAs, and then calculates the ABSMs of interest.\n\nget_absms <- function(zip_codes) {\n  absms <- tidycensus::get_acs(\n    year = 2019,\n    geography = 'zcta',\n    state = 'IL',\n    zcta = zip_codes,\n    variables = absms_dictionary$var, #Get the variables indicated in the data dictionary.\n    geometry = FALSE # We already have the geometry so we don't need that.\n  )\n\n  # pivot wider so that each row corresponds to a ZCTA\n  absms %<>% dplyr::select(-moe) %>%\n    tidyr::pivot_wider(names_from = variable, values_from = estimate)\n# Change the new column names to reflect variables names from the dictionary\n  rename_vars <- setNames(absms_dictionary$var, absms_dictionary$varname)\n  absms <- absms %>% rename(!!rename_vars)\n\n  absms %<>%\n    mutate(\n      # we calculate the people of color low income counts as the overall\n      # low income counts minus the white non-hispanic low income counts\n      people_of_color_low_income =\n        (hhinc_total_1 + hhinc_total_2 + hhinc_total_3 + hhinc_total_4) -\n        (hhinc_w_1 + hhinc_w_2 + hhinc_w_3 + hhinc_w_4),\n      # sum up the white non-hispanic high income counts\n      white_non_hispanic_high_income =\n        (hhinc_w_5 + hhinc_w_6 + hhinc_w_7 + hhinc_w_8),\n      # calculate the index of concentration at the extremes for racialized\n      # economic segregation (high income white non-hispanic vs. low income\n      # people of color)\n      ICEraceinc =\n        (white_non_hispanic_high_income - people_of_color_low_income) /\n        hhinc_total,\n\n      prop_in_poverty = in_poverty / total_pop_for_poverty_estimates,\n      \n      crowding = (owner_occupied_crowding1 + owner_occupied_crowding2 + owner_occupied_crowding3 +\n        renter_occupied_crowding1 + renter_occupied_crowding2 + renter_occupied_crowding3) / crowding_total,\n      \n      prop_black = total_black / total_popsize,\n      prop_hispanic = total_hispanic / total_popsize,\n      prop_white_nh = total_white_nh / total_popsize\n    ) %>%\n    dplyr::select(GEOID, ICEraceinc, prop_in_poverty, median_income, crowding, prop_black, prop_hispanic, prop_white_nh)\n\n  return(absms)\n}\n\nabsms <- get_absms(unique(df$geoid))\n\n# merge in absm data to outcome data\ndf %<>% left_join(absms, by = c('geoid' = 'GEOID'))\n\n# get cutpoints for ICEraceinc in illinois --------------------------------\n\n# we're doing this because we want our cutpoints to be in reference to the \n# state distribution for the ICEraceinc variable;  other cutpoints can be used, \n# but it's important to be transparent about what cutpoints are used and consider\n# how the choice of cutpoints may affect the analysis.\n# \nillinois_absms <-\n  tidycensus::get_acs(\n    year = 2019,\n    geography = 'zcta',\n    state = 'IL', # Getting for all of IL\n    variables = absms_dictionary$var,\n    geometry = FALSE\n  ) %>%\n  dplyr::select(-moe) %>%\n  tidyr::pivot_wider(names_from = variable, values_from = estimate)\n\nrename_vars <- setNames(absms_dictionary$var, absms_dictionary$varname)\nillinois_absms %<>% rename(!!rename_vars)\n\nillinois_absms %<>%\n  mutate(\n    # we calculate the people of color low income counts as the overall\n    # low income counts minus the white non-hispanic low income counts\n    people_of_color_low_income =\n      (hhinc_total_1 + hhinc_total_2 + hhinc_total_3 + hhinc_total_4) -\n      (hhinc_w_1 + hhinc_w_2 + hhinc_w_3 + hhinc_w_4),\n    # sum up the white non-hispanic high income counts\n    white_non_hispanic_high_income =\n      (hhinc_w_5 + hhinc_w_6 + hhinc_w_7 + hhinc_w_8),\n    # calculate the index of concentration at the extremes for racialized\n    # economic segregation (high income white non-hispanic vs. low income\n    # people of color)\n    ICEraceinc =\n      (white_non_hispanic_high_income - people_of_color_low_income) /\n      hhinc_total,\n    \n    # we don't need poverty here since we're going to use pre-specified cutpoints\n    # of 0-5%, 5-10%, 10-20% and 20%+ \n    \n    # calculate crowding\n    crowding = (owner_occupied_crowding1 + owner_occupied_crowding2 + owner_occupied_crowding3 +\n        renter_occupied_crowding1 + renter_occupied_crowding2 + renter_occupied_crowding3) / crowding_total,\n    \n    # racial/ethnic composition variables\n    prop_black = total_black / total_popsize,\n    prop_hispanic = total_hispanic / total_popsize,\n    prop_white_nh = total_white_nh / total_popsize\n  ) \n\n# Using the state-wide distribution of ABSMs at the ZCTA-level, we select cutpoints for quantiles.\nillinois_ICEraceinc_cutpoints <-\n  Hmisc::wtd.quantile(illinois_absms$ICEraceinc, #Weighted quantile of ICE\n                      illinois_absms$total_popsize, #Weights proportional to population size\n                      seq(0, 1, .2), # Cutpoints at 0, 0.2, 0.4, 0.8, 1\n                      na.rm = T)\n\nillinois_ICEraceinc_cutpoints <-\n  Hmisc::wtd.quantile(illinois_absms$ICEraceinc,\n                      illinois_absms$total_popsize,\n                      seq(0, 1, .2),\n                      na.rm = T)\n\nillinois_median_income_cutpoints <-\n  Hmisc::wtd.quantile(illinois_absms$median_income,\n                      illinois_absms$total_popsize,\n                      seq(0, 1, .2),\n                      na.rm = T)\n\n\nillinois_crowding_cutpoints <-\n  Hmisc::wtd.quantile(illinois_absms$crowding,\n                      illinois_absms$total_popsize,\n                      seq(0, 1, .2),\n                      na.rm = T)\n\nillinois_prop_hispanic_cutpoints <-\n  Hmisc::wtd.quantile(illinois_absms$prop_hispanic,\n                      illinois_absms$total_popsize,\n                      seq(0, 1, .2),\n                      na.rm = T)\n\nillinois_prop_black_cutpoints <-\n  Hmisc::wtd.quantile(illinois_absms$prop_black,\n                      illinois_absms$total_popsize,\n                      seq(0, 1, .2),\n                      na.rm = T)\n\nillinois_prop_white_nh_cutpoints <-\n  Hmisc::wtd.quantile(illinois_absms$prop_white_nh,\n                      illinois_absms$total_popsize,\n                      seq(0, 1, .2),\n                      na.rm = T)\n\n# create cutpoint-leveled version of key ABSM variables\ndf$ICEraceinc_cut <- df$ICEraceinc %>% cut(., illinois_ICEraceinc_cutpoints, include.lowest=TRUE)\ndf$median_income_cut <- df$median_income %>% cut(., illinois_median_income_cutpoints, include.lowest=TRUE)\ndf$prop_in_poverty_cut <- df$prop_in_poverty %>% cut(.,  c(0, .05, .1, .2, 1), include.lowest=TRUE)\ndf$crowding_cut <- df$crowding %>% cut(., illinois_crowding_cutpoints, include.lowest=TRUE)\ndf$prop_black_cut <- df$prop_black %>% cut(., illinois_prop_black_cutpoints, include.lowest=TRUE)\ndf$prop_hispanic_cut <- df$prop_hispanic %>% cut(., illinois_prop_hispanic_cutpoints, include.lowest=TRUE)\ndf$prop_white_nh_cut <- df$prop_white_nh %>% cut(., illinois_prop_white_nh_cutpoints, include.lowest=TRUE)\n# prepare data for modeling  ----------------------------------------------\n\n# remove infinite or NA mortality rates since they will cause errors in trying\n# to fit the models\ndf_prepped <- df %>% filter(\n  is.finite(mortality_per100k_py) &\n    ! is.na(mortality_per100k_py))\n\n# ungroup\ndf_prepped %<>% ungroup()\n\n# make the most privileged the reference category\ndf_prepped %<>% mutate(\n  race_ethnicity = forcats::fct_relevel(factor(race_ethnicity), \"White, non-Hispanic\"),\n  age_group = age_group,\n  ICEraceinc_cut = forcats::fct_rev(ICEraceinc_cut),\n  median_income_cut = forcats::fct_rev(median_income_cut)\n)\n\n# rename the estimate variable to 'population_estimate' to be more clear\ndf_prepped %<>% rename(population_estimate = estimate)\n\n# save data \n# saveRDS(df_prepped, here(\"data/09-cook-county-covid/cook_county_mortality_cleaned.rds\"))"},{"path":"cook-county-covid.html","id":"visualizing-your-data-1","chapter":"9 Case Study 3: COVID-19 Mortality in Cook County (March 2020 - March 2022)","heading":"9.6 Visualizing Your Data","text":"want start using clean dataset can start point . Now clean dataset, can create exploratory visualizations. code chunk exploratory visualizations data. first series maps show crude mortality rates ZCTA, racialized group age group. see larger version image right click select open image new tab.can see number zero-counts, rates noisier population sizes smaller.figure illustrates lot variability mortality rates ZCTA-level population count small. Thus need careful interpreting extreme values obtained small areas.map crude rates small areas low person-time risk may misleading. may identify areas extreme rates, may just due chance small size population risk.code produces series maps visualize different ABSMs use analysis. Note Chicago situated Cook County.\ncontrast ABSMs, Index Concentration Extremes (ICE) highlights racialized economic segregation (.e., race/ethnicity + income). measures extent area’s population concentrated extremes deprivation privilege. scaled -1 1: value -1 means 100% population concentrated deprived group (analysis, conceptualized population color low-income households), value 1 means 100% population concentrated privileged group (analysis, conceptualized White non-Hispanic population high-income households).maps utilize various color scales. think can affect communication maps? preference? might use diverging versus continuous color scale? Sometimes can also useful plot interactive map, shown .can utilize mapview package plot interactive graphs.","code":"\n#change this to point to where your downloaded file is\ndf <- readRDS(\"./data/09-cook-county-covid/cook_county_mortality_cleaned.rds\") \n\n#Read in the county boundary shapefile to plot maps\ncounties <- read_sf('./data/09-cook-county-covid/cb_2018_us_county_5m/cb_2018_us_county_5m.shp') \n\n# get the map for cook county\ncook_county <- counties %>% filter(\n  # get the 5-digit FIPS or GEOID for Cook County programmatically by filtering\n  # the tigris::fips_codes dataframe, or if you happen to know it's 17031 \n  # you could code it explicitly instead\n  GEOID == tigris::fips_codes %>% \n    filter(county == 'Cook County', state == 'IL') %$% \n    paste0(state_code, county_code))\n\n# let's start by mapping the crude mortality rates in each racialized group and \n# age strata\ndf %<>% mutate(deaths = ifelse(is.na(deaths), 0, deaths)) # Convert missing cells to 0\ndf %<>% mutate(mortality_per100k_py = deaths / person_time * 1e5) # Calculate mortality rate per 100,000 person years\n\n# plot raw mortality rates\ndf %>% \n  ggplot(aes(fill = mortality_per100k_py)) + \n  geom_sf(data = cook_county, fill = 'dimgrey') + \n  geom_sf(size = 0) + \n  facet_grid(forcats::fct_rev(race_ethnicity)~age_group) + # Create maps of mortality rates by age-group and racialized/ethnic groups\n  scale_fill_distiller(palette = 'Reds', \n                       trans = scales::pseudo_log_trans(sigma = 100), #log transform since distribution is skewed\n                       direction = 1, #Higher is darker\n                       labels = scales::comma_format(), \n                       na.value = 'dimgrey', #NAs as grey\n                       breaks = c(0, 1000, 10000, 80000)) +\n  scale_x_continuous(breaks = c( -88.1, -87.7)) +   \n  theme_bw() + \n  theme(legend.position = 'bottom', \n        panel.grid = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.ticks.y = element_blank(),\n        legend.text = element_text(angle = 75, hjust=1)\n        ) +\n  labs(fill = 'COVID-19 Mortality per 100,000 person years') + \n  ggtitle(\"Crude COVID-19 Mortality Rates by ZCTA, Racialized Group and Age Group\",\n          subtitle = \"March 2020 - March 2022\")\n\n# ggsave(here(\"images/09-cook-county-covid/raw_rates_by_zcta.png\"), width = 14, height = 8)\n\n# plot population sizes\ndf %>% ggplot(aes(fill = population_estimate)) + \n  geom_sf(data = cook_county, fill = 'dimgrey') + \n  geom_sf(size = 0) + \n  facet_grid(forcats::fct_rev(race_ethnicity)~age_group) + \n  scale_fill_distiller(palette = 'Greens', trans = scales::pseudo_log_trans(sigma = 100), direction = 1, \n                       labels = scales::comma_format(), na.value = 'dimgrey',\n                       limits = c(0, NA), \n                       breaks = c(0, 1000, 10000, 30000)) +\n  scale_x_continuous(breaks = c( -88.1, -87.7)) +   \n  theme_bw() + \n  theme(legend.position = 'bottom', \n        panel.grid = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.ticks.y = element_blank(),\n        legend.text = element_text(angle = 75, hjust=1)\n        ) +\n  labs(fill = 'Population Estimate') + \n  ggtitle(\"Population Size by ZCTA, Racialized Group and Age Group\",\n          subtitle = \"Data from ACS 2015-2019\")\n\n# ggsave(\"./images/09-cook-county-covid/population_size_by_zcta.png\", width = 14, height = 8)\n\n#Plot mortality rates\ndf %>% ggplot(aes(x = mortality_per100k_py, fill = race_ethnicity)) +\n  geom_histogram(alpha = .8, position = 'identity') +\n  facet_grid(forcats::fct_rev(race_ethnicity) ~ age_group,\n             scales = 'free') +\n  scale_x_continuous(\n    trans = scales::pseudo_log_trans(sigma = 10),\n    labels = scales::comma_format(),\n    breaks = c(0, 100, 1000, 5e4)\n  ) + \n  xlab(\"Mortality Rate per 100,000 Person Years\") + \n  ylab(\"Count\") +\n  labs(fill = 'Racialized Groups') + \n  ggtitle(\"Histogram of Mortality Rates by ZCTA, Age Group and Racialized Group\",\n          subtitle = \"March 2020 - March 2022\") +\n  theme(legend.position = 'bottom')\n\n# ggsave(here(\"images/09-cook-county-covid/raw_rates_histogram.png\"), width = 16, height = 8)\n\ndf %>% \n  ggplot(\n    aes(x = population_estimate, \n        y = mortality_per100k_py, \n        color = race_ethnicity)) +\n  geom_point(alpha = .8) +\n  facet_grid( forcats::fct_rev(race_ethnicity) ~ age_group) +\n  scale_y_continuous(\n    trans = scales::pseudo_log_trans(sigma = 10),\n    labels = scales::comma_format(),\n    breaks = c(0, 100, 1000, 5e4)\n  ) + \n  scale_x_continuous(\n    trans = scales::log_trans(),\n    labels = scales::comma_format()\n  ) + \n  ylab(\"Mortality Rate per 100,000 Person Years\") + \n  xlab(\"Population Size\") + \n  theme(legend.position = 'bottom') + \n  labs(fill = 'Racialized Groups') + \n  ggtitle(\"Population Size and Mortality Rates by ZCTA, Age Group and Racialized Group\")\n\n# ggsave(here(\"images/09-cook-county-covid/raw_rates_scatter.png\"), width = 16, height = 8)\n# in the following visualizations the motivating principle behind the direction of the \n# color palette is to show (for sequential palettes) higher density with darker colors; \n# for the ICEraceinc variable a divergent color palette is used to draw attention to the \n# extreme ends of the scale\n\n# visualize the proportional racial/ethnic breakdown\ndf %>% dplyr::select(geoid, prop_black, prop_white_nh, prop_hispanic) %>% \n  tidyr::pivot_longer(\n    cols = c(prop_black, prop_white_nh, prop_hispanic),\n    names_to = \"race_ethnicity\",\n    values_to = 'proportion'\n  ) %>% \n  mutate(\n    race_ethnicity = recode(race_ethnicity, \n                            prop_white_nh = 'White, non-Hispanic',\n                            prop_black = 'Black, Hispanic or non-Hispanic',\n                            prop_hispanic = 'Hispanic'\n                            )\n  ) %>% \n  ggplot(aes(fill = proportion)) + \n  geom_sf(size = 0) + \n  facet_grid(~race_ethnicity) + \n  scale_fill_distiller(\n    palette = \"Greens\",\n    direction = 1,\n    labels = scales::percent_format()\n  ) + \n  theme_bw() + \n  labs(fill = \"Population Percentage\") + \n  ggtitle(\"Composition in Relation to Racialized Groups\",\n          subtitle = \"ZCTAs in Cook County (including Chicago); Data from ACS 2015-2019\") + \n  theme(legend.position = 'bottom',\n        legend.text = element_text(angle = 75, hjust=1),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.ticks.y = element_blank()\n        )\n\n# ggsave(\"./images/09-cook-county-covid/racial_ethnic_composition.png\", width = 12, height = 8)\n\n# visualize median income\ndf %>% \n  ggplot(aes(fill = median_income)) + \n  geom_sf(size = 0) + \n  scale_fill_viridis_c(direction = 1, \n                       labels = scales::dollar_format()) +\n  theme_bw() + \n  labs(fill = 'Median Income') + \n  theme(legend.position = 'bottom',\n        legend.text = element_text(angle = 75, hjust=1),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.ticks.y = element_blank()\n        ) + \n  ggtitle(\"Median Income\",\n          subtitle = \"ZCTAs in Cook County (including Chicago); Data from ACS 2015-2019\")\n\n# ggsave(\"./images/09-cook-county-covid/median_income.png\", width = 8, height = 8)\n\n# visualize ICEraceinc\ndf %>% \n  ggplot(aes(fill = ICEraceinc)) + \n  geom_sf(size = 0.01) + \n  scale_fill_distiller(palette = 'PRGn', direction = 1, limits = c(-1,1),\n                       labels = scales::number_format(accuracy = .01)) +\n  theme_bw() + \n  labs(fill = paste0(\"High Income (>$100k annual household income) White non-Hispanic (high) vs.\\n\",\n                     \"Low Income (<$25k annual household income) People of Color (low)\")) + \n  ggtitle(\"Index of Concentration at the Extremes for Racialized Economic Segregation\",\n          \"ZCTAs in Cook County (including Chicago); Data from ACS 2015-2019\") + \n  guides(fill = guide_colourbar(\n    title.position = \"top\",\n    title.hjust = 0.5,\n    barwidth = 10\n    \n  ))+\n  theme(\n    legend.position = 'bottom',\n        legend.title = element_text(size = 9),\n        legend.text = element_text(angle = 75, hjust = 1),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.ticks.y = element_blank()\n        )\n\n# ggsave(\"./images/09-cook-county-covid/ICEraceinc.png\", width = 8, height = 8)\n\n\n# visualize crowding \ndf %>% \n  ggplot(aes(fill = crowding)) + \n  geom_sf(size = 0) + \n  theme_bw() + \n    scale_fill_viridis_c(direction = 1, \n                       labels = scales::percent_format()) +\n  labs(fill = \"% Household Crowding\") + \n  ggtitle(\"Household Crowding\",\n          \"ZCTAs in Cook County (including Chicago); Data from ACS 2015-2019\") + \n  theme(\n    legend.position = 'bottom',\n        legend.title = element_text(size = 9),\n        legend.text = element_text(angle = 75, hjust = 1),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.ticks.y = element_blank()\n        )\n\n# ggsave(\"./images/09-cook-county-covid/crowding.png\", width = 8, height = 8)\n\n\n# visualize poverty\ndf %>% \n  ggplot(aes(fill = prop_in_poverty)) + \n  geom_sf(size = 0) + \n  scale_fill_distiller(palette = 'Purples', direction = 1, limits = c(0,NA),\n                       labels = scales::percent_format()) +\n  theme_bw() + \n  labs(fill = 'Poverty Level') + \n  ggtitle(\"Poverty Level\",\n          subtitle = \"ZCTAs in Cook County (including Chicago); Data from ACS 2015-2019\") + \n  theme(legend.position = 'bottom',\n        legend.text = element_text(angle = 75, hjust=1),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.ticks.y = element_blank()\n        )\n\n# ggsave(\"./images/09-cook-county-covid/poverty.png\", width = 8, height = 8)\n# launch an interactive map to view Cook County ICEraceinc by zip code\n# mapview::mapview(df, zcol = \"ICEraceinc\")"},{"path":"cook-county-covid.html","id":"analyzing-the-data","chapter":"9 Case Study 3: COVID-19 Mortality in Cook County (March 2020 - March 2022)","heading":"9.7 Analyzing the data","text":"","code":""},{"path":"cook-county-covid.html","id":"differences-in-covid-19-mortality-rates-by-racialized-group","chapter":"9 Case Study 3: COVID-19 Mortality in Cook County (March 2020 - March 2022)","heading":"9.7.1 Differences in COVID-19 mortality rates by racialized group","text":"First, can take look COVID-19 mortality rates racialized group aggregate. aggregating County level, looking gradients ABSMs.racialized disparities mortality across age groups. interpret , expected see? ?can fit models aggregate data adjust age obtain relative risk estimates.can fit Poisson, Quasi-Poisson, Negative Binomial models , depending assumptions overdispersion. code fits different models plots together demonstrate modeling assumptions may affect results. repeatedly various analysis can tedious. One idea write function fits three models given dataset model specifications.Poisson Quasi-Poisson models point estimate confidence interval Quasi-Poisson model much greater. Negative Binomial models different point estimates confidence intervals. expected? ?presenting, likely easier communicate results showing results one type model.","code":"\n#Since we are not taking into account ABSMs in this first analysis, aggregate the data and visualize\ncrude_mortality <- df %>% st_drop_geometry() %>% #dplyr works faster if we drop the geometry column.\n  group_by(age_group, race_ethnicity) %>%\n  dplyr::summarize(population_estimate = sum(population_estimate),\n            deaths = sum(deaths),\n            person_time = sum(person_time)) %>%\n  mutate(mortality_per100k_py = (deaths / person_time)*100000)\n\n#plot clude mortality and confidence intervals\nggplot() + \n  geom_pointrange(\n    data = crude_mortality,\n    aes(\n      x = age_group,\n      color = race_ethnicity,\n      y = mortality_per100k_py,\n      ymin = epitools::pois.exact(x=deaths, pt=person_time, conf.level=0.95)[,4]*1e5,\n      ymax = epitools::pois.exact(x=deaths, pt=person_time, conf.level=0.95)[,5]*1e5, #CIs calculated using pos.exact from epitools package\n      shape = race_ethnicity\n    ),\n    position = position_dodge(width = .4)) + \n  xlab(\"Age Group\") +\n  ylab(\"Mortality per 100,000 person years\")  +\n  labs(color = 'Racialized Group', shape = 'Racialized Group') +\n  scale_color_manual(values = c(\"#E69F00\", \"#56B4E9\", \"#009E73\")) + \n  scale_y_log10() + \n  ggtitle(\"Mortality Rates by Racialized Group\",\n          subtitle = \"Poisson Model Confidence Intervals Shown\") + \n  theme(axis.text.x = element_text(angle = 75, hjust=1)) \n\n# ggsave(\"./images/09-cook-county-covid/crude_rates_by_raceethnicity.png\", width = 8, height = 6)\n#Fit a Poisson model with offset for person time to get rate\ncrude_poisson_model <- glm(deaths ~ race_ethnicity + age_group + offset(log(person_time / 1e5)),\n                           data = crude_mortality, family = poisson(link = \"log\"))\n\n#Quasipoisson model\ncrude_quasipoisson_model <- glm(deaths ~ race_ethnicity + age_group + offset(log(person_time / 1e5)),\n                           data = crude_mortality, family = quasipoisson(link = 'log'))\n\n#Negative binomial model from MASS package\ncrude_negbin_model <- MASS::glm.nb(deaths ~ race_ethnicity + age_group + offset(log(person_time / 1e5)),\n                           data = crude_mortality) ##The MASS package is used to fit the negative binomial model\n\n# extract the model coefficients\ncrude_results <- bind_rows(\n  #The tidy function from broom makes it easy to extract useful model outputs\n  broom::tidy(crude_poisson_model, exponentiate = TRUE, conf.int = TRUE) %>% mutate(Model = \"Poisson\"),\n  broom::tidy(crude_quasipoisson_model, exponentiate = TRUE, conf.int = TRUE) %>% mutate(Model = \"Quasi-Poisson\"),\n  broom::tidy(crude_negbin_model, exponentiate = TRUE, conf.int = TRUE) %>% mutate(Model = \"Negative Binomial\")\n) %>%\n  dplyr::select(term, estimate, conf.low, conf.high, Model) %>%\n  filter(term != '(Intercept)') %>% # Remove the intercept term\n  filter(! stringr::str_detect(term, \"age_group\")) %>% #Clean up the names of variables\n  mutate(term = str_replace(term, \"race_ethnicity\", \"\")) #Clean up names of variables\n\n#Plot model results on same plot to compare\nggplot(crude_results,\n       aes(\n         x = estimate,\n         xmax = conf.high, #for confidence interval error bars\n         xmin = conf.low,\n         y = term,\n         color = Model, #Color, fill, shape grouped by model type.\n         fill = Model,\n         shape = Model,\n         group = Model\n       )\n) +\n  geom_vline(xintercept = 1, linetype = 'dashed') + #Line to represent RR Of 1\n  geom_pointrange(position = position_dodge(width = .45)) + #pointrange plots confidence interval and estimate\n  scale_x_continuous(\n    limits = c(0, NA),\n    n.breaks = 4,\n    trans = scales::pseudo_log_trans()\n  ) +\n  ylab(\"Racialized Group\") + \n  theme(axis.text.x = element_text(angle = 75, hjust = 1)) + \n  ggtitle(\"Modeled Incident Rate Ratios\",\n          subtitle = \"White, Non-Hispanic is the reference racialized group\")\n\n# ggsave(\"./images/09-cook-county-covid/modeled_IRR_by_raceethnicity.png\", width = 6, height = 4)"},{"path":"cook-county-covid.html","id":"gradients-in-covid-19-mortality-by-in-relation-to-absms","chapter":"9 Case Study 3: COVID-19 Mortality in Cook County (March 2020 - March 2022)","heading":"9.7.2 Gradients in COVID-19 mortality by in relation to ABSMs","text":"looked overall racialized disparities COVID-19 mortality, let us now take look mortality varies ABSMs. Like section , take age account adjusting age model. fit separate models ABSM considering.","code":"\n## We will aggregate data across racialized groups since we are looking at overall relationships\nage_ZCTA_mortality <- df %>% st_drop_geometry() %>%\n  group_by(geoid, age_group) %>%\n  dplyr::summarize(population_estimate = sum(population_estimate),\n            deaths = sum(deaths),\n            person_time = sum(person_time),\n            ICEraceinc_cut = first(ICEraceinc_cut), #The ABSMs are the same for every strata in each ZCTA, so we can just take the first one.\n            prop_in_poverty_cut = first(prop_in_poverty_cut),\n            median_income_cut = first(median_income_cut),\n            crowding_cut = first(crowding_cut),\n            prop_black_cut = first(prop_black_cut),\n            prop_hispanic_cut = first(prop_hispanic_cut),\n            prop_white_nh_cut = first(prop_white_nh_cut)) %>%\n  mutate(mortality_per100k_py = (deaths / person_time)*100000) %>%\n  mutate(race_ethnicity=\"All\")\n\n# specify our variables of interest\nvariables_of_interest <- c(\"ICEraceinc_cut\",\n      \"prop_in_poverty_cut\",\n      \"median_income_cut\",\n      \"crowding_cut\",\n      \"prop_black_cut\",\n      \"prop_hispanic_cut\",\n      \"prop_white_nh_cut\")\n\n# specify our model formulae with each variable of interest\nmodel_formula <- \n  purrr::map(variables_of_interest,\n    ~ paste0(\"deaths ~ \",\n             ., \" + age_group + offset(log(person_time / 1e5))\")\n  )\n\n# set the names of the model formulae accordingly \nnames(model_formula) <- variables_of_interest\n\n# estimate overall models\noverall_models <- \n  age_ZCTA_mortality %>% ungroup() %>% \n  nest_by(race_ethnicity) %>%\n  mutate(poisson = list(\n    purrr::map(model_formula, ~ glm(\n      .,\n      family = poisson(link = 'log'),\n      data = data\n    ))),\n    quasipoisson = list(purrr::map(model_formula, ~ glm(\n      .,\n      family = quasipoisson(link = 'log'),\n      data = data\n    ))),\n    negbin = list(purrr::map(model_formula, ~ MASS::glm.nb(.,\n                               data = data)))\n  )\n\n# extract the model coefficients\noverall_models %<>% mutate(\n  poisson = list(purrr::map(poisson, broom::tidy, exponentiate = TRUE, conf.int = TRUE)), \n  quasipoisson = list(purrr::map(quasipoisson, broom::tidy, exponentiate = TRUE, conf.int = TRUE)), \n  negbin = list(purrr::map(negbin, broom::tidy, exponentiate = TRUE, conf.int = TRUE))\n)\n\n# insert names for which model distribution they're associated with\noverall_models %<>% mutate(\n  poisson = list(purrr::map(names(poisson), ~ mutate(poisson[[.]], exposure_var = .))),\n  quasipoisson = list(purrr::map(names(quasipoisson), ~ mutate(quasipoisson[[.]], exposure_var = .))),\n  negbin = list(purrr::map(names(negbin), ~ mutate(negbin[[.]], exposure_var = .)))\n)\n\n# pivot longer so that each row represents a model coefficient\noverall_models %<>% pivot_longer(\n  cols = c(poisson, quasipoisson, negbin),\n  names_to = 'model',\n  values_to = 'coefficients'\n)\n\n# unnest the list column for coefficients\noverall_models %<>% tidyr::unnest(cols = coefficients)\noverall_models %<>% tidyr::unnest(cols = coefficients)\n\noverall_models %>%\n  filter(term != '(Intercept)') %>%\n  filter(! stringr::str_detect(term, \"age_group\")) %>%\n  mutate(term = stringr::str_remove(term, exposure_var)) %>% \n  #mutate(exposure_var = stringr::str_replace_all(exposure_var, \"_\", \" \")) %>% \n  mutate(exposure_var = recode(exposure_var, \n                               crowding_cut = \"Household Crowding\",\n                               ICEraceinc_cut = \"ICE for Racialized Economic Segregation\",\n                               median_income_cut = \"Median Income\",\n                               prop_hispanic_cut = \"Proportion Hispanic\",\n                               prop_black_cut = \"Proportion Black, Hispanic and non-Hispanic\",\n                               prop_white_nh_cut = \"Proportion White, non-Hispanic\",\n                               prop_in_poverty_cut = \"Proportion in Poverty\"\n                               )) %>%\n  mutate(exposure_var = stringr::str_wrap(exposure_var, 10)) %>% \n  mutate(term_size = as.numeric(stringr::str_extract(term, \"(?<=[\\\\(\\\\[])(.*)(?=,)\"))) %>% \n  mutate(term = forcats::fct_reorder(term, -term_size)) %>% \n  ggplot(\n    aes(\n      x = estimate,\n      xmax = conf.high,\n      xmin = conf.low,\n      y = term,\n      color = model,\n      fill = model,\n      shape = model,\n      group = model  \n    )\n  ) +\n  geom_vline(xintercept = 1, linetype = 'dashed') +\n  geom_pointrange(position = position_dodge(width=.45)) +\n  facet_grid(exposure_var~., scales='free_y') + \n  scale_x_continuous(\n    limits = c(0, NA),\n    n.breaks = 4,\n    trans = scales::pseudo_log_trans()\n  ) +\n  ylab(\"Weighted Quantile (excluding the Reference Category)\") + \n  theme(axis.text.x = element_text(angle = 75, hjust = 1)) + \n  ggtitle(\"Incident Rate Ratios\",\n          subtitle = \"Separate Models Fit for Each Categorical Exposure\")\n\n# ggsave(here(\"images/09-cook-county-covid/exposure_coefficient_estimates_ecological.png\"), width = 8, height = 8)"},{"path":"cook-county-covid.html","id":"gradients-in-covid-19-mortality-by-racialized-group-in-relation-to-absms","chapter":"9 Case Study 3: COVID-19 Mortality in Cook County (March 2020 - March 2022)","heading":"9.7.3 Gradients in COVID-19 mortality by Racialized Group in relation to ABSMs","text":"Now can take look mortality varies relation ABSMs, racialized group. build code section , now models racialized group ABSM.fit three types models, three racialized groups, seven ABSMs. Writing 3x3x7 models take space time, prone mistakes. code uses map function purrr package efficiently. agree approach? ways approach ?can change code focus fewer ABSMs investigate ABSMs interest presentation.various models ABSMs shown demonstration. However, may better focus one particular aspect (e.g. particular ABSM model), driven specific question understanding context. focus ?","code":"\n# specify our variables of interest\nvariables_of_interest <- c(\"ICEraceinc_cut\",\n      \"prop_in_poverty_cut\",\n      \"median_income_cut\",\n      \"crowding_cut\",\n      \"prop_black_cut\",\n      \"prop_hispanic_cut\",\n      \"prop_white_nh_cut\")\n\n# specify our model formulae with each variable of interest\nmodel_formula <- \n  purrr::map(variables_of_interest,\n    ~ paste0(\"deaths ~ \",\n             ., \" + age_group + offset(log(person_time / 1e5))\")\n  )\n\n# set the names of the model formulae accordingly \nnames(model_formula) <- variables_of_interest\n\n# estimate race/ethnicity stratified models\nrace_ethnicity_stratified_models <- \n  df %>% \n  nest_by(race_ethnicity) %>%\n  mutate(poisson = list(\n    purrr::map(model_formula, ~ glm(\n      .,\n      family = poisson(link = 'log'),\n      data = data\n    ))),\n    quasipoisson = list(purrr::map(model_formula, ~ glm(\n      .,\n      family = quasipoisson(link = 'log'),\n      data = data\n    ))),\n    negbin = list(purrr::map(model_formula, ~ MASS::glm.nb(.,\n                               data = data)))\n  )\n\n# extract the model coefficients\nrace_ethnicity_stratified_models %<>% mutate(\n  poisson = list(purrr::map(poisson, broom::tidy, exponentiate = TRUE, conf.int = TRUE)), \n  quasipoisson = list(purrr::map(quasipoisson, broom::tidy, exponentiate = TRUE, conf.int = TRUE)), \n  negbin = list(purrr::map(negbin, broom::tidy, exponentiate = TRUE, conf.int = TRUE))\n)\n\n# insert names for which model distribution they're associated with\nrace_ethnicity_stratified_models %<>% mutate(\n  poisson = list(purrr::map(names(poisson), ~ mutate(poisson[[.]], exposure_var = .))),\n  quasipoisson = list(purrr::map(names(quasipoisson), ~ mutate(quasipoisson[[.]], exposure_var = .))),\n  negbin = list(purrr::map(names(negbin), ~ mutate(negbin[[.]], exposure_var = .)))\n)\n\n# pivot longer so that each row represents a model coefficient\nrace_ethnicity_stratified_models %<>% pivot_longer(\n  cols = c(poisson, quasipoisson, negbin),\n  names_to = 'model',\n  values_to = 'coefficients'\n)\n\n# unnest the list column for coefficients\nrace_ethnicity_stratified_models %<>% tidyr::unnest(cols = coefficients)\nrace_ethnicity_stratified_models %<>% tidyr::unnest(cols = coefficients)\n\nrace_ethnicity_stratified_models %>%\n  filter(term != '(Intercept)') %>%\n  filter(! stringr::str_detect(term, \"age_group\")) %>%\n  mutate(term = stringr::str_remove(term, exposure_var)) %>% \n  #mutate(exposure_var = stringr::str_replace_all(exposure_var, \"_\", \" \")) %>% \n  mutate(exposure_var = recode(exposure_var, \n                               crowding_cut = \"Household Crowding\",\n                               ICEraceinc_cut = \"ICE for Racialized Economic Segregation\",\n                               median_income_cut = \"Median Income\",\n                               prop_hispanic_cut = \"Proportion Hispanic\",\n                               prop_black_cut = \"Proportion Black, Hispanic and non-Hispanic\",\n                               prop_white_nh_cut = \"Proportion White, non-Hispanic\",\n                               prop_in_poverty_cut = \"Proportion in Poverty\"\n                               )) %>%\n  mutate(exposure_var = stringr::str_wrap(exposure_var, 10)) %>% \n  mutate(term_size = as.numeric(stringr::str_extract(term, \"(?<=[\\\\(\\\\[])(.*)(?=,)\"))) %>% \n  mutate(term = forcats::fct_reorder(term, -term_size)) %>% \n  ggplot(\n    aes(\n      x = estimate,\n      xmax = conf.high,\n      xmin = conf.low,\n      y = term,\n      color = model,\n      fill = model,\n      shape = model,\n      group = model  \n    )\n  ) +\n  geom_vline(xintercept = 1, linetype = 'dashed') +\n  geom_pointrange(position = position_dodge(width=.45)) +\n  scale_x_continuous(limits = c(0, NA), n.breaks = 4, trans = scales::pseudo_log_trans()) + \n  facet_grid(exposure_var~race_ethnicity, scales='free_y') + \n  theme(axis.text.x = element_text(angle = 75, hjust = 1)) + \n  ggtitle(\"Incident Rate Ratios\",\n          subtitle = \"Separate Models Fit for Each Categorical Exposure and Racialized Group\")\n\n# ggsave(here(\"images/09-cook-county-covid/exposure_coefficient_estimates.png\"), width = 14, height = 18)\n\nrace_ethnicity_stratified_models %>%\n  filter(term != '(Intercept)') %>%\n  filter(stringr::str_detect(term, \"age_group\")) %>%\n  ggplot(\n    aes(\n      x = estimate,\n      xmax = conf.high,\n      xmin = conf.low,\n      y = term,\n      color = model,\n      fill = model,\n      shape = model,\n      group = model \n    )\n  ) +\n  geom_vline(xintercept = 1, linetype = 'dashed') +\n  geom_pointrange(position = position_dodge(width=.45)) +\n  scale_x_continuous(limits = c(0, NA), n.breaks = 4, trans = scales::pseudo_log_trans()) + \n  facet_grid(exposure_var~race_ethnicity, scales='free_y') + \n  theme(axis.text.x = element_text(angle = 75, hjust = 1)) + \n  ggtitle(\"Incident Rate Ratios\")\n\n# ggsave(here(\"images/09-cook-county-covid/age_coefficient_estimates.png\"), width = 14, height = 18)"},{"path":"cook-county-covid.html","id":"hierarchical-and-spatial-models","chapter":"9 Case Study 3: COVID-19 Mortality in Cook County (March 2020 - March 2022)","heading":"9.7.4 Hierarchical and Spatial Models","text":"","code":""},{"path":"cook-county-covid.html","id":"background-1","chapter":"9 Case Study 3: COVID-19 Mortality in Cook County (March 2020 - March 2022)","heading":"9.7.4.1 Background","text":"examples , modeled associations individual- area-based measures outcome aggregate across county. However, may also interested modeling risk mortality ZCTA. can help us better understand spatial distribution risk.may also want take account fact ZCTAs likely independent , rather related spatially structured fashion. , ZCTAs next likely similar rates ZCTAs farther apart.One statistical challenges estimating risk ZCTA (small areal unit), , due small underlying populations sparse events interspersed time, small-area estimation (SAE) risk incidence rates can quite unstable. led use various strategies smooth estimates, borrowing information nearby spatial units. Thus, accounting fact ZCTAs closer together likely similar, can obtain spatially smoothed risk estimates.Put another way: let us say particular part county high mortality rates. look observations limited period time, specific ZCTA separately, may miss just chance due fact populations individual ZCTAs small, thus fewer events observed. However, pool information neighboring ZCTAs, able obtain stable estimate risk.Hierarchical Bayesian models popular set tools conduct estimations. Often, approaches, model Standardized Mortality Ratio (SMR), standardized age. relative risk SMR area \\(\\), (\\(\\theta_i\\)) obtained dividing expected count (\\(E_i\\)) observed count \\(O_i\\)). calculate “expected count” depends objectives study analysis. example, know strong age effects, want estimate excess risk accounting age-composition differences ZCTAs, can take age-strata-specific mortality rates Cook County, apply age-distribution ZCTA. tells us “expected” mortality count ZCTA experienced Cook County’s average age-specific mortality rates. One also standardize composition relation racialized groups, able estimate membership racialized groups associated mortality. Similarly, age-standardization precludes ability look interaction age membership racialized groups, example.","code":""},{"path":"cook-county-covid.html","id":"smr-calculation","chapter":"9 Case Study 3: COVID-19 Mortality in Cook County (March 2020 - March 2022)","heading":"9.7.4.2 SMR Calculation","text":"case, estimate SMR using age-standardization. racialized group ZCTA, obtain age standardized SMR., dataset can calculate Observed count \\(O_i\\), Expected count \\(E_i\\) (based age standardization), SMR \\(\\theta_i\\). observed count approximately follows Poisson distribution, :\\[\nO_i \\sim Poisson(\\theta_i*E_i)\n\\], observed county one ZCTA follows Poisson distribution whose mean expected county times relative risk / SMR. interested last component since quantifies “excess” “reduced” risk area beyond “expected” age-composition. particular case, since actually strata racialized groups within ZCTA, can create hierarchical structure :\\[\nO_{ij} \\sim Poisson(\\theta_{ij}*E_{ij})\n\\]Now, looking observed, expected counts SMR within racialized group strata \\(j\\), within ZCTA \\(\\).Let us visualize crude SMRs ZCTA-racialized group strata, along confidence intervalsHere can see large variability strata-specific SMRs, well large confidence intervals many strata — especially predicted rate zero.","code":"\n## We will need to have a ZCTA specific ID number that starts from 1 eventually, so let's first make that\ndf <- df %>% group_by(geoid) %>%\n  mutate(id = cur_group_id()) %>% ungroup()\n  \n\n## Calculate overall age-specific mortality\noverall_mortality <- df %>% st_drop_geometry %>%\n  group_by(age_group) %>%\n  summarise(deaths = sum(deaths),\n            person_time = sum(person_time)) %>%\n  mutate(mortality_per_py = (deaths/person_time))\n\n\n## Apply this to each ZCTA\nsmr_df <- df %>% st_drop_geometry %>%\n  left_join(overall_mortality[, c(\"age_group\", \"mortality_per_py\")], by=\"age_group\") %>% #Join the overall age-specific mortality rates\n  mutate(Exp = mortality_per_py * person_time, #Multiply the overall age-specific mortality to the person-time in each ZCTA-race-age strata %>%\n         Obs = deaths) %>%\n  group_by(geoid, id, race_ethnicity) %>%\n  summarise(Exp = sum(Exp),\n            Obs = sum(Obs),\n            across(ICEraceinc:prop_white_nh_cut, ~ first(.x))) %>%\n  mutate(raw_SMR = Obs/Exp,\n         raw_CI95low = pois.exact(x=Obs, pt=Exp, conf.level=0.95)[,4],\n         raw_CI95up = pois.exact(x=Obs, pt=Exp, conf.level=0.95)[,5]) #calculate 95% CI bounds based on Poisson parameters \nggplot(data = smr_df %>% arrange(raw_SMR, desc(Exp)) %>% ungroup() %>% \n         mutate(order_id = row_number(),\n                raw_CI95up = ifelse(raw_CI95up > 40, 40, raw_CI95up))) + # Since there are large confidence intervals, we are going to cut them off at 40 for easier visualization.\n  geom_point(aes(x = order_id, y = raw_SMR), size = 0.2) +\n  geom_errorbar(aes(x = order_id, ymin = raw_CI95low, ymax = raw_CI95up),\n                size=0.2, alpha=0.4) +\n  ylim(0,40) +\n  ylab(\"Crude SMR\") +\n  xlab(\"ZCTA-Racialized Group strata\") +\n  theme_bw() +\n  theme(axis.text.x = element_blank())\n\n# ggsave(here(\"images/09-cook-county-covid/raw_SMR_caterpillar.png\"), height = 6, width = 8)"},{"path":"cook-county-covid.html","id":"fitting-models","chapter":"9 Case Study 3: COVID-19 Mortality in Cook County (March 2020 - March 2022)","heading":"9.7.4.3 Fitting Models","text":"Instead just calculating crude SMRs, can model using Poisson log-normal model :\\[\nlog(\\theta_{ij}) = \\beta_0 + v_i \\\\\n\\ v_i \\sim Normal(0,\\sigma^2_v)\n\\]county-wide intercept \\(\\beta_0\\), ZCTA random effects \\(v_i\\) quantify excess/reduced risk area county-wide average. Note spatial model still assuming ZCTAs independent .However, model fixed effects individual-level variables, estimating SMR racialized group strata within ZCTA. may just individual-level data. However, case, interested estimating effect membership racialized groups, can include fixed part model :\\[\nlog(\\theta_{ij}) = \\beta_0 + \\mathbf{\\beta}(race\\_ethnicity_j) + v_i \\\\\n\\ v_i \\sim Normal(0,\\sigma^2_v)\n\\]\n, county-wide intercept reference racialized group \\(\\beta_0\\), \\(\\beta\\)s racialized group. Note modeling constant effect racialized group across ZCTAs.Let us fit modelNext, mentioned , may want take account fact neighboring ZCTAs closely related. One way fit “Besag-York-Mollie” BYM model. builds previous model, partitions random effects two components - spatially structured one, spatially unstructured one. BYM model :\\[\nlog(\\theta_{ij}) = \\beta_0 + \\mathbf{\\beta}(race/ethnicity) + u_i + v_i \\\\\n\\ v_i \\sim Normal(0,\\sigma^2_v) \\ \\\\\nu_i \\sim Conditional \\ Autoregressive(W, \\sigma_u^2)\n\\]specifies part residual variation across ZCTAs spatially structured neighboring ZCTAs similar excess risks. part indicated new \\(u_i\\) introduced . accounting spatial structure, residual variation indicated \\(v_i\\). fitting models, can also calculate proportion residual variance spatially structured (, \\(u_i\\)), known spatial fraction.Let us fit model:","code":"\nformula_poisson_mod1 <- Obs ~ 1 + race_ethnicity + f(geoid, model=\"iid\")\n\nmodel_poisson_mod1 <- inla(formula_poisson_mod1, family=\"poisson\", data=smr_df, E=Exp, control.predictor=list(compute=TRUE), control.compute = list(dic = TRUE), verbose = F)\n#The control.compute option here calculates the Deviance Information Criteria which we could use to assess model fit, especially when comparing different formulations\n#The control.predictor option here computes the predicted values\n\n# Save the fitted SMRs and their confidence intervals for each ZCTA into the dataframe we have.\nsmr_df <- smr_df %>% ungroup() %>%\n  mutate(poissonmod1_SMR = model_poisson_mod1$summary.fitted.values$mean,\n         poissonmod1_CI95low = model_poisson_mod1$summary.fitted.values$`0.025quant`,\n         poissonmod1_CI95up = model_poisson_mod1$summary.fitted.values$`0.975quant`) %>%\n  left_join(model_poisson_mod1$summary.random$geoid[, c(\"ID\", \"mean\")], by=c(\"geoid\"=\"ID\")) %>%\n  mutate(mean = exp(mean)) %>%\n  rename(poissonmod1_RE = mean)\n#Calculate Neighbours matrix\nW.nb <- poly2nb(unique(df %>% dplyr::select(id)), snap=0.001)\n#W.list <- nb2listw(W.nb, style=\"B\", zero.policy = TRUE)\n\n# Visualize the neighbourhood matrix\ncoords <- st_coordinates(st_centroid(st_geometry(unique(df %>% dplyr::select(id)))))\nplot(st_geometry(unique(df %>% dplyr::select(id))), border=\"grey\")\nplot(W.nb, coords, add=TRUE)\n\n#Make adjacency matrix in format INLA can understand\nnb2INLA(\"INLA_adj_mat\", W.nb) #this saves a file in the working directory\nINLA_adj_mat <- \"INLA_adj_mat\"\n\nformula_bym_mod1 <- Obs ~ 1 + race_ethnicity + f(id, model=\"bym2\", graph=INLA_adj_mat, scale.model=TRUE, constr=TRUE)\n\nmodel_bym_mod1 <- inla(formula_bym_mod1, family=\"poisson\", data=smr_df, E=Exp, \n                       control.predictor=list(compute=TRUE), \n                       control.compute = list(dic = TRUE), verbose = F)\n\n#Save the fitted model estimates and Confidence intervals into the dataframe\nsmr_df <- smr_df %>% ungroup() %>%\n  mutate(bymmod1_SMR = model_bym_mod1$summary.fitted.values$mean,\n         bymmod1_CI95low = model_bym_mod1$summary.fitted.values$`0.025quant`,\n         bymmod1_CI95up = model_bym_mod1$summary.fitted.values$`0.975quant`) %>%\n  left_join(model_bym_mod1$summary.random$id[, c(\"ID\", \"mean\")], by=c(\"id\"=\"ID\")) %>%\n  mutate(mean = exp(mean)) %>%\n  rename(bymmod1_RE = mean)\n\n## Calculate Spatial Fraction\n# This is a bit verbose, and INLA does not come with a built-in function to calculate this. \n# Going to create a function to calculate this\n\nspatial_frac <- function(inla_model, numarea){\n  mat.marg <- matrix(NA, nrow=numarea, ncol=100000) #create empty matrix\n  m <- inla_model$marginals.random$id\n  \n  for (i in 1:numarea){\n  #the first block of the random effects matrix contains area-specific effects (u + v), \n  #and the second block contains spatially structured residuals (u). \n  #So this is extracting from the second block of rows\n    u <- m[[numarea+i]] \n    mat.marg[i,] <- inla.rmarginal(100000, u) #randomly pick 100000 values from posterior distributions of area-specific spatially structures residuals\n    }\n#Get empirical variance from 100000 obs\n  var.u <- apply(mat.marg, 2, var) \n\n#Get unstructured variance\n  var.v <- inla.rmarginal(100000,\n                          inla.tmarginal(function(x) 1/x, \n                                         inla_model$marginals.hyperpar$`Precision for id`))\n\n#Calculate spatially structured variance percentage\n  perc.var.u <- mean(var.u/(var.u+var.v))\n  return(perc.var.u)\n}\n\n\nspatial_frac(model_bym_mod1, length(unique(smr_df$geoid)))"},{"path":"cook-county-covid.html","id":"comparing-fitted-smrs","chapter":"9 Case Study 3: COVID-19 Mortality in Cook County (March 2020 - March 2022)","heading":"9.7.4.4 Comparing Fitted SMRs","text":"fit models, let us visualize components see gain using different types models, differ.First, can look estimates fixed effects. Since non-hierarchical models also estimated fixed effects, can include comparison, caveat modeling SMRs hierarchical models.One motivations fitting hierarchical models get stable smoothed estimates ZCTA. Let us visualize modeled SMRs ZCTA-racialized group strata, along confidence intervals, compare crude SMRs calculated beginning.caterpillar plots demonstrate noisiness raw SMRs. Note cut y-axis range allow comparison within limited range. models smooth SMRs.plots also demonstrates pitfalls identifying small areas extreme rates based crude SMRs . may small underlying population, high confidence intervals.hierarchical models can useful visualize estimated ZCTA-specific effects. Since racialized group strata within ZCTAs, can plot maps group.spatial fraction 72%.see crude calculated SMRs noisy. Using Poisson hierarchical BYM models clear benefits smoothing rates. However, seem much difference, high level, Poisson hierarchical BYM model.** IMPORTANT NOTE **look smoothed maps, can see risk lowest White non-Hispanic group. However, spatial patterning risk similar racialized group maps. cautious concluding spatial patterning . result constrained model, model spatial component separately group. going next fitting separate models racialized group.spatial fraction residual variance 61% White, non-Hispanic model; 67.8% Black; Hispanic non-Hispanic model; 77% Hispanic model.can visualize smoothed SMRs separate models.","code":"\n#The tidy functions from broom work on most commonly used models in R, but INLA is not one of them. \n#This is a helper function to extract coefficients and credible/\"confidence\" intervals.\ntidy.inla <- function(x){\n  \n  # x = model_inla\n  term_names <- rownames(x$summary.fixed)\n\n  tibble::as_tibble(x$summary.fixed) %>%\n    dplyr::mutate(terms = term_names) %>%\n    dplyr::rename(term = terms,\n                  estimate = mean,\n                  std.error = sd,\n                  conf.low = `0.025quant`, \n                  conf.high = `0.975quant`) %>%\n    dplyr::select(term, estimate, std.error,\n                  conf.low, conf.high)\n}\n\ncoefficients_together <-\n  bind_rows(crude_results,\n            tidy.inla(model_poisson_mod1) %>%\n              mutate(Model = \"Hierarchical Poisson\",\n                     across(estimate:conf.high, ~ exp(.x))),\n            tidy.inla(model_bym_mod1) %>%\n              mutate(Model = \"BYM\",\n                     across(estimate:conf.high, ~ exp(.x)))) %>%\n  mutate(term = str_replace(term, \"race_ethnicity\", \"\"))\n# visualize the effects in each model\ncoefficients_together %>%\n  filter(term != '(Intercept)') %>%\n  filter(! stringr::str_detect(term, \"age_group\")) %>%\n  ggplot(aes(x = estimate, xmax = conf.high, xmin = conf.low, y = term, color = Model, shape = Model)) +\n  geom_vline(xintercept = 1, linetype = 'dashed') +\n  geom_pointrange(position = position_dodge(width=.45)) +\n  scale_x_continuous(limits = c(0, NA), n.breaks = 4) +\n  ggtitle(\"Comparing Coefficient Estimates for Racialized Groups from Different Models\")\n\n# ggsave(here(\"images/09-cook-county-covid/coefficient_comparison.png\"), height = 6, width = 8)\n# Creating a caterpillar plot to visualize the unsmoothed and smoothed SMRs.\ncaterpillar_plot <- smr_df %>% arrange(raw_SMR, desc(Exp)) %>% ungroup() %>% \n  mutate(order_id = row_number(),\n         raw_CI95up = ifelse(raw_CI95up > 10, 10, raw_CI95up)) %>%\n  dplyr::select(!ends_with(\"_RE\")) %>%\n  #Pivot longer to help with ggplot\n  pivot_longer(cols = raw_SMR:bymmod1_CI95up, \n               names_to = c(\"SMR_type\", \".value\"),\n               names_sep = \"_\") %>%\n  mutate(SMR_type = ifelse(SMR_type == \"bymmod1\", \"BYM\",\n                           ifelse(SMR_type == \"poissonmod1\", \"Hierarchical Poisson\",\n                                  \"Raw\")))\n\nggplot(data = caterpillar_plot) +\n  geom_point(aes(x = order_id, y = SMR, color = SMR_type, \n                 group = SMR_type), size = 0.2) +\n  geom_errorbar(aes(x = order_id, ymin = CI95low, ymax = CI95up, color = SMR_type, \n                 group = SMR_type),\n                size=0.2, alpha=0.4) +\n  facet_wrap(~SMR_type) +\n  ylim(0,10) +\n  ylab(\"Estimated and Crude SMRs\") +\n  xlab(\"ZCTA-Racialized Group strata\") +\n  theme_bw() +\n  theme(axis.text.x = element_blank()) +\n  labs(color = \"SMR Type\") +\n  ggtitle(\"Raw and Modeled SMRs for each ZCTA-Racialized Group strata\")\n  \n\n# ggsave(here(\"images/09-cook-county-covid/caterpillar_plot_SMRs.png\"), width = 9, height = 6)\n# Similar code as for the caterpillar plot above, but we are going to plot just the point estimates on a map, and not the CIs\nsmr_df %>% \n  arrange(raw_SMR, desc(Exp)) %>% ungroup() %>% \n  mutate(order_id = row_number(),\n         raw_CI95up = ifelse(raw_CI95up > 10, 10, raw_CI95up)) %>%\n  dplyr::select(!ends_with(\"_RE\")) %>%\n  pivot_longer(cols = raw_SMR:bymmod1_CI95up, \n               names_to = c(\"SMR_type\", \".value\"),\n               names_sep = \"_\") %>%\n  left_join(df %>% dplyr::select(geoid, geometry), by=\"geoid\") %>% st_as_sf(sf_column_name = \"geometry\") %>%\n  mutate(SMR_type = ifelse(SMR_type == \"bymmod1\", \"BYM\",\n                           ifelse(SMR_type == \"poissonmod1\", \"Hierarchical Poisson\",\n                                  \"Raw\"))) %>%\n  ggplot(aes(fill = SMR)) + \n  geom_sf(size = 0.1) + \n  facet_grid(forcats::fct_rev(race_ethnicity)~SMR_type) + \n  scale_fill_distiller(palette = \"BrBG\",\n                       trans = scales::pseudo_log_trans(sigma=0.01),\n                       limits = exp(c(-1,1)*log(4)),\n                       breaks = c(0.25,0.5,1,2,4), \n                       oob = scales::squish) +\n  theme_bw() + \n  theme(legend.position = 'bottom',\n        legend.text = element_text(angle = 75, hjust=1),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.ticks.y = element_blank(),\n        panel.grid = element_blank()\n        ) +\n  ggtitle(\"Raw and Modeled SMR Maps\")\n\n# ggsave(here(\"images/09-cook-county-covid/modeled_SMR_comparison_1.png\"), width = 10, height = 8)\n## Fitting the models for the White Non-Hispanic group\nsmr_df_wnh <- smr_df %>% \n  filter(race_ethnicity == \"White, non-Hispanic\")\n#Poisson\nformula_poisson_wnh <- Obs ~ 1 + f(geoid, model=\"iid\")\n\nmodel_poisson_wnh <- inla(formula_poisson_wnh, family=\"poisson\", \n                          data = smr_df_wnh, E=Exp,\n                          control.predictor=list(compute=TRUE), \n                          control.compute = list(dic = TRUE), verbose = F)\n\n# As above for the overall models, save the fitted values and CIs\nsmr_df_wnh <- smr_df_wnh %>% ungroup() %>%\n  mutate(poissonmodspecific_SMR = model_poisson_wnh$summary.fitted.values$mean,\n         poissonmodspecific_CI95low = model_poisson_wnh$summary.fitted.values$`0.025quant`,\n         poissonmodspecific_CI95up = model_poisson_wnh$summary.fitted.values$`0.975quant`) %>%\n  left_join(model_poisson_wnh$summary.random$geoid[, c(\"ID\", \"mean\")], by=c(\"geoid\"=\"ID\")) %>%\n  mutate(mean = exp(mean)) %>%\n  rename(poissonmodspecific_RE = mean)\n\n\n#BYM\nformula_bym_wnh <- Obs ~ 1 + f(id, model=\"bym2\", graph=INLA_adj_mat, scale.model=TRUE, constr=TRUE)\n\nmodel_bym_wnh <- inla(formula_bym_wnh, family=\"poisson\", \n                          data = smr_df_wnh, E=Exp,\n                          control.predictor=list(compute=TRUE), \n                          control.compute = list(dic = TRUE), verbose = F)\n\n#Save fitted values from model\nsmr_df_wnh <- smr_df_wnh %>% ungroup() %>%\n  mutate(bymmodspecific_SMR = model_bym_wnh$summary.fitted.values$mean,\n         bymmodspecific_CI95low = model_bym_wnh$summary.fitted.values$`0.025quant`,\n         bymmodspecific_CI95up = model_bym_wnh$summary.fitted.values$`0.975quant`) %>%\n  left_join(model_bym_wnh$summary.random$id[, c(\"ID\", \"mean\")], by=c(\"id\"=\"ID\")) %>%\n  mutate(mean = exp(mean)) %>%\n  rename(bymmodspecific_RE = mean)\n\nspatial_frac(model_bym_wnh, length(unique(smr_df_wnh$geoid)))\n\n## Fitting Black Hispanic and non-Hispanic groups\nsmr_df_bhnh <- smr_df %>% \n  filter(race_ethnicity == \"Black, Hispanic or non-Hispanic\")\n#Poisson\nformula_poisson_bhnh <- Obs ~ 1 + f(geoid, model=\"iid\")\n\nmodel_poisson_bhnh <- inla(formula_poisson_bhnh, family=\"poisson\", \n                          data = smr_df_bhnh, E=Exp,\n                          control.predictor=list(compute=TRUE), \n                          control.compute = list(dic = TRUE), verbose = F)\n\n#Save fitted values from model\nsmr_df_bhnh <- smr_df_bhnh %>% ungroup() %>%\n  mutate(poissonmodspecific_SMR = model_poisson_bhnh$summary.fitted.values$mean,\n         poissonmodspecific_CI95low = model_poisson_bhnh$summary.fitted.values$`0.025quant`,\n         poissonmodspecific_CI95up = model_poisson_bhnh$summary.fitted.values$`0.975quant`) %>%\n  left_join(model_poisson_bhnh$summary.random$geoid[, c(\"ID\", \"mean\")], by=c(\"geoid\"=\"ID\")) %>%\n  mutate(mean = exp(mean)) %>%\n  rename(poissonmodspecific_RE = mean)\n\n\n#BYM\nformula_bym_bhnh <- Obs ~ 1 + f(id, model=\"bym2\", graph=INLA_adj_mat, scale.model=TRUE, constr=TRUE)\n\nmodel_bym_bhnh <- inla(formula_bym_bhnh, family=\"poisson\", \n                          data = smr_df_bhnh, E=Exp,\n                          control.predictor=list(compute=TRUE), \n                          control.compute = list(dic = TRUE), verbose = F)\n\n#Save fitted values from model\nsmr_df_bhnh <- smr_df_bhnh %>% ungroup() %>%\n  mutate(bymmodspecific_SMR = model_bym_bhnh$summary.fitted.values$mean,\n         bymmodspecific_CI95low = model_bym_bhnh$summary.fitted.values$`0.025quant`,\n         bymmodspecific_CI95up = model_bym_bhnh$summary.fitted.values$`0.975quant`) %>%\n  left_join(model_bym_bhnh$summary.random$id[, c(\"ID\", \"mean\")], by=c(\"id\"=\"ID\")) %>%\n  mutate(mean = exp(mean)) %>%\n  rename(bymmodspecific_RE = mean)\n\nspatial_frac(model_bym_bhnh, length(unique(smr_df_bhnh$geoid)))\n\n## Fitting Hispanic Group\nsmr_df_h <- smr_df %>% \n  filter(race_ethnicity == \"Hispanic\")\n#Poisson\nformula_poisson_h <- Obs ~ 1 + f(geoid, model=\"iid\")\n\nmodel_poisson_h <- inla(formula_poisson_h, family=\"poisson\", \n                          data = smr_df_h, E=Exp,\n                          control.predictor=list(compute=TRUE), \n                          control.compute = list(dic = TRUE), verbose = F)\n\n#Save fitted values from model\nsmr_df_h <- smr_df_h %>% ungroup() %>%\n  mutate(poissonmodspecific_SMR = model_poisson_h$summary.fitted.values$mean,\n         poissonmodspecific_CI95low = model_poisson_h$summary.fitted.values$`0.025quant`,\n         poissonmodspecific_CI95up = model_poisson_h$summary.fitted.values$`0.975quant`) %>%\n  left_join(model_poisson_h$summary.random$geoid[, c(\"ID\", \"mean\")], by=c(\"geoid\"=\"ID\")) %>%\n  mutate(mean = exp(mean)) %>%\n  rename(poissonmodspecific_RE = mean)\n\n\n#BYM\nformula_bym_h <- Obs ~ 1 + f(id, model=\"bym2\", graph=INLA_adj_mat, scale.model=TRUE, constr=TRUE)\n\nmodel_bym_h <- inla(formula_bym_h, family=\"poisson\", \n                          data = smr_df_h, E=Exp,\n                          control.predictor=list(compute=TRUE), \n                          control.compute = list(dic = TRUE), verbose = F)\n\n#Save fitted values from model\nsmr_df_h <- smr_df_h %>% ungroup() %>%\n  mutate(bymmodspecific_SMR = model_bym_h$summary.fitted.values$mean,\n         bymmodspecific_CI95low = model_bym_h$summary.fitted.values$`0.025quant`,\n         bymmodspecific_CI95up = model_bym_h$summary.fitted.values$`0.975quant`) %>%\n  left_join(model_bym_h$summary.random$id[, c(\"ID\", \"mean\")], by=c(\"id\"=\"ID\")) %>%\n  mutate(mean = exp(mean)) %>%\n  rename(bymmodspecific_RE = mean)\n\nspatial_frac(model_bym_h, length(unique(smr_df_h$geoid)))\n#Combine all of the racialized group specific results into one\nsmr_df <- smr_df %>%\n  left_join(\n    bind_rows(smr_df_wnh, smr_df_bhnh, smr_df_h) %>%\n      dplyr::select(geoid, race_ethnicity, poissonmodspecific_SMR:bymmodspecific_RE),\n    by = c(\"geoid\", \"race_ethnicity\"))\n\n# Similar to how we plotted the maps for overall fitted SMRs in each ZCTA, we do the same by racialized group\nsmr_df %>% \n  arrange(raw_SMR, desc(Exp)) %>% ungroup() %>% \n  mutate(order_id = row_number(),\n         raw_CI95up = ifelse(raw_CI95up > 10, 10, raw_CI95up)) %>%\n  dplyr::select(!ends_with(\"_RE\")) %>%\n  pivot_longer(cols = c(raw_SMR:raw_CI95up, poissonmodspecific_SMR:bymmodspecific_CI95up), \n               names_to = c(\"SMR_type\", \".value\"),\n               names_sep = \"_\") %>%\n  left_join(df %>% dplyr::select(geoid, geometry), by=\"geoid\") %>% st_as_sf(sf_column_name = \"geometry\") %>%\n  mutate(SMR_type = recode(SMR_type,\n                           bymmodspecific = \"BYM\", \n                           poissonmodspecific = \"Hierarchical Poisson\",\n                           raw = \"Raw\")) %>%\n  ggplot(aes(fill = SMR)) + \n  geom_sf(size = 0.1) + \n  facet_grid(forcats::fct_rev(race_ethnicity)~SMR_type) + \n  scale_fill_distiller(palette = \"BrBG\",\n                       trans = scales::pseudo_log_trans(sigma=0.01),\n                       limits = exp(c(-1,1)*log(4)),\n                       breaks = c(0.25,0.5,1,2,4), \n                       oob = scales::squish) +\n  theme_bw() + \n  theme(legend.position = 'right', \n        panel.grid = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.ticks.y = element_blank()\n        ) +\n  ggtitle(\"Modeled SMR Maps\", subtitle = \"Each Racialized Group Modeled Separately\") \n\n# ggsave(here(\"images/09-cook-county-covid/modeled_SMR_race_specific.png\"), width = 12, height = 8)"},{"path":"cook-county-covid.html","id":"additional-analysis","chapter":"9 Case Study 3: COVID-19 Mortality in Cook County (March 2020 - March 2022)","heading":"9.7.4.5 Additional Analysis","text":"may also interested looking area-specific residuals. give us sense much area’s risk deviates county-wide mean, conditional covariates include model. can visualize residuals models (covariates included).Finally, non-hierarchical models , can also add ABSMs hierarchical models. model follows:\\[\nlog(\\theta_{ij}) = \\beta_0 + \\mathbf{\\beta}(ABSM_i) + u_i + v_i \\\\\n\\ v_i \\sim Normal(0,\\sigma^2_v) \\ ,\\  \\\\\nu_i \\sim Conditional \\ Autoregressive(W, \\sigma_u^2)\n\\]won’t explore models , simply add covariates model equations fit code .","code":"\nsmr_df %>% \n  arrange(raw_SMR, desc(Exp)) %>% ungroup() %>% \n  mutate(order_id = row_number(),\n         raw_CI95up = ifelse(raw_CI95up > 10, 10, raw_CI95up)) %>%\n  dplyr::select(!ends_with(c(\"_SMR\", \"_CI95up\", \"_CI95low\"))) %>%\n  pivot_longer(cols = c(poissonmodspecific_RE:bymmodspecific_RE), \n               names_to = c(\"RE_type\", \".value\"),\n               names_sep = \"_\") %>%\n  left_join(df %>% dplyr::select(geoid, geometry), by=\"geoid\") %>% st_as_sf(sf_column_name = \"geometry\") %>%\n  mutate(SMR_type = recode(RE_type,\n                           bymmodspecific = \"BYM\", \n                           poissonmodspecific = \"Hierarchical Poisson\")) %>%\n  ggplot(aes(fill = RE)) + \n  geom_sf(size = 0.1) + \n  facet_grid(forcats::fct_rev(race_ethnicity)~SMR_type) + \n  scale_fill_distiller(palette = \"BrBG\",\n                       trans = scales::pseudo_log_trans(sigma=0.01),\n                       limits = exp(c(-1,1)*log(4)),\n                       breaks = c(0.25,0.5,1,2,4), \n                       oob = scales::squish) +\n  theme_bw() + \n  theme(legend.position = 'right', \n        panel.grid = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.ticks.y = element_blank()\n        ) +\n  labs(fill = \"Residual SMR\") +\n  ggtitle(\"ZCTA-specific model residuals\", subtitle = \"Each Racialized Group Modeled Separately\") \n\n# ggsave(here(\"images/09-cook-county-covid/model_residuals_race_specific.png\"), width = 9, height = 8)"},{"path":"temporal-health-insurance.html","id":"temporal-health-insurance","chapter":"10 Case Study 4: Case Study on Temporal Trends using American Community Survey (ACS) data (2012-2019)","heading":"10 Case Study 4: Case Study on Temporal Trends using American Community Survey (ACS) data (2012-2019)","text":": Dena Javadi, Tamara Rushovich, Christian Testa","code":""},{"path":"temporal-health-insurance.html","id":"introduction-2","chapter":"10 Case Study 4: Case Study on Temporal Trends using American Community Survey (ACS) data (2012-2019)","heading":"10.1 Introduction","text":"American Community Survey (ACS), conducted U.S. census bureau yearly survey (monthly samples providing annual updates) provides vital information educational attainment, jobs, occupations, veterans, house ownership, disability, poverty status, demographics, .learn survey methodology used, please visit ACS website.","code":""},{"path":"temporal-health-insurance.html","id":"motivation-research-questions-and-learning-objectives-3","chapter":"10 Case Study 4: Case Study on Temporal Trends using American Community Survey (ACS) data (2012-2019)","heading":"10.2 Motivation, Research Questions, and Learning Objectives","text":"goal case study gain familiarity ACS, visualize trends time, model effect different Area-Based Social Metrics (ABSMs) trends particular outcome.outcome interest case study health insurance. specific learning objectives :Download health outcome ABSM data ACSDownload health outcome ABSM data ACSVisualize (plot) map dataVisualize (plot) map dataCharacterize trends outcome interest ABSMsCharacterize trends outcome interest ABSMsWork estimate uncertainty (margin error) data :\nAggregate stratified estimates (like aggregating private public insurance rates together overall insurance rates)\nVisualize uncertainty individual areal units time\nModel trends time\nWork estimate uncertainty (margin error) data :Aggregate stratified estimates (like aggregating private public insurance rates together overall insurance rates)Visualize uncertainty individual areal units timeModel trends timeAlthough case study follow insurance rates time specified counties, skills learned generalizable pulling ACS variables geographic levels.case study, particularly interested observed differences health insurance coverage across four US states (Massachusetts, California, Texas, Florida) 2012-2019. research questions :trends health insurance coverage 2012-2019 state level certainty trends?county-level poverty associated health insurance coverage 2012-2019 state?county-level racialized economic segregation measured Index Concentration Extremes (ICE) associated health insurance coverage 2012-2019 state?next section, Downloading Data, show download ACS data querying census API manipulate data format need rest analysis. case study investigates ACS variable health insurance four states. See can replicate analysis different ACS variable different states!","code":""},{"path":"temporal-health-insurance.html","id":"downloading-your-data","chapter":"10 Case Study 4: Case Study on Temporal Trends using American Community Survey (ACS) data (2012-2019)","heading":"10.3 Downloading your data","text":"Explore ACS data dictionary identify variables interest create data dictionary. example 2016 dictionary.Congrats! ’ve downloaded dataset!","code":"\n# We need to load the following packages:\nlibrary(tidycensus)\nlibrary(tidyverse)\nlibrary(tidyr)\nlibrary(magrittr) # magrittr defines the %<>% operator, read more here: https://magrittr.tidyverse.org/\nlibrary(patchwork)\n\n# build a data dictionary - based on ACS 2013\ndata_dictionary <- tibble::tribble(\n  ~variable,     ~shortname,                       ~description,\n  \"B18135_002\",  'under18_denom',                   \"Estimate!!Total!!Under 18 years\",\n  \"B18135_004\",  'under18_insured_with_disability', \"Estimate!!Total!!Under 18 years!!With a disability!!With health insurance coverage\",\n  \"B18135_009\",  'under18_insured_no_disability',   \"Estimate!!Total!!Under 18 years!!No disability!!With health insurance coverage\",\n  \"B18135_013\",  'adult_denom',                     \"Estimate!!Total!!18 to 64 years\",\n  \"B18135_015\",  'adult_insured_with_disability',   \"Estimate!!Total!!18 to 64 years!!With a disability!!With health insurance coverage\",\n  \"B18135_020\",  'adult_insured_no_disability',     \"Estimate!!Total!!18 to 64 years!!No disability!!With health insurance coverage\",\n  # ICEraceinc variables\n  \"B19001_001\",  'hhinc_total',   \"total population for household income estimates\",\n  \"B19001A_002\", 'hhinc_w_1',     \"white n.h. pop with household income <$10k\",\n  \"B19001A_003\", 'hhinc_w_2',     \"white n.h. pop with household income $10k-14 999k\",\n  \"B19001A_004\", 'hhinc_w_3',     \"white n.h. pop with household income $15k-19 999k\",\n  \"B19001A_005\", 'hhinc_w_4',     \"white n.h. pop with household income $20k-24 999k\",\n  \"B19001A_014\", 'hhinc_w_5',     \"white n.h. pop with household income $100 000 to $124 999\",\n  \"B19001A_015\", 'hhinc_w_6',     \"white n.h. pop with household income $125k-149 999k\",\n  \"B19001A_016\", 'hhinc_w_7',     \"white n.h. pop with household income $150k-199 999k\",\n  \"B19001A_017\", 'hhinc_w_8',     \"white n.h. pop with household income $196k+\",\n  \"B19001_002\",  'hhinc_total_1', \"total pop with household income <$10k\",\n  \"B19001_003\",  'hhinc_total_2', \"total pop with household income $10k-14 999k\",\n  \"B19001_004\",  'hhinc_total_3', \"total pop with household income $15k-19 999k\",\n  \"B19001_005\",  'hhinc_total_4', \"total pop with household income $20k-24 999k\",\n  \"B05010_002\",  'in_poverty',    \"total pop in poverty\",\n  \"B05010_001\", 'total_pop_for_poverty_estimates', \n                                  \"denominator population for poverty estimates\"\n)\n\n# another way to create a data dictionary is to use tidycensus to get the variables tables and filter it:\nacs2012variables <- tidycensus::load_variables(year = 2012, dataset = 'acs5')\nacs2012variables %<>% filter(\n  name %in% c(\"B18135_002\", \"B18135_004\", \"B18135_009\", \"B18135_013\", \"B18135_015\", \"B18135_020\")) \n# then you could append `shortname` data if you'd like\n\n# write a function to help query county insurance rates by state\nget_county_health_insurance <- function(state, year) {\n  get_acs(state = state, year = year, geography = 'county', variables = data_dictionary$variable)\n}\n\n# create a table shell for each year of data for each of texas, massachusetts, florida, and california\nhealth_insurance <- expand.grid(state = c('TX', 'MA', 'FL', 'CA'), year = 2012:2019)\n\n# query the data from ACS\nhealth_insurance %<>% \n  rowwise() %>% \n  mutate(\n  acs_data = list(get_county_health_insurance(state = state, year = year))) \n\n# unnest our data\nhealth_insurance %<>% tidyr::unnest(cols = acs_data)\n\n# recode variables to use friendlier shortnames\n# the !!! operator, sometimes called the splat operator, expands a named vector into named argumes for a function;\n# in this case, the output of setNames() is a named character vector with variable shortnames as the names and \n# variable codes as the values.  Passing these to !!! and then recode says \"rename the variable codes to the \n# more human-readable, shortnames\"\nhealth_insurance$variable %<>% recode( !!! setNames(data_dictionary$shortname, data_dictionary$variable))\n\n# pivot into wide format\nhealth_insurance %<>% \n  tidyr::pivot_wider(\n    id_cols = c('year', 'state', 'GEOID', 'NAME'),\n    names_from = 'variable',\n    values_from = c('estimate', 'moe'))\n\n# calculate adult (18+ years), child, and 0-64 insurance rates by county and year \n# Below we are add the estimated number of insured adults with a disability and the number of insured adults without a disability \n# to get the total number of insured adults. We do the same for children and people aged 0-64 years.\nhealth_insurance %<>% mutate(\n  # calculate numerators\n  estimate_adult_insurance_numerator = (estimate_adult_insured_with_disability + estimate_adult_insured_no_disability),\n  estimate_under18_insurance_numerator = (\n    estimate_under18_insured_with_disability + estimate_under18_insured_no_disability\n  ),\n  estimate_zero_to_64_insurance_numerator = (\n    estimate_adult_insured_with_disability +\n      estimate_adult_insured_no_disability +\n      estimate_under18_insured_with_disability +\n      estimate_under18_insured_no_disability\n  ),\n  # calculate denominator\n  estimate_zero_to_64_insurance_denom = estimate_under18_denom + estimate_adult_denom,\n  # calculate proportions\n  estimate_adult_insurance_prop = estimate_adult_insurance_numerator / estimate_adult_denom,\n  estimate_under18_insurance_prop = estimate_under18_insurance_numerator / estimate_under18_denom,\n  estimate_zero_to_64_insurance_prop = estimate_zero_to_64_insurance_numerator / (estimate_zero_to_64_insurance_denom),\n  \n  # calculate social metrics\n  poverty_prop = estimate_in_poverty / estimate_total_pop_for_poverty_estimates,\n  \n      # we calculate the people of color low income counts as the overall \n    # low income counts minus the White non-Hispanic low income counts\n    people_of_color_low_income = \n      (estimate_hhinc_total_1 + estimate_hhinc_total_2 + estimate_hhinc_total_3 + estimate_hhinc_total_4) - \n      (estimate_hhinc_w_1 + estimate_hhinc_w_2 + estimate_hhinc_w_3 + estimate_hhinc_w_4),\n    # sum up the White non-Hispanic high income counts\n    White_non_hispanic_high_income = \n      (estimate_hhinc_w_5 + estimate_hhinc_w_6 + estimate_hhinc_w_7 + estimate_hhinc_w_8),\n    # calculate the index of concentration at the extremes for racialized \n    # economic segregation (high income White non-Hispanic vs. low income \n    # people of color)\n    ICEraceinc = \n      (white_non_hispanic_high_income - people_of_color_low_income) / \n      estimate_hhinc_total,\n  )\n\n# use the moe_sum and moe_prop functions from tidycensus to create margin of error\n# for aggregated estimates and proportion variables\nhealth_insurance %<>% rowwise() %>% \n  mutate(\n    # calculate moe for numerators\n  moe_adult_insurance_numerator = tidycensus::moe_sum(\n    moe = c(moe_adult_insured_with_disability, moe_adult_insured_no_disability),\n    estimate = c(estimate_adult_insured_with_disability, estimate_adult_insured_no_disability)), \n  moe_under18_insurance_numerator = tidycensus::moe_sum(\n    moe = c(moe_under18_insured_with_disability, moe_under18_insured_no_disability),\n    estimate = c(estimate_under18_insured_with_disability, estimate_under18_insured_no_disability)), \n  moe_zero_to_64_insurance_numerator = tidycensus::moe_sum(\n    moe = c(moe_adult_insured_with_disability,\n      moe_adult_insured_no_disability,\n      moe_under18_insured_with_disability,\n      moe_under18_insured_no_disability),\n    estimate = c(estimate_adult_insured_with_disability,\n      estimate_adult_insured_no_disability,\n      estimate_under18_insured_with_disability,\n      estimate_under18_insured_no_disability)),\n  # calculate moe for proportions\n  moe_adult_insurance_prop = tidycensus::moe_prop(\n    num = estimate_adult_insurance_numerator,\n    denom = estimate_adult_denom,\n    moe_num = moe_adult_insurance_numerator,\n    moe_denom = moe_adult_denom), \n  moe_under18_insurance_prop = tidycensus::moe_prop(\n    num = estimate_under18_insurance_numerator,\n    denom = estimate_under18_denom,\n    moe_num = moe_under18_insurance_numerator,\n    moe_denom = moe_under18_denom), \n  moe_zero_to_64_insurance_prop = tidycensus::moe_prop(\n    num = estimate_zero_to_64_insurance_numerator,\n    denom = estimate_zero_to_64_insurance_denom,\n    moe_num = moe_zero_to_64_insurance_numerator,\n    moe_denom = tidycensus::moe_sum(\n      moe = c(moe_under18_denom, moe_adult_denom),\n      estimate = c(estimate_under18_denom, estimate_adult_denom))\n  ))\n\n# use full state names\nhealth_insurance$state %<>% recode(\n  CA = 'California',\n  FL = 'Florida',\n  MA = 'Massachusetts',\n  TX = 'Texas')\n\nhealth_insurance$state %<>% factor(levels = sort(c('California', 'Florida', 'Massachusetts', 'Texas')))\n\n# remove any unncessary variables going forward\nhealth_insurance %<>% select(\n  year, state, GEOID, NAME,\n  estimate_adult_denom,\n  estimate_under18_denom,\n  estimate_zero_to_64_insurance_denom,\n  estimate_adult_insurance_numerator,\n  estimate_under18_insurance_numerator,\n  estimate_zero_to_64_insurance_numerator,\n  estimate_adult_insurance_prop,\n  estimate_under18_insurance_prop,\n  estimate_zero_to_64_insurance_prop,\n  moe_adult_insurance_prop,\n  moe_under18_insurance_prop,\n  moe_zero_to_64_insurance_prop,\n  poverty_prop,\n  ICEraceinc\n)\n\n# table the top few lines of our dataframe so we can check that everything is\nknitr::kable(head(health_insurance))"},{"path":"temporal-health-insurance.html","id":"visualizing-your-data-2","chapter":"10 Case Study 4: Case Study on Temporal Trends using American Community Survey (ACS) data (2012-2019)","heading":"10.4 Visualizing your Data","text":"Now visualizing estimates:Now, might want map quantities interest!Since didn’t download geometry queries Census API \ntidycensus, can now download county geometry tigris merge \n.didn’t want download geometry data calls tidycensus two\nreasons:tidycensus package returns tidy formatted data (one variable\nobservation per row), geometry data repeated many times taking\nspace necessary; andBecause tidycensus package returns tidy formatted data (one variable\nobservation per row), geometry data repeated many times taking\nspace necessary; andBecause ’s easier aggregation data manipulation without \nspatial/geometry data add afterwards functions \ndplyr tidyverse may work well spatial data frames.’s easier aggregation data manipulation without \nspatial/geometry data add afterwards functions \ndplyr tidyverse may work well spatial data frames.look maps, make sure note scales presented legend sometimes \ndiffer (.e. pay attention range legend represents)!Notice, Texas one county seems experience decrease insurance coverage time. seems strange! think reason ? Let’s investigate .One way identify county Texas appears decreasing using R package plotly. function ggplotly, produces map allow hover cursor county see name county percent health insurance. can identify percent health insurance McCulloch County appears decreasing time. Try !information ACS calculates margin error, see https://www.census.gov/data/academy/webinars/2020/calculating-margins--error-acs.htmlThe margin error measure uncertainty estimate. Often width margin error sample size, whereby small sample sizes yield estimates large margins error. think important communicate uncertainty data public?Congrats! can now visualize estimates uncertainty plots maps.","code":"\nlibrary(colorblindr)\n# create plot of county level % with health insurance by year and smooth with geom_smooth method = 'loess' by year \nggplot(health_insurance, aes(x = year, y = estimate_zero_to_64_insurance_prop, color = state)) + \n  geom_jitter(alpha = 0.6, height = 0) + \n  geom_smooth(method = 'loess', color = 'dimgrey') + \n  facet_wrap(~state) + \n  scale_fill_manual(\n    values = setNames(palette_OkabeIto[c(1,2,3,6)],\n                      c('Texas', 'Massachusetts', 'Florida', 'California'))\n                      ) + \n  scale_color_manual(\n    values = setNames(palette_OkabeIto[c(1,2,3,6)],\n                      c('Texas', 'Massachusetts', 'Florida', 'California'))\n                      ) + \n  xlab(\"Year\") + \n  ylab(\"Percent with Health Insurance (age 0-64)\") + \n  scale_y_continuous(labels = scales::percent_format()) + \n  ggtitle(\"% with Health Insurance by County and State\", \n          \"Each dot represents a county observation from ACS\\nSmooth fit shows average county observation\")\n\n# here's a version using ggdist to plot quantile ribbons on top of the jittered points \nlibrary(ggdist)\nggplot(health_insurance,\n       aes(\n         x = year,\n         y = estimate_zero_to_64_insurance_prop,\n         fill = factor(state),\n         color = factor(state)\n       )) +\n  geom_jitter(alpha = 0.3, height = 0) + \n  stat_lineribbon(aes(fill_ramp = stat(level)), color = 'black', alpha = 0.6) + \n  facet_wrap(~state) + \n  scale_fill_manual(\n    values = setNames(palette_OkabeIto[c(1,2,3,6)],\n                      c('Texas', 'Massachusetts', 'Florida', 'California'))\n                      ) + \n  scale_color_manual(\n    values = setNames(palette_OkabeIto[c(1,2,3,6)],\n                      c('Texas', 'Massachusetts', 'Florida', 'California'))\n                      ) + \n  xlab(\"Year\") + \n  ylab(\"Percent with Health Insurance (ages 0-64)\") + \n  ggtitle(\"% with Health Insurance by County and State, Ages 0-64\", \n          \"Each dot represents a county observation from ACS\\nSmoothed line represents median county estimate\") +\n  labs(\n    fill_ramp = 'Quantile Range',\n    color = 'State',\n    fill = 'State',\n    ) + \n  theme_bw() + \n  guides(fill = guide_legend(reverse = TRUE)) + \n  guides(color = guide_legend(reverse = TRUE)) + \n  scale_y_continuous(labels = scales::percent_format())\ncounties_sf <- \n  tigris::counties(\n    state = c('CA', 'FL', 'MA', 'TX'),\n    cb = TRUE,\n    resolution = '20m') # here we're downloading 1:20m resolution data \n\n# make sure to do an inner join since we have more than 1 row per county\n# for more information on different types of joins: https://statisticsglobe.com/r-dplyr-join-inner-left-right-full-semi-anti\nhealth_insurance_sf <- inner_join(\n  counties_sf %>% select(GEOID, STATE_NAME), # when joining spatial data in, put the spatial data argument first so\n  # the output inherits the spatial data class \n  health_insurance,\n  by = c('GEOID' = 'GEOID'))\n\n# Function to make maps for each of California, Florida, Massachusetts, Texas\n# counties over time\nmap_health_insurance_over_time <- function(state_name, outcome_var,\n                                           fill_label = 'Percent with Health Insurance') {\n  health_insurance_sf %>% \n    filter(STATE_NAME == state_name) %>% \n    ggplot(aes(fill = {{ outcome_var }}, label = NAME)) + \n    geom_sf(color = 'white', size = .15) + \n    facet_wrap(~year, nrow = 2) + \n    scale_fill_distiller(palette = 'Blues', direction = 1, labels = scales::percent_format()) + \n    theme_bw() + \n    labs(fill = fill_label) + \n    theme(legend.position = 'bottom',\n          axis.text.x = element_blank(),\n          axis.text.y = element_blank(),\n          axis.ticks.x = element_blank(),\n          axis.ticks.y = element_blank(),\n          legend.key.width = unit(1.5, 'cm')\n          ) + \n    ggtitle(\"Percent with Health Insurance by County over Time\", state_name) \n}\n\n# create choropleth maps over time for each measure\nmap_health_insurance_over_time('California', estimate_zero_to_64_insurance_prop)\nggsave(\"images/10-temporal-health-insurance/california.png\", width = 8, height = 5)\nmap_health_insurance_over_time('Florida', estimate_zero_to_64_insurance_prop)\nggsave(\"images/10-temporal-health-insurance/florida.png\", width = 8, height = 5)\nmap_health_insurance_over_time('Massachusetts', estimate_zero_to_64_insurance_prop)\nggsave(\"images/10-temporal-health-insurance/massachusetts.png\", width = 8, height = 5)\nmap_health_insurance_over_time('Texas', estimate_zero_to_64_insurance_prop)\nggsave(\"images/10-temporal-health-insurance/texas.png\", width = 8, height = 5)\nlibrary(plotly)\nggplotly(map_health_insurance_over_time('Texas', estimate_zero_to_64_insurance_prop))\n\n#Now we can take a closer look at McCulloch County, TX and see that it has a large margin of error!\nhealth_insurance %>% \n  filter(NAME == 'McCulloch County, Texas') %>% \n  ggplot(aes(x = year, y = estimate_zero_to_64_insurance_prop,\n             ymax = pmin(moe_zero_to_64_insurance_prop/2 + estimate_zero_to_64_insurance_prop, 1),\n             ymin = pmax(estimate_zero_to_64_insurance_prop - moe_zero_to_64_insurance_prop/2), 0)) + \n  geom_ribbon(fill = '#3182BD', alpha = .8, size = 0) + \n  geom_line() + \n  geom_point() + \n  scale_y_continuous(labels = scales::percent_format()) + \n  ylab(\"Percent with Health Insurance\") +\n  ggtitle(\"% with Health Insurance, McCulloch County, Texas\",\n          \"90% Margin of Error and Estimates Shown\") \n\n# One reason that a county would have a large margin of error is if it has a small population size. Let's see what the population size is of McCulloch County, TX.\n#Here we see that the adult population of McCulloch county went from 4,649 in 2012 to 4,364 in 2019.\n\nMcCulloch<-health_insurance %>%\n  filter(NAME==\"McCulloch County, Texas\")\n\nMcCulloch$estimate_adult_denom\n\n#Here we can see that the range of the adult population across all counties in the four states we are examining is 48 - 6,496,638. That means that that McCulloch county, with a population of around 4,500 falls in the bottom quantile, which likely explains the large margin of error.\n\nsummary(health_insurance$estimate_adult_denom)\n# visualize the margin of error data.\nmap_health_insurance_over_time('Texas', outcome_var = moe_zero_to_64_insurance_prop,\n                               fill_label = 'Margin of Error') +\n  scale_fill_distiller(\n    palette = 'Greys',\n    direction = 1,\n    trans = 'log10',\n    labels = scales::percent_format()\n  )\n\nggsave(\"images/10-temporal-health-insurance/texasmoe.png\", width = 8, height = 5)\n\nmap_health_insurance_over_time('Massachusetts',\n                               outcome_var = moe_zero_to_64_insurance_prop,\n                               fill_label = 'Margin of Error') +\n  scale_fill_distiller(\n    palette = 'Greys',\n    direction = 1,\n    trans = 'log10',\n    labels = scales::percent_format()\n  )\n\nggsave(\"images/10-temporal-health-insurance/massmoe.png\", width = 8, height = 5)\n\nmap_health_insurance_over_time('Florida', outcome_var = moe_zero_to_64_insurance_prop,\n                               fill_label = 'Margin of Error') +\n  scale_fill_distiller(\n    palette = 'Greys',\n    direction = 1,\n    trans = 'log10',\n    labels = scales::percent_format()\n  )\n\nggsave(\"images/10-temporal-health-insurance/floridamoe.png\", width = 8, height = 5)"},{"path":"temporal-health-insurance.html","id":"adding-area-based-social-metrics","chapter":"10 Case Study 4: Case Study on Temporal Trends using American Community Survey (ACS) data (2012-2019)","heading":"10.5 Adding Area Based Social Metrics","text":"Now let’s look ABSMs. going assess association poverty health insurance racialized economic segregation (measured ICE) health insurance.First, assess association poverty health insurance plotting scatter plots county percent poverty county percent health insurance. add smoothed line scatter using linear regression. plot 1, use unweighted data, plot 2, use inverse margin error weight data. Can see difference inverse margin error weights included?Next, plot trend heath insurance status quintiles poverty ICE using loess smoothing.\nFigure 10.1: County observations adult (age 19-64) health insurance rates California\nNotice difference weighted vs unweighted estimates.Now explore relationships ICE health insurance, weighted inverse margin error well poverty health insurance, weighted inverse margin error.Now, going use linear regression inverse margin error weighting assess trends insurance coverage county. categorize county increase, stable/uncertain, decreasing. visualize results maps histograms.\nFigure 10.2: County observations adult (age 19-64) health insurance rates California\nadditional analyses might interested ?","code":"\n# look at relationship between poverty and insurance proportion\nplt1 <- \nhealth_insurance %>% \n  filter(state == 'Texas') %>% \n  ggplot(aes(x = poverty_prop, y = estimate_zero_to_64_insurance_prop)) + \n  geom_point() + \n  geom_smooth(method = 'lm') + \n  facet_wrap(~year, nrow = 2) + \n  xlab(\"Percent in Poverty\") + \n  ylab(\"Percent with Health Insurance\") + \n  scale_x_continuous(labels = scales::percent_format()) + \n  scale_y_continuous(labels = scales::percent_format()) + \n  ggtitle(\"Relationship Between Percent in Poverty and Percent with Health Insurance\",\n          \"County Observations, Texas -- Unweighted\") \n\n# the same as above, but with inverse margin of error weighting\nplt2 <- \nhealth_insurance %>% \n  filter(state == 'Texas') %>% \n  ggplot(aes(x = poverty_prop, y = estimate_zero_to_64_insurance_prop, weight = 1/moe_zero_to_64_insurance_prop)) + \n  geom_point() + \n  geom_smooth(method = 'lm') + \n  facet_wrap(~year, nrow = 2) + \n  xlab(\"Percent in Poverty\") + \n  ylab(\"Percent with Health Insurance\") + \n  scale_x_continuous(labels = scales::percent_format()) + \n  scale_y_continuous(labels = scales::percent_format()) + \n  ggtitle(\"Relationship Between Percent in Poverty and Percent with Health Insurance\",\n          \"County Observations, Texas -- Weighted by Inverse Margin of Error\") \n\n# display plot 1 and 2 over each other\nplt1 / plt2 \n\n# massachusetts poverty relationship unweighted\nplt1 <- \nhealth_insurance %>% \n  filter(state == 'Massachusetts') %>% \n  ggplot(aes(x = poverty_prop, y = estimate_zero_to_64_insurance_prop)) + \n  geom_point() + \n  geom_smooth(method = 'lm') + \n  facet_wrap(~year, nrow = 2) + \n  xlab(\"Percent in Poverty\") + \n  ylab(\"Percent with Health Insurance\") + \n  scale_x_continuous(labels = scales::percent_format()) + \n  scale_y_continuous(labels = scales::percent_format()) + \n  labs(title=str_wrap(\"Relationship Between Percent in Poverty and Percent with Health Insurance\",width =33, indent=0, exdent=0),\n       subtitle=str_wrap(\"County Observations, Massachusetts -- Unweighted\",width =42, indent=0, exdent=0)) \n\n# massachusetts poverty relationship weighted by inverse margin of error\nplt2 <- \nhealth_insurance %>% \n  filter(state == 'Massachusetts') %>% \n  ggplot(aes(x = poverty_prop, y = estimate_zero_to_64_insurance_prop, weight = 1/moe_zero_to_64_insurance_prop)) + \n  geom_point() + \n  geom_smooth(method = 'lm') + \n  facet_wrap(~year, nrow = 2) + \n  xlab(\"Percent in Poverty\") + \n  ylab(\"Percent with Health Insurance\") + \n  scale_x_continuous(labels = scales::percent_format()) + \n  scale_y_continuous(labels = scales::percent_format()) + \n  labs(title=str_wrap(\"Relationship Between Percent in Poverty and Percent with Health Insurance\",width =33, indent=0, exdent=0),\n       subtitle=str_wrap(\"County Observations, Massachusetts -- Weighted by Inverse Margin of Error\",width =42, indent=0, exdent=0)) \n\n# display plot 1 and 2 side by side\nplt1 + plt2 \n# look at the relationship with ICEraceinc and insurance, weighted by inverse margin of error\nhealth_insurance %>% \n  filter(state == 'Texas') %>% \n  ggplot(aes(x = ICEraceinc, y = estimate_zero_to_64_insurance_prop, weight = 1/moe_zero_to_64_insurance_prop)) + \n  geom_point() + \n  geom_smooth(method = 'lm') + \n  facet_wrap(~year, nrow = 2) + \n  xlab(\"ICEraceinc\") + \n  ylab(\"Percent with Health Insurance\") + \n  scale_y_continuous(labels = scales::percent_format()) + \n  ggtitle(\"Relationship Between ICE for Racialized Economic Segregation and \\nPercent with Health Insurance\",\n          \"County Observations, Texas -- Weighted by Inverse Margin of Error\") ## `geom_smooth()` using formula 'y ~ x'\n# smoothed insurance relationship with ICEraceinc quantiles over time \n# using loess smoothing\nhealth_insurance %>% \n  filter(state == 'Texas') %>% \n  ggplot(aes(x = year, y = estimate_zero_to_64_insurance_prop, \n             color = cut(ICEraceinc, quantile(ICEraceinc, seq(0, 1, .2)), include.lowest = T),\n             fill = cut(ICEraceinc, quantile(ICEraceinc, seq(0, 1, .2)), include.lowest = T),\n             weight = 1/moe_zero_to_64_insurance_prop)) + \n  # geom_jitter(height = 0) + \n  geom_smooth(alpha = 0.5, alpha = .5) +\n  scale_color_brewer(palette = 'RdYlBu') + \n  scale_fill_brewer(palette = 'RdYlBu') + \n  xlab(\"Year\") +\n  ylab(\"Percent with Health Insurance\") + \n  guides(fill = guide_legend(nrow = 2), color = guide_legend(nrow = 2)) + \n  labs(fill = \"ICEraceinc Quantile\", color = \"ICEraceinc Quantile\") + \n  theme(legend.position = 'bottom') + \n  scale_y_continuous(labels = scales::percent_format()) + \n  ggtitle(\"Relationship Between Percent with Health Insurance by ICE for Racialized \\nEconomic Segregation over Time\",\n          \"County Observations, Texas -- Weighted by Inverse Margin of Error\") ## Warning: Duplicated aesthetics after name standardisation: alpha## `geom_smooth()` using method = 'loess' and formula 'y ~ x'\nggsave(\"images/10-temporal-health-insurance/healthinsurance_ICE_time.png\", width = 8, height = 5)## `geom_smooth()` using method = 'loess' and formula 'y ~ x'\n# smoothed relationship between poverty (cut in quantiles) and insurance rates\n# using loess smoothing\nhealth_insurance %>% \n  filter(state == 'Texas', ! is.na(poverty_prop)) %>% \n  ggplot(aes(x = year, y = estimate_zero_to_64_insurance_prop, \n             color = cut(poverty_prop, quantile(poverty_prop, seq(0, 1, .2),na.rm=T),\n                         include.lowest = TRUE),\n             fill = cut(poverty_prop, quantile(poverty_prop, seq(0, 1, .2),na.rm=T),\n                         include.lowest = TRUE),\n             weight = 1/moe_zero_to_64_insurance_prop\n             )) + \n  geom_smooth() + \n  scale_color_brewer(palette = 'RdYlBu', direction = -1) + \n  scale_fill_brewer(palette = 'RdYlBu', direction = -1) + \n  xlab(\"Year\") +\n  ylab(\"Percent with Health Insurance\") + \n  scale_y_continuous(labels = scales::percent_format()) + \n  guides(fill = guide_legend(nrow = 2), color = guide_legend(nrow = 2)) + \n  labs(fill = \"Poverty Quantile\", color = \"Poverty Quantile\") + \n  theme(legend.position = 'bottom') + \n  ggtitle(\"Relationship Between Percent with Health Insurance by Percent in Poverty \\nover Time\",\n          \"County Observations, Texas -- Weighted by Inverse Margin of Error\")## `geom_smooth()` using method = 'loess' and formula 'y ~ x'\nggsave(\"images/10-temporal-health-insurance/healthinsurance_poverty_time.png\", width = 8, height = 5)## `geom_smooth()` using method = 'loess' and formula 'y ~ x'\n# cleaning before modeling\nhealth_insurance %<>% rename(county = NAME)\nhealth_insurance %<>% mutate(year_centered = year - mean(c(2012, 2019)))\n\n# create linear models with inverse margin of error weighting for each county\n# regressing insurance rates on year\nmodel_df <- \n  health_insurance %>% \n  nest_by(GEOID, county, state) %>% \n  mutate(\n    # for each county, fit a linear model and store it in a list column\n    model = list(\n      lm(\n        estimate_zero_to_64_insurance_prop ~ year_centered,\n        data = data,\n        weights = 1 / data$moe_zero_to_64_insurance_prop\n      )\n    ), \n    # use broom::tidy to extract coefficients\n    coefs = list(broom::tidy(model, conf.int=T)),\n  ) %>% \n  rowwise() %>% \n  # extract coefficients on year for each model\n  mutate(\n    coef_on_year = coefs %>% filter(term == 'year_centered') %>% pull(estimate),\n    coef_on_year_ci_low = coefs %>% filter(term == 'year_centered') %>% pull(conf.low),\n    coef_on_year_ci_high = coefs %>% filter(term == 'year_centered') %>% pull(conf.high)\n  )\n\n# plot coefficients distribution\nmodel_df %>% \n  ggplot(aes(x = coef_on_year)) + \n  geom_vline(xintercept = 0, alpha = .5, linetype = 'dashed') + \n  geom_histogram() + \n  scale_x_continuous(labels = scales::percent_format()) + \n  xlab(\"Coefficient Estimates for Average Percentage Point Increase Each Year\") + \n  facet_wrap(~state) + \n  ggtitle(\"Estimates for County Change in Percent with Health Insurance\")\n\nggsave(\"images/10-temporal-health-insurance/estimate_countychange.png\", width = 8, height = 4)\n\n# classify into increasing, decreasing, or uncertain/stable\nmodel_df %<>% mutate(\n  change_over_time = case_when(\n    coef_on_year_ci_low > 0 ~ 'increasing',\n    coef_on_year_ci_high < 0 ~ 'decreasing',\n    TRUE ~ 'stable/uncertain'\n  )\n)\n\n# make factor levels for change over time\nmodel_df %<>% mutate(change_over_time = factor(\n  change_over_time,\n  levels = c('decreasing', 'stable/uncertain', 'increasing')\n))\n\n# plot distribution of change over time categories\nmodel_df %>% \n  ggplot(aes(x = change_over_time)) + \n  geom_bar() + \n  facet_wrap(~state) + \n  theme(axis.text.x = element_text(angle = 75, hjust = 1))\n\n# merge in spatial data and coefficients\nhealth_insurance_sf_w_model_coefs <- \n  health_insurance %>% \n  # nest by GEOID, county, and state so we can make interactive popup graphs by county \n  nest_by(GEOID, county, state) %>% \n  # join in the change over time variables\n  left_join(model_df %>% ungroup() %>% select(GEOID, coef_on_year, change_over_time), by = c('GEOID' = 'GEOID')) %$% \n  # join in the spatial data; \n  # we use %$% for this because when joining spatial data and tabular data, it is critical \n  # that the spatial data be passed as the first argument to the join function\n  left_join(health_insurance_sf %>% filter(year == min(year)) %>% select(GEOID),\n            ., by = c('GEOID'))\n\n# plot texas change over time \nhealth_insurance_sf_w_model_coefs %>% \n  filter(state == 'Texas') %>% \n  ggplot(aes(fill = change_over_time)) + \n  geom_sf(color = 'white', size = .05, alpha = .8) + \n  scale_fill_manual(values = c('increasing' = '#67A9CF', \n                                'stable/uncertain' = '#FFFFBF', \n                                'decreasing' = '#EF8A62'\n                               )) + \n  theme_bw() + \n  labs(fill = 'Change over Time') + \n  ggtitle(\"Change in Health Insurance Rates, 2012-2019, Texas\")\n\nhealth_insurance_sf_w_model_coefs %>% \n  filter(state == 'Texas') %>% \n  ggplot(aes(fill = coef_on_year)) + \n  geom_sf(color = 'white', size = .1) + \n  scale_fill_gradient2(high = '#0072B2',\n                       mid = '#FFFFBF',\n                       low = '#D55E00') + \n  theme_bw() + \n  labs(fill = 'Change over Time') + \n  ggtitle(\"Change in Health Insurance Rates, 2012-2019, Texas\")\n\n\nlibrary(leaflet)\nlibrary(leafpop)\n\npal <- colorFactor(palette = RColorBrewer::brewer.pal(name='RdBu', n = 3), \n                   domain = health_insurance_sf_w_model_coefs$change_over_time)\n\ntx_health_insurance_sf_w_model_coefs <- \n  health_insurance_sf_w_model_coefs %>% filter(state == 'Texas')\n\ntx_health_insurance_sf_w_model_coefs %<>% \n  rowwise() %>% mutate(\n  plot = list(ggplot(\n    data = data,\n    mapping = aes(\n      x = year, \n      y = estimate_zero_to_64_insurance_prop, \n      ymin = pmax(estimate_zero_to_64_insurance_prop - (1.96/1.645)*moe_zero_to_64_insurance_prop, 0),\n      ymax = pmin(estimate_zero_to_64_insurance_prop + (1.96/1.645)*moe_zero_to_64_insurance_prop, 1)\n      )) + \n      geom_pointrange() + \n      geom_line() + \n      ylab(\"Percent with Health Insurance\\nAge 0-64\") +\n      ggtitle(county) + \n      scale_y_continuous(labels = scales::percent_format())\n  )\n)\n\nmap <- \n  tx_health_insurance_sf_w_model_coefs %>% \n  leaflet() %>% \n  addTiles() %>% \n  addPolygons(color = ~pal(change_over_time), \n              label = ~county,\n              popup = popupGraph(tx_health_insurance_sf_w_model_coefs$plot, type = 'png', width = 400, height = 400),\n              weight = 1) %>% \n  addLegend(\n    position = 'topright',\n    pal = pal,\n    values = c('increasing', 'stable/uncertain', 'decreasing'),\n    labels = c('Increasing', 'Stable/Uncertain', 'Decreasing')\n  )\n\n# additional ways to map change in insurance coverage over time\nlibrary(htmlwidgets)\nsaveWidget(map, \"map.html\")\n\nhealth_insurance_sf %>% \n  filter(year == min(year)) %>% \n  left_join(model_df %>% ungroup() %>% select(GEOID, change_over_time), by = c('GEOID' = 'GEOID')) %>% \n  filter(state == 'Texas') %>% \n  ggplot(aes(fill = change_over_time)) + \n  geom_sf(color = 'white', size = .05) + \n  scale_fill_manual(values = c('increasing' = '#67A9CF', \n                                'uncertain/stable' = '#FFFFBF', \n                                'decreasing' = '#EF8A62'\n                               )) \n  \nhealth_insurance_sf %>% \n  filter(year == min(year)) %>% \n  left_join(model_df %>% ungroup() %>% select(GEOID, change_over_time), by = c('GEOID' = 'GEOID')) %>% \n  filter(state == 'Florida') %>% \n  ggplot(aes(fill = change_over_time)) + \n  geom_sf(color = 'white', size = .05) + \n  scale_fill_manual(values = c('increasing' = '#67A9CF', \n                                'uncertain/stable' = '#FFFFBF', \n                                'decreasing' = '#EF8A62'\n                               )) \n\nhealth_insurance_sf %>% \n  filter(year == min(year)) %>% \n  left_join(model_df %>% ungroup() %>% select(GEOID, change_over_time), by = c('GEOID' = 'GEOID')) %>% \n  filter(state == 'California') %>% \n  ggplot(aes(fill = change_over_time)) + \n  geom_sf(color = 'white', size = .05) + \n  scale_fill_manual(values = c('increasing' = '#67A9CF', \n                                'uncertain/stable' = '#FFFFBF', \n                                'decreasing' = '#EF8A62'\n                               ))"},{"path":"health-insurance-comparison.html","id":"health-insurance-comparison","chapter":"11 Case Study 5: Case Study Comparing County Analyses of Inequities in Health Insurance using ACS vs. CDC PLACES data (2019)","heading":"11 Case Study 5: Case Study Comparing County Analyses of Inequities in Health Insurance using ACS vs. CDC PLACES data (2019)","text":": Christian Testa, Dena Javadi","code":""},{"path":"health-insurance-comparison.html","id":"introduction-3","chapter":"11 Case Study 5: Case Study Comparing County Analyses of Inequities in Health Insurance using ACS vs. CDC PLACES data (2019)","heading":"11.1 Introduction","text":"CDC PLACES dataset American Community Survey (ACS)\nrepresent two distinct approaches collecting reporting small area\nlevel data: CDC PLACES uses method called small area estimation (SAE)\nproduce estimates Census tract level, ACS provides survey\nbased estimates directly Census tract level.methodology page CDC PLACES provides\nexplanation exactly implemented SAE.article Multilevel Regression Postratification Small-Area Estimation Population Health Outcomes: Case Study Chronic Obstructive Pulmonary Disease Prevalence Using Behavioral Risk Factor Surveillance System, Zhang et al. describe SAE performed estimate\nchronic obstructive pulmonary disease (COPD) CDC PLACES.","code":""},{"path":"health-insurance-comparison.html","id":"motivation-research-questions-and-learning-objectives-4","chapter":"11 Case Study 5: Case Study Comparing County Analyses of Inequities in Health Insurance using ACS vs. CDC PLACES data (2019)","heading":"11.2 Motivation, Research Questions, and Learning Objectives","text":"goal case study gain familiarity ACS CDC’s\nPLACES, understand SAEs created, visualize comparisons SAEs.\noutcome interest case study health insurance.specific learning objectives :Download specific ACS CDC’s PLACES dataVisualize (plot) map estimates Characterize trends estimates area\nbased social measures (ABSMs)Understand limitations SAEs Explore\nvalidity model-based estimatesUnderstand effect smoothing estimates small areasGiven CDC PLACES ACS provide estimates Census tract prevalence health insurance, interested assessing difference two datasets difference may vary according ABSM.","code":""},{"path":"health-insurance-comparison.html","id":"understanding-small-area-estimates","chapter":"11 Case Study 5: Case Study Comparing County Analyses of Inequities in Health Insurance using ACS vs. CDC PLACES data (2019)","heading":"11.3 Understanding Small Area Estimates","text":"Small Area Estimation?Goal: provide finer geographic detail published statistics \nvarious subpopulations.Problem: Surveys generally provide large enough samples \nreliable direct estimates small areas (counties, especially\ncensus tracts).Challenges: model misspecification failure account \ninformative sampling.valid reliable PLACES county level estimates?Depends quality underlying dataDepends assumptions appropriateness statistical models\nusedsmaller uncertainty ranges model-based estimates compared \ndirect estimatessmoothing local geographic variation results \npotentially underestimating small areas high prevalence\nestimates overestimating small areas lower prevalence\nestimateshigher discrepancies behavior indicators diagnosed\nchronic diseases potentially due different biases \nself-report recall biasModel based estimates also sensitive local area interventions \nshocks, meaning prevalence may underestimated based prior\ntrends without accounting local surveys may capture - \ntargeted local intervention disaster event. example can seen\nZhang et al’s study model-based estimates current smoking prevalence\ntwo adjacent counties Missouri 24.2% 25.0% direct survey\nestimates local survey 25.3% 13.5% due latter county’s\nlocal tobacco control initiative.Multilevel Regression Post-stratification (MRP)?Multilevel statistical modeling framework linking geocoded health\nsurveys high spatial resolution population demographic \nsocioeconomic dataTo conduct MRP, need:Surveys outcome interest, demographic data, geographic\nindicatorThe geographic demographic data used predictors \nfirst stage model define geographic demographic levels\nsecond stageContinuous variables need split intervals create\nlevelsFactors levels must match poststratification table\n(individual level demographic geographic variables must match\npopulation level variables)postratification table?Representation combination demographic geographic\nfactors correspond number individuals population\ninterestThe number rows poststratification table correspond \nproduct number options variables\nincluded\n.e. 6 age groups * 3 gender categories * 8 racialized\ncategories * 50 states = 7200 rows\n.e. 6 age groups * 3 gender categories * 8 racialized\ncategories * 50 states = 7200 rowsFirst stage\nMultilevel logistic regression model\nMultilevel logistic regression modelSecond stage\nWeight model predictions subgroup population\nestimated frequency subgroups\nConsiderations: missing variables post-stratification table,\nnonresponse, missing data, inclusion interaction terms\n(useful studying demographic subgroups within geographic\nlevels) use structured priors share information across\nlevels factor, violation positivity assumption\nWeight model predictions subgroup population\nestimated frequency subgroupsConsiderations: missing variables post-stratification table,\nnonresponse, missing data, inclusion interaction terms\n(useful studying demographic subgroups within geographic\nlevels) use structured priors share information across\nlevels factor, violation positivity assumption","code":""},{"path":"health-insurance-comparison.html","id":"create-our-cleaned-dataset","chapter":"11 Case Study 5: Case Study Comparing County Analyses of Inequities in Health Insurance using ACS vs. CDC PLACES data (2019)","heading":"11.4 Create our Cleaned Dataset","text":"","code":"\n# dependencies \nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(tidycensus)\nlibrary(patchwork)\n\n# get our data from PLACES \n# download the county level data from https://chronicdata.cdc.gov/500-Cities-Places/PLACES-Local-Data-for-Better-Health-County-Data-20/swc5-untb \n# rename it to \"places_2021_release_county_level.csv\"\nplaces <- readr::read_csv(\"data/11-health-insurance-comparison/places_2021_release_county_level.csv\")\n\n\n# filter for our measure of interest\nplaces %<>% filter(Measure == \"Current lack of health insurance among adults aged 18-64 years\") \n\n# download ACS data\n# remember you can check the variable table here: \n# https://api.census.gov/data/2019/acs/acs5/variables.html  \nacs <- tidycensus::get_acs(\n  geography = 'county',\n  variables = c(\n    'total_population' = 'B18135_013E',\n    'with_disability_no_insurance' = 'B18135_018E',\n    'no_disability_no_insurance' = 'B18135_023E'\n  ), \n  year = 2019,\n  state = c(state.abb, 'DC'),\n  geometry = TRUE,\n  output = 'tidy',\n  survey = 'acs5')\n\n# recode the variable names\nacs_cleaned <- acs %>% \n  as_tibble() %>%  # as_tibble drops the geometry, which causes an error later with data manipulation\n  select(GEOID, variable, estimate, moe) %>% \n  mutate(\n    variable = recode(variable,\n                      \"B18135_013\" = \"total 19-64 population\",\n                      \"B18135_018\" = \"uninsured 19-64 population, disabled\",\n                      \"B18135_023\" = \"uninsured 19-64 population, not disabled\"))\n\n# pivot wider so we can do arithmetic with the columns\nacs_cleaned %<>% \n  pivot_wider(  # pivot into a wide format so we can do arithmetic with the columns\n    id_cols = GEOID,\n    names_from = variable,\n    values_from = c(estimate, moe)) \n\n# calculate rates of uninsurance and corresponding margin of error\nacs_cleaned %<>% \n  janitor::clean_names() %>% \n  rowwise() %>% # rowwise is necessary for the tidycensus::moe_* functions to work correctly\n  mutate(\n    # calculate uninsured numerator \n    uninsured_numerator = (\n      estimate_uninsured_19_64_population_disabled + estimate_uninsured_19_64_population_not_disabled\n    ),\n    uninsured_numerator_moe = tidycensus::moe_sum(\n      moe = c(\n        moe_uninsured_19_64_population_disabled,\n        moe_uninsured_19_64_population_not_disabled\n      ),\n      estimate = c(\n        estimate_uninsured_19_64_population_disabled,\n        estimate_uninsured_19_64_population_not_disabled\n      )\n    ),\n    # calculate proportion uninsured\n    pct_uninsured_19_64_acs = uninsured_numerator / estimate_total_19_64_population * 100,\n    # calculate moe for percent uninsured\n    pct_uninsured_19_64_acs_moe =\n      tidycensus::moe_prop(\n        num = uninsured_numerator,\n        denom = estimate_total_19_64_population,\n        moe_num = uninsured_numerator_moe,\n        moe_denom = moe_total_19_64_population\n      ) * 100\n  ) %>%\n  select(geoid, pct_uninsured_19_64_acs, pct_uninsured_19_64_acs_moe, estimate_total_19_64_population) # select only the columns we need\n\n# select for the crude prevalence\nplaces %<>% filter(Data_Value_Type == 'Crude prevalence')\n\n# join the places and acs datasets\nuninsurance <- places %>% full_join(acs_cleaned, by = c(\"LocationID\" = \"geoid\"))\n\n# rename to geoid for consistency\nuninsurance %<>% rename(geoid = LocationID)\n\n# extract the counties shapefile from what we downloaded from acs\ncounty_sf <-\n  acs %>%\n  # it doesn't matter which variable is used here, we just have to make sure\n  # we only get one measure per census tract\n  filter(variable == variable[[1]]) %>%\n  # select just the GEOID so we get the GEOID and geometry\n  select(GEOID)\n  \n# join in the counties shapefile data\nuninsurance <- \n  county_sf %>% \n  left_join(\n    uninsurance,\n    by = c(\"GEOID\" = 'geoid'))\n\n# clean the column names\nuninsurance %<>% select(\n  GEOID,\n  Year,\n  StateAbbr,\n  StateDesc,\n  LocationName,\n  Data_Value,\n  Low_Confidence_Limit,\n  High_Confidence_Limit,\n  pct_uninsured_19_64_acs,\n  pct_uninsured_19_64_acs_moe,\n  estimate_total_19_64_population\n) %>%\n  janitor::clean_names() %>%\n  ungroup() %>%\n  rename(\n    state_name = state_desc,\n    county_name = location_name,\n    pct_uninsured_18_64_places = data_value,\n    pct_uninsured_18_64_places_ci_low = low_confidence_limit,\n    pct_uninsured_18_64_places_ci_high = high_confidence_limit\n  )\n\n# calculate confidence intervals for the percent uninsured \nuninsurance %<>% mutate(\n  pct_uninsured_19_64_acs_moe_95pct = 1.96 / 1.645 * pct_uninsured_19_64_acs_moe,\n  pct_uninsured_18_64_acs_ci_low = pmax(\n    pct_uninsured_19_64_acs - pct_uninsured_19_64_acs_moe_95pct,\n    0\n  ),\n  pct_uninsured_18_64_acs_ci_high = pmin(\n    pct_uninsured_19_64_acs + pct_uninsured_19_64_acs_moe_95pct,\n    100\n  )\n)"},{"path":"health-insurance-comparison.html","id":"visualizing-the-data","chapter":"11 Case Study 5: Case Study Comparing County Analyses of Inequities in Health Insurance using ACS vs. CDC PLACES data (2019)","heading":"11.5 Visualizing the Data","text":"\nFigure 11.1: scatterplot ACS vs. PLACES estimates % without health insurance. solid line shoes line slope = 1 intercept = 0, line blue shows linear model line best fit.\nlast three figures help us see -average differences ACS PLACES\nestimates health uninsurance rates. histogram figure, can see \nPLACES estimates uninsurance often higher ACS, scatter-plot\nfigures show us relationship changes population size.","code":"\n# load our cleaned dataset\nuninsurance <- readRDS(here(\"data/11-health-insurance-comparison/county_uninsurance.rds\"))\n\n# calculate the difference in the \nuninsurance %<>% mutate(\n  difference_in_acs_minus_places = pct_uninsured_19_64_acs - pct_uninsured_18_64_places\n)\n\n# plot the distribution of difference in ACS vs. PLACES\nggplot(uninsurance, aes(x = difference_in_acs_minus_places)) + \n  geom_histogram(bins = 100) + \n  ylab(\"Count of County Observations\") + \n  xlab(\"% Uninsured in ACS - % Uninsured in PLACES\") + \n  ggtitle(\"Difference Between ACS and PLACES County Estimates for % without Health Insurance\",\n      subtitle = \"American Community Survey 2015-2019 Data; PLACES 2021 data based on 2019 BRFSS\") \nggplot(uninsurance,\n       aes(x = pct_uninsured_19_64_acs, y = pct_uninsured_18_64_places))+\n  geom_point(alpha = 0.5) + \n  geom_abline(slope = 1, intercept = 0) + \n  geom_smooth(method = 'lm') + \n  scale_x_continuous(breaks = seq(0, 60, 10), limits = c(0, NA)) + \n  scale_y_continuous(breaks = seq(0, 60, 10), limits = c(0, NA)) + \n  ylab(\"PLACES Estimate of % without Health Insurance\") + \n  xlab(\"ACS Estimate of % without Health Insurance\") + \n  ggtitle(\"Comparing ACS vs. PLACES Estimate of % without Health Insurance by County\",\n    subtitle = \"American Community Survey 2015-2019 Data; PLACES 2021 data based on 2019 BRFSS\")\nggplot(uninsurance,\n       aes(x = estimate_total_19_64_population, y = difference_in_acs_minus_places ))+\n  geom_point(alpha = 0.5) + \n  scale_x_log10(\n    breaks = c(1e4,1e5,1e7),\n    labels = c(\"1,000\", \"100,000\", \"10m\")\n    ) + \n  labs(x = \"19-64 Population Size Estimate, ACS 2015-2019\",\n       y = \"ACS % Uninsured - PLACES % Uninsured\") +\n  ggtitle(\"Comparing ACS vs. PLACES Estimate of % without Health Insurance by County\",\n    subtitle = \"American Community Survey 2015-2019 Data; PLACES 2021 data based on 2019 BRFSS\")"},{"path":"health-insurance-comparison.html","id":"adding-absms","chapter":"11 Case Study 5: Case Study Comparing County Analyses of Inequities in Health Insurance using ACS vs. CDC PLACES data (2019)","heading":"11.6 Adding ABSMs","text":"graphs analyses reveal value looking relationships PLACES ACS health insurance data different states?show relationship differs level state’s overall value health insurance coverage well distribution uninsurance?make ask place matters?","code":"\nvariables_dict <-\n  tibble::tribble(\n  # ICEraceinc\n  ~var, ~varname, ~description,\n  # total population\n  \"B01001_001\",  \"total_popsize\", \"total population estimate\", \n  \n  # racial composition \n  'B01003_001',  \"race_ethnicity_total\", \"race_ethnicity_total\",\n  \n  # ICEraceinc\n  \"B19001_001\",  'hhinc_total',   \"total population for household income estimates\",\n  \"B19001A_002\", 'hhinc_w_1',     \"white n.h. pop with household income <$10k\",\n  \"B19001A_003\", 'hhinc_w_2',     \"white n.h. pop with household income $10k-14 999k\",\n  \"B19001A_004\", 'hhinc_w_3',     \"white n.h. pop with household income $15k-19 999k\",\n  \"B19001A_005\", 'hhinc_w_4',     \"white n.h. pop with household income $20k-24 999k\",\n  \"B19001A_014\", 'hhinc_w_5',     \"white n.h. pop with household income $100 000 to $124 999\",\n  \"B19001A_015\", 'hhinc_w_6',     \"white n.h. pop with household income $125k-149 999k\",\n  \"B19001A_016\", 'hhinc_w_7',     \"white n.h. pop with household income $150k-199 999k\",\n  \"B19001A_017\", 'hhinc_w_8',     \"white n.h. pop with household income $196k+\",\n  \"B19001_002\",  'hhinc_total_1', \"total pop with household income <$10k\",\n  \"B19001_003\",  'hhinc_total_2', \"total pop with household income $10k-14 999k\",\n  \"B19001_004\",  'hhinc_total_3', \"total pop with household income $15k-19 999k\",\n  \"B19001_005\",  'hhinc_total_4', \"total pop with household income $20k-24 999k\",\n\n  # poverty\n  \"B05010_002\",  'in_poverty',    \"population with household income < poverty line\",\n  \"B05010_001\",  'total_pop_for_poverty_estimates',  \"total population for poverty estimates\",\n\n  # median income\n  \"B06011_001\",  'median_income',  \"median income estimate for total population\",\n  \n  # crowded housing\n  \"B25014_005\",  'owner_occupied_crowding1', 'owner occupied, 1 to 1.5 per room',\n  \"B25014_006\",  'owner_occupied_crowding2', 'owner occupied, 1.51 to 2 per room',\n  \"B25014_007\",  'owner_occupied_crowding3', 'owner occupied, 2.01 or more per room',\n  \"B25014_011\",  'renter_occupied_crowding1', 'owner occupied, 1 to 1.5 per room',\n  \"B25014_012\",  'renter_occupied_crowding2', 'owner occupied, 1.51 to 2 per room',\n  \"B25014_013\",  'renter_occupied_crowding3', 'owner occupied, 2.01 or more per room',\n  \"B25014_001\",  'crowding_total',            'total for crowding (occupants per room)',\n  \n  \"B01001I_001\",  'total_hispanic',           'total hispanic population estimate',\n  \"B01001B_001\",  'total_black',              'total black, hispanic or non-hispanic estimate',\n  \"B01001H_001\",  'total_white_nh',           'total white, non-hispanic population estimate'\n)\n\n# fetch the absms from the American Community Survey API \nabsms <- tidycensus::get_acs(\n  geography = 'county',\n  state = c(state.abb, 'DC'),\n  year = 2019,\n  variables = variables_dict$var)\n\n\n# pivot to a wide format for renaming, dropping the margin of error data\nabsms %<>% select(-moe) %>% pivot_wider(names_from = variable, values_from = estimate)\n\n# rename the columns using our rename_vars\n# \n# first we create a named vector, rename_vars, which has elements that are the\n# acs variables we request and convenient, human readable names.\n# \n# then we use rename_vars with the rename function from dplyr. \n# typically the rename function takes a syntax as follows: \n#   data %>% rename(oldname1 = newname1, oldname2 = newname2, ...)\n# but in our case, we already have a named vector (rename_vars) that we \n# want to use, and so to use the rename_vars named vector inside rename\n# we use the injection-operator `!!`.  you can learn more about the injection\n# operator by running ?`!!` in your R console. \nrename_vars <- setNames(variables_dict$var, variables_dict$varname)\nabsms <- absms %>% rename(!!rename_vars)\n\n# calculate ABSMs\nabsms %<>%\n  mutate(\n    # we calculate the people of color low income counts as the overall\n    # low income counts minus the white non-hispanic low income counts\n    people_of_color_low_income =\n      (hhinc_total_1 + hhinc_total_2 + hhinc_total_3 + hhinc_total_4) -\n      (hhinc_w_1 + hhinc_w_2 + hhinc_w_3 + hhinc_w_4),\n    # sum up the white non-hispanic high income counts\n    white_non_hispanic_high_income =\n      (hhinc_w_5 + hhinc_w_6 + hhinc_w_7 + hhinc_w_8),\n    # calculate the index of concentration at the extremes for racialized\n    # economic segregation (high income white non-hispanic vs. low income\n    # people of color)\n    ICEraceinc =\n      (white_non_hispanic_high_income - people_of_color_low_income) /\n      hhinc_total,\n\n    prop_in_poverty = in_poverty / total_pop_for_poverty_estimates,\n    \n    crowding = (\n      owner_occupied_crowding1 + owner_occupied_crowding2 + owner_occupied_crowding3 +\n        renter_occupied_crowding1 + renter_occupied_crowding2 + renter_occupied_crowding3\n    ) / crowding_total,\n    \n    prop_black = total_black / total_popsize,\n    prop_hispanic = total_hispanic / total_popsize,\n    prop_white_nh = total_white_nh / total_popsize\n  ) %>%\n  select(\n    GEOID,\n    ICEraceinc,\n    prop_in_poverty,\n    median_income,\n    crowding,\n    prop_black,\n    prop_hispanic,\n    prop_white_nh\n  )\n\n# join in the ABSMs data\nuninsurance %<>% left_join(\n  select(\n    absms,\n    GEOID,\n    ICEraceinc,\n    prop_in_poverty,\n    median_income,\n    crowding,\n    prop_black,\n    prop_hispanic,\n    prop_white_nh\n  ),\n  by = c('geoid' = 'GEOID')\n)\n\n# add quantile cutpoints to ICEraceinc\nICEraceinc_cutpoints <- Hmisc::wtd.quantile(uninsurance$ICEraceinc,\n                                            weights = uninsurance$estimate_total_19_64_population,\n                                            probs = seq(0,1,.2))\nuninsurance %<>% mutate(\n  ICEraceinc_cut = cut(ICEraceinc, ICEraceinc_cutpoints, include.lowest = TRUE))\n\n# reverse the factor ordering so the most advantaged group is the reference category\nuninsurance$ICEraceinc_cut %<>% forcats::fct_rev()\n\n# add cutpoints to pct in poverty\nuninsurance %<>% mutate(\n  poverty_cut = cut(\n    prop_in_poverty, \n    c(0,.05,.1,.15,.2,1),\n    include.lowest=TRUE\n  )\n)\n\n# add cutpoints to median income\nmedian_income_cutpoints <- Hmisc::wtd.quantile(uninsurance$median_income,\n                                               weights = uninsurance$total_popsize,\n                                               probs = seq(0,1,.2))\nuninsurance %<>% mutate(\n  median_income_cut = cut(\n    median_income, \n    median_income_cutpoints,\n    include.lowest=TRUE\n  )\n)\nuninsurance$median_income_cut %<>% forcats::fct_rev()\n\n# add cutpoints to crowding\ncrowding_cutpoints <- Hmisc::wtd.quantile(uninsurance$crowding,\n                                               weights = uninsurance$total_popsize,\n                                               probs = seq(0,1,.2))\nuninsurance %<>% mutate(\n  crowding_cut = cut(\n    crowding, \n    crowding_cutpoints,\n    include.lowest=TRUE\n  )\n)\n\n\n# add cutpoints to proportion black\nprop_black_cutpoints <- Hmisc::wtd.quantile(uninsurance$prop_black,\n                                               weights = uninsurance$total_popsize,\n                                               probs = seq(0,1,.2))\nuninsurance %<>% mutate(\n  prop_black_cut = cut(\n    prop_black, \n    prop_black_cutpoints,\n    include.lowest=TRUE\n  )\n)\n\n# add cutpoints to proportion hispanic\nprop_hispanic_cutpoints <- Hmisc::wtd.quantile(uninsurance$prop_hispanic,\n                                               weights = uninsurance$total_popsize,\n                                               probs = seq(0,1,.2))\nuninsurance %<>% mutate(\n  prop_hispanic_cut = cut(\n    prop_hispanic, \n    prop_hispanic_cutpoints,\n    include.lowest=TRUE\n  )\n)\n\n# add cutpoints to proportion white\nprop_white_nh_cutpoints <- Hmisc::wtd.quantile(uninsurance$prop_white_nh,\n                                               weights = uninsurance$total_popsize,\n                                               probs = seq(0,1,.2))\nuninsurance %<>% mutate(\n  prop_white_nh_cut = cut(\n    prop_white_nh, \n    prop_white_nh_cutpoints,\n    include.lowest=TRUE\n  )\n)\n# relationship between ICE and percent without insurance in PLACES\nmodel <- lm(\n  formula = pct_uninsured_18_64_places ~ ICEraceinc_cut,\n  data = uninsurance\n)\n\nbroom::tidy(model)## # A tibble: 5 × 5\n##   term                         estimate std.error statistic   p.value\n##   <chr>                           <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)                     13.3      0.371     35.9  4.82e-236\n## 2 ICEraceinc_cut(0.207,0.276]      1.38     0.463      2.98 2.88e-  3\n## 3 ICEraceinc_cut(0.144,0.207]      2.80     0.427      6.55 6.54e- 11\n## 4 ICEraceinc_cut(0.106,0.144]      5.22     0.462     11.3  5.01e- 29\n## 5 ICEraceinc_cut[-0.476,0.106]     8.07     0.422     19.1  3.79e- 77\n# relationship between ICE and percent without insurance in ACS\nmodel <- lm(\n  formula = pct_uninsured_19_64_acs ~ ICEraceinc_cut,\n  data = uninsurance\n)\n\nbroom::tidy(model)## # A tibble: 5 × 5\n##   term                         estimate std.error statistic   p.value\n##   <chr>                           <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)                      9.11     0.386     23.6  1.17e-113\n## 2 ICEraceinc_cut(0.207,0.276]      1.65     0.484      3.41 6.50e-  4\n## 3 ICEraceinc_cut(0.144,0.207]      3.09     0.445      6.94 4.76e- 12\n## 4 ICEraceinc_cut(0.106,0.144]      5.98     0.484     12.4  2.47e- 34\n## 5 ICEraceinc_cut[-0.476,0.106]     9.60     0.441     21.8  3.60e- 98\n# relationship between poverty and percent without insurance in ACS\nmodel <- lm(\n  formula = pct_uninsured_19_64_acs ~ poverty_cut,\n  data = uninsurance\n)\n\nbroom::tidy(model)## # A tibble: 5 × 5\n##   term                  estimate std.error statistic  p.value\n##   <chr>                    <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)             10.5       0.692    15.2   3.91e-50\n## 2 poverty_cut(0.05,0.1]   -0.325     0.776    -0.419 6.75e- 1\n## 3 poverty_cut(0.1,0.15]    0.731     0.747     0.978 3.28e- 1\n## 4 poverty_cut(0.15,0.2]    2.90      0.740     3.91  9.28e- 5\n## 5 poverty_cut(0.2,1]       6.67      0.716     9.32  2.22e-20\n# create a data frame with the names for each combination of the percent\n# uninsured in PLACES or ACS variables in one column and each of the categorical\n# exposure variables in another column\nmodel_formulae <- \n  tidyr::expand_grid(\n    exposure = c('ICEraceinc_cut',\n      'poverty_cut',\n      'median_income_cut',\n      'crowding_cut',\n      'prop_black_cut',\n      'prop_hispanic_cut',\n      'prop_white_nh_cut'),\n    outcome = c('pct_uninsured_19_64_acs', 'pct_uninsured_18_64_places'))\n\n# create a column that puts the uninsurance variables on the left-hand-side\n# and the exposure variables on the right hand side\nmodel_formulae %<>% mutate(formula = paste0(outcome, \" ~ \", exposure))\n\n# fit regression models using the robust linear model rlm method from \n# the MASS package\nmodel_formulae %<>% rowwise() %>% mutate(\n  model = list(\n    # note that you do need to have the MASS package installed;\n    # we aren't loading it with library() anywhere it because it causes\n    # conflicts with the dplyr::select function\n    MASS:::rlm.formula(\n      formula = formula,\n      data = uninsurance,\n      weights = uninsurance$estimate_total_19_64_population\n    )\n  ),\n  model_coefs = list(broom::tidy(model, conf.int=TRUE))\n)\n\n# extract the coefficient estimates on the exposure terms\nmodel_formulae %<>% mutate(model_coefs = list(filter(model_coefs, term != '(Intercept)')))\n\n# unnest the model coefficients data\nmodel_formulae %<>% unnest(cols = model_coefs)\n\n# create a plot showing the coefficient estimates from each model  - \n# \n# the visualization here is what's typically called a \"forest plot\" and shows\n# the point-estimates for the coefficients and their 95% confidence intervals\n# along with a dashed line at zero so we can see if the coefficients are\n# significantly different from the null hypothesis (the null hypothesis being\n# that the coefficient is zero).\nmodel_formulae %>% \n  mutate(\n    term = stringr::str_remove(term, exposure), # remove the exposure string repeated in each term\n    term_size = as.numeric(stringr::str_extract(term, \"[0-9\\\\-\\\\.e\\\\+]+\")), # get the numeric value of each term;\n    # e.g. if the term was originally poverty_cut[0,0.05) then this regular expression extracts the 0\n    term = forcats::fct_reorder(term, -term_size)\n    # using the term size, we order the terms so they appear in the correct order\n  ) %>% \n  ggplot(aes(x = estimate, xmin = conf.low, xmax = conf.high, y = term,\n             shape = outcome, color = outcome)) + \n  geom_vline(xintercept = 0, linetype='dashed') + \n  geom_linerange(position = position_dodge(width=.4)) + \n  geom_point(position = position_dodge(width=.4)) + \n  facet_wrap(~exposure, scales='free_y') + \n  xlab(\"Additional Percentage Points of Uninsurance\") + \n  ggtitle(\"Modeled Effects Associated with Each ABSM\",\n          subtitle = paste0('Models were fit separately for each ABSM')\n  )\n# let's take a look at some specific states\n\n# extract the maximum uninsurance rate across both ACS and PLACES so that we can\n# use it in making the color palettes consistent between the two maps for ACS\n# and PLACES\nmax_uninsurance_rate <- \n  pmax(max(uninsurance %>% \n    filter(state_abbr == 'TX') %>% pull(pct_uninsured_19_64_acs)),\n  max(uninsurance %>% \n    filter(state_abbr == 'TX') %>% pull(pct_uninsured_18_64_places)))\n\n# create a map showing the ACS direct estimates of uninsurance rates\nacs_figure <- uninsurance %>% \n  filter(state_abbr == 'TX') %>% \n  ggplot(aes(fill = pct_uninsured_19_64_acs)) + \n  geom_sf(size = .1, color = 'white') + \n  scale_fill_viridis_c(limits = c(0, max_uninsurance_rate),\n                       labels = scales::percent_format(scale = 1)) + \n  labs(fill = \"Percent Uninsured (19-64)\") + \n  ggtitle(\"ACS Percent Uninsurance Estimates\")\n\n# create a map showing the PLACES estimates of uninsurance rates\nplaces_figure <- uninsurance %>% \n  filter(state_abbr == 'TX') %>% \n  ggplot(aes(fill = pct_uninsured_18_64_places)) + \n  geom_sf(size = .1, color = 'white') + \n  scale_fill_viridis_c(limits = c(0, max_uninsurance_rate),\n                       labels = scales::percent_format(scale = 1)) + \n  labs(fill = \"Percent Uninsured (18-64)\") + \n  ggtitle(\"Places Percent Uninsurance Estimates\")\n\n# combine the two figures into one side-by-side figure\nacs_figure + places_figure\n\n# save the figure\nggsave(\"images/11-health-insurance-comparison/texas_comparison.png\",\n       height = 6, width = 12)\n\n# create a scatter plot showing the ACS vs. PLACES relationship in Texas\nuninsurance %>% \n  filter(state_abbr == 'TX') %>% \n  ggplot(aes(x = pct_uninsured_19_64_acs, y = pct_uninsured_18_64_places)) + \n  geom_point() + \n  xlim(c(0, max_uninsurance_rate)) + \n  ylim(c(0, max_uninsurance_rate)) + \n  geom_abline(slope = 1, intercept = 0)\n\n# save the figure\nggsave(\"images/11-health-insurance-comparison/texas_comparison_scatter.png\",\n       height = 6, width = 6)\n\n\n# florida --- \n\n# extract the maximum uninsurance rate across both ACS and PLACES so that we can\n# use it in making the color palettes consistent between the two maps for ACS\n# and PLACES\nmax_uninsurance_rate <- \n  pmax(max(uninsurance %>% \n    filter(state_abbr == 'FL') %>% pull(pct_uninsured_19_64_acs)),\n  max(uninsurance %>% \n    filter(state_abbr == 'FL') %>% pull(pct_uninsured_18_64_places)))\n\n# create a map showing the ACS direct estimates of uninsurance rates\nacs_figure <- uninsurance %>% \n  filter(state_abbr == 'FL') %>% \n  ggplot(aes(fill = pct_uninsured_19_64_acs)) + \n  geom_sf(size = .1, color = 'white') + \n  scale_fill_viridis_c(limits = c(0, max_uninsurance_rate),\n                       labels = scales::percent_format(scale = 1)) + \n  labs(fill = \"Percent Uninsured (19-64)\") + \n  ggtitle(\"ACS Percent Uninsurance Estimates\")\n\n# create a map showing the PLACES estimates of uninsurance rates\nplaces_figure <- uninsurance %>% \n  filter(state_abbr == 'FL') %>% \n  ggplot(aes(fill = pct_uninsured_18_64_places)) + \n  geom_sf(size = .1, color = 'white') + \n  scale_fill_viridis_c(limits = c(0, max_uninsurance_rate),\n                       labels = scales::percent_format(scale = 1)) + \n  labs(fill = \"Percent Uninsured (18-64)\") + \n  ggtitle(\"Places Percent Uninsurance Estimates\")\n\n# combine the two figures into one side-by-side figure\nacs_figure + places_figure\n\n# save the figure\nggsave(\"images/11-health-insurance-comparison/florida_comparison.png\",\n       height = 6, width = 12)\n\n# create a scatter plot showing the ACS vs. PLACES relationship in Florida\nuninsurance %>% \n  filter(state_abbr == 'FL') %>% \n  ggplot(aes(x = pct_uninsured_19_64_acs, y = pct_uninsured_18_64_places)) + \n  geom_point() + \n  xlim(c(0, max_uninsurance_rate)) + \n  ylim(c(0, max_uninsurance_rate)) + \n  geom_abline(slope = 1, intercept = 0)\n\nggsave(\"images/11-health-insurance-comparison/florida_comparison_scatter.png\",\n       height = 6, width = 6)\n\n\n# minnesota --- \n\n# follows the same approach as shown above for Texas and Florida \n\nmax_uninsurance_rate <- \n  pmax(max(uninsurance %>% \n    filter(state_abbr == 'MN') %>% pull(pct_uninsured_19_64_acs)),\n  max(uninsurance %>% \n    filter(state_abbr == 'MN') %>% pull(pct_uninsured_18_64_places)))\n\nacs_figure <- uninsurance %>% \n  filter(state_abbr == 'MN') %>% \n  ggplot(aes(fill = pct_uninsured_19_64_acs)) + \n  geom_sf(size = .1, color = 'white') + \n  scale_fill_viridis_c(limits = c(0, max_uninsurance_rate),\n                       labels = scales::percent_format(scale = 1)) + \n  labs(fill = \"Percent Uninsured (19-64)\") + \n  ggtitle(\"ACS Percent Uninsurance Estimates\")\n\nplaces_figure <- uninsurance %>% \n  filter(state_abbr == 'MN') %>% \n  ggplot(aes(fill = pct_uninsured_18_64_places)) + \n  geom_sf(size = .1, color = 'white') + \n  scale_fill_viridis_c(limits = c(0, max_uninsurance_rate),\n                       labels = scales::percent_format(scale = 1)) + \n  labs(fill = \"Percent Uninsured (18-64)\") + \n  ggtitle(\"Places Percent Uninsurance Estimates\")\n\nacs_figure + places_figure\n\nggsave(\"images/11-health-insurance-comparison/minnesota_comparison.png\",\n       height = 6, width = 12)\n\n# wisconsin --- \n\n# follows the same approach as shown above for Texas and Florida \n\nuninsurance %>% \n  filter(state_abbr == 'MN') %>% \n  ggplot(\n    aes(x = pct_uninsured_19_64_acs, \n        xmin = pct_uninsured_18_64_acs_ci_low, \n        xmax = pct_uninsured_18_64_acs_ci_high,\n        y = pct_uninsured_18_64_places, \n        ymin = pct_uninsured_18_64_places_ci_low,\n        ymax = pct_uninsured_18_64_places_ci_high,\n        size = estimate_total_19_64_population)\n  ) + \n  geom_point(alpha = .5) + \n  geom_errorbar(size = .1) + \n  geom_errorbarh(size = .1) + \n  xlim(c(0, max_uninsurance_rate+8)) + \n  ylim(c(0, max_uninsurance_rate+8)) + \n  scale_size_continuous(trans = scales::log10_trans(), labels = scales::comma_format(),\n                        range = c(0.1, 3)) + \n  geom_abline(slope = 1, intercept = 0) + \n  ggtitle(\"Percent Without Health Insurance in Minnesota\",\n          \"95% Margins of Error Shown for Each of PLACES and ACS\")\n\nggsave(\"images/11-health-insurance-comparison/minnesota_comparison_scatter.png\",\n       height = 6, width = 9)\n\n\n# wisconsin --- \n\n# follows the same approach as above for Texas, Florida, Minnesota\n\nmax_uninsurance_rate <- \n  pmax(max(uninsurance %>% \n    filter(state_abbr == 'WI') %>% pull(pct_uninsured_19_64_acs)),\n  max(uninsurance %>% \n    filter(state_abbr == 'WI') %>% pull(pct_uninsured_18_64_places)))\n\nacs_figure <- uninsurance %>% \n  filter(state_abbr == 'WI') %>% \n  ggplot(aes(fill = pct_uninsured_19_64_acs)) + \n  geom_sf(size = .1, color = 'white') + \n  scale_fill_viridis_c(limits = c(0, max_uninsurance_rate),\n                       labels = scales::percent_format(scale = 1)) + \n  labs(fill = \"Percent Uninsured (19-64)\") + \n  ggtitle(\"ACS Percent Uninsurance Estimates\")\n\n\n\nplaces_figure <- uninsurance %>% \n  filter(state_abbr == 'WI') %>% \n  ggplot(aes(fill = pct_uninsured_18_64_places)) + \n  geom_sf(size = .1, color = 'white') + \n  scale_fill_viridis_c(limits = c(0, max_uninsurance_rate),\n                       labels = scales::percent_format(scale = 1)) + \n  labs(fill = \"Percent Uninsured (18-64)\") + \n  ggtitle(\"Places Percent Uninsurance Estimates\")\n\n\nacs_figure + places_figure\n\nggsave(\"images/11-health-insurance-comparison/wisconsin_comparison.png\",\n       height = 6, width = 12)\n\n# this figure slightly differs from the above approach in that it includes error-bars\nuninsurance %>% \n  filter(state_abbr == 'WI') %>% \n  ggplot(\n    aes(x = pct_uninsured_19_64_acs, \n        xmin = pct_uninsured_18_64_acs_ci_low, \n        xmax = pct_uninsured_18_64_acs_ci_high,\n        y = pct_uninsured_18_64_places, \n        ymin = pct_uninsured_18_64_places_ci_low,\n        ymax = pct_uninsured_18_64_places_ci_high,\n        size = estimate_total_19_64_population)\n  ) + \n  geom_point(alpha = .5) + \n  geom_errorbar(size = .1) + \n  geom_errorbarh(size = .1) + \n  xlim(c(0, max_uninsurance_rate+8)) + \n  ylim(c(0, max_uninsurance_rate+8)) + \n  scale_size_continuous(trans = scales::log10_trans(), labels = scales::comma_format(),\n                        range = c(0.1, 3)) + \n  geom_abline(slope = 1, intercept = 0) + \n  ggtitle(\"Percent Without Health Insurance in Wisconsin\",\n          \"95% Margins of Error Shown for Each of PLACES and ACS\")\n\nggsave(\"images/11-health-insurance-comparison/wisconsin_comparison_scatter.png\",\n       height = 6, width = 9)"},{"path":"conclusions.html","id":"conclusions","chapter":"12 Conclusion","heading":"12 Conclusion","text":"manual highlighted history, context, rationale, approaches using spatial analysis tools area based social metrics (ABSMs) describe, analyze, visualize communicate health inequities across geographic levels.case studies serve opportunities apply topics highlighted manual specific datasets research questions. workshops held Summer 2022, participants worked case studies using following presentation template.template offers systematic way work key questions carrying spatial analyses using ABSMs applying health equity lens. highlight key questions elements template serve guide developing research projects studies nature. hope may use current future work!","code":""},{"path":"conclusions.html","id":"key-questions-to-ask","chapter":"12 Conclusion","heading":"12.1 Key questions to ask","text":"State study objective include:\nPopulation\nTimeframe\ninterest?\nwant , , use knowledge generated study advance health justice?\nState study objective include:PopulationTimeframeWhy interest?want , , use knowledge generated study advance health justice?key health equity concerns?key health equity concerns?ABSMs relevant objective?\nABSM intended measure?\nincluded measurement (numerator/denominator)?\n\ndata source?\nkey concerns?\nABSMs relevant objective?ABSM intended measure?\nincluded measurement (numerator/denominator)?\nincluded measurement (numerator/denominator)?data source?key concerns?geographic level used? ?geographic level used? ?analytic approach?\noutcome interest\ncharacterizing relationships health outcome interest ABSM interest? (e.g. Risk ratio? Risk difference? Rate difference? Rate ratio?)\nage adjustment ?\nusing regression model? , type?\ngoing model spatial effects? (e.g. Multilevel spatial modeling approach)\nanalytic approach?outcome interestHow characterizing relationships health outcome interest ABSM interest? (e.g. Risk ratio? Risk difference? Rate difference? Rate ratio?)age adjustment ?using regression model? , type?going model spatial effects? (e.g. Multilevel spatial modeling approach)key findings?\nPresent exploratory (univariate) tables, figures /maps help contextualize results\nPresent regression models relevant\nSummarize association ABSMs health outcome\nComment effect age adjustment\nPresent maps visualizations\nkey findings?Present exploratory (univariate) tables, figures /maps help contextualize resultsPresent regression models relevantSummarize association ABSMs health outcomeComment effect age adjustmentPresent maps visualizationsFinally, found ideas examples training manual use, encourage share widely colleagues networks links manual Public Health Disparities Geocoding Project. researchers, health agencies, community-based organizations rigorously use methods, informed concepts woven throughout manual, richer better evidence base inform analysis action rectify health inequities. Let us part!","code":""},{"path":"glossary.html","id":"glossary","chapter":"13 Glossary","heading":"13 Glossary","text":"ABSM see “Area-based socioeconomic measure”/”Area-based social metric”address cleaning process taking original address retaining key elements address (building number, street street type), well correcting spelling errors standardizing abbreviations.age stratum One category age series age categories.American Community Survey new national survey administered US Census Bureau provides yearly data states counties decennial censuses , 2008, provide data census tracts well. information see https://www.census.gov/programs-surveys/acs/ .area geographic region whose boundaries may defined socially, topographically, ecologically (singly combination).area-based measure see “area-based socioeconomic measure”/”area-based social metric”area-based socioeconomic measure/area-based social metric specifically defined measure used characterize social contextual conditions area (opposed social economic characteristics individuals). “area-based socioeconomic measures,” example, might pertain “percent persons living poverty”; “area-based social metric” broader construct can include limited economic data, e.g., metric measure racialized segregation racialized economic segregation.block group “subdivision census tract, generally containing 600 3,000 people, optimum size 1,500 people. block groups delineated local participants part U.S. Census Bureau’s Participant Statistical Areas Program. lowest level geographic hierarchy U.S. Census Bureau tabulates presents sample data. (Appendix . Census 2000 Geographic Terms Concepts. https://www2.census.gov/geo/pdfs/reference/glossry2.pdf )case record see case reportcase report Data individual indicates incidence prevalence morbidity mortality outcome.cdf see cumulative distribution functioncell basic unit aggregation based cross-classification number categorical variables. example, cases occurring among women ages 40-44 given census tract aggregated single cell defined gender, age, area.census geography scheme classification areas used U.S. census. example, census tract block group types areas data classified U.S. census data.census tract “small relatively permanent statistical subdivision delineated local participants part U.S. Census Bureau’s Participant Statistical Areas program. first delineated designed relatively homogenous respect population characteristics, economic status living conditions. average size 1,500 8,000 people, optimum size 4,000 people. geographic size varies considerably depending population density. (Appendix . Census 2000 Geographic Terms Concepts. http://www.census.gov/geo/www/tiger/ glossry2.pdf)census variable Items data organized U.S. Census bureau. Data variables structured form census tables, may include one census variables.class see social classcomma-delimited file text file format data fields separated commas. Microsoft Excel file extension type data .csv .composite index see composite measurecomposite measure measure combines information one component variable. example, Townsend index consists percent unemployment, percent renters, percent owning car, percent crowding.compositional factors Attributes areas derive characteristics individuals.construct theoretical concept idea.contextual factors Attributes areas derive structural social characteristics area.CT see census tractcumulative distribution function given value, area probability function value (.e. cdf(x) = Pr[X<=x]). calculated part deriving relative index inequality, cumulative distribution function area-based socioeconomic measure (ordered affluent deprived) given value can interpreted proportion population affluent.denominator two definitions denominator depend measure calculated. calculating rates, denominator amount person-time observed time cases eligible occur. calculating ABSMs, denominator total number persons area ABSM measured.deprivation “Deprivation can conceptualized measured, individual area level, relation : material deprivation, referring ‘dietary, clothing, housing, home facilities, environment, location work (paid unpaid), social deprivation, referring rights relation ’employment, family activities, integration community, formal participation social institutions, recreation education’ “(Krieger N. Glossary Social Epidemiology, J Epidemiol Community Health 2001; 55:693-700.)direct age standardization method adjusting population rate age, yielding hypothetical rate observed population studied age distribution externally defined standard population. direct standardization, stratum specific rates multiplied weights derived standard reference population, summed yield summary rate. Rates standardized external standard may meaningfully compared examine differences due age.ecosocial theory theory seeks “integrate social biological reasoning dynamic, historical ecological perspective develop new insights determinants population distributions disease social inequalities health.” core concepts ecosocial theory include 1. embodiment, 2. pathways embodiment, 3. cumulative interplay exposure, susceptibility, resistance, 4. accountability agency. (Krieger N. Glossary Social Epidemiology, J Epidemiol Community Health 2001; 55:693-700.)etiologic period duration time disease develops, referring time initial exposure time outcome caused exposure occurs.exact confidence limits Exact confidence limits rely normal approximation. used exact confidence limits calculate confidence intervals rate zero.gamma confidence intervals Confidence intervals direct standardized rate based gamma distribution. practical consequence using gamma confidence intervals confidence intervals rates cross zero. details see Fay MP, Feuer EJ. Confidence intervals directly standardized rates: method based gamma distribution. Statistics Medicine 1997;16:791-801gender “social construct regarding culture-bound conventions, roles behaviors , well relationships among, women men boys girls.” (Krieger N. Glossary Social Epidemiology, J Epidemiol Community Health 2001; 55:693-700.) – definition focused social divisions predicated dominant social structures norms shaped sexism gender binarism (see: Krieger N. Measures Racism, Sexism, Heterosexism, Gender Binarism Health Equity Research: Structural Injustice Embodied Harm-Ecosocial Analysis. Annu Rev Public Health. 2020 Apr 2;41:37-62. doi: 10.1146/annurev-publhealth-040119-094017. Epub 2019 Nov 25.).geocoding assignment numeric code geographical locationgeographical information systems Technology based systems combine layers geographic data offer greater understanding characteristics places.Gini measurement inequality ranges 0 1, ratio area Lorenz curve area diagonal graph Lorenz curve. value one indicate complete inequality distribution, 0 indicates inequality.GIS see geographical information systemsincidence rate number events divided person-time risk.incidence rate difference absolute difference two incidence rates. incidence rate among exposed proportion population, minus incidence rate unexposed portion population, gives absolute measure effect given exposure.incidence rate ratio ratio two incidence rates. incidence rate among exposed proportion population, divided incidence rate unexposed portion population, gives relative measure effect given exposure.index concentration extremes (ICE) measure spatial social polarization quantifying concentrations, within specified area, social groups defined extremes deprivation privilege specific metric chosen; examples ICE measures can relation income (quantifying concentration high vs. low income households, capturing high vs low economic privilege), racialized segregation (e.g., using social groups White non-Hispanic vs. Black non-Hispanic persons, thereby capturing high vs low racialized privilege), racialized economic segregation (e.g., using social groups white non-Hispanic high income households vs. Black non-Hispanic low-income households, thereby capturing joint exposure racialized economic residential segregation).indirect age standardization method adjusting population rate age, yielding hypothetical rate observed population studied age distribution externally defined standard population. Indirect standardization based deriving expected number events using externally defined standard population, contrasting value observed number events population studied. expected number events derived multiplying stratum-specific counts study population stratum-specific rates standard population. ratio total observed events number expected standardized mortality (morbidity) ratio (SMR). indirect standardized rate calculated multiplying SMR crude rate standard population.lifecourse perspective “Refers health status given age, given birth cohort, reflects contemporary conditions embodiment prior living circumstances, utero onwards” (Krieger N. Glossary Social Epidemiology, J Epidemiol Community Health 2001; 55:693-700.), analyses taking account age exposure, duration exposure, etiologic period, whether critical sensitive periods (age group) exposures likely increase risk (protect ) specified health outcomes.material deprivation see deprivationmultilevel analysis Analyses conceptualize analyze associations multiple levels, e.g., employ individual- area-based data relation specified outcome. analyses typically entail use variance components models partition variance multiple levels, examine contribution factors measured different levels overall variation outcome.numerator two definitions numerator depend measure calculated. calculating rates, numerator number events observed. calculating ABSMs, numerator number persons households area socioeconomic characteristic interest.occupational class measurement socioeconomic position based upon job characteristics. One example original British Registrar General’s Social Class scheme, created early 20th c CE, based skill. replaced 2001 National Statistics Socio-Economic Classification system (NS-SEC), occupational metric based “employment relations conditions occupations” (see: https://www.ons.gov.uk/methodology/classificationsandstandards/otherclassifications/thenationalstatisticssocioeconomicclassificationnssecrebasedonsoc2010 ). US context, NS-SEC can adapted create ABSM “working class” measure, comprised occupations employed primarily non-supervisory employees (see: Krieger N, Barbeau EM, Soobader MJ. Class matters: U.S. versus U.K. measures occupational disparities access health services health status 2000 U.S. National Health Interview Survey. Int J Health Serv. 2005;35(2):213-36. doi: 10.2190/JKRE-AH92-EDV8-VHYC.)operational definition description variable terms variable actually measured.person-time sum time risk persons population.Poisson model regression model used count data.population attributable fraction theoretical reduction incidence expected entire population level exposure specified referent group (group low exposure).poverty “impoverished lack denied adequate resources participate meaningfully society” (Krieger N. Glossary Social Epidemiology, J Epidemiol Community Health 2001; 55:693-700.)poverty area US, federal criteria “poverty area” area 20% population poverty line (see: https://www.census.gov/library/publications/1995/demo/sb95-13.html ).poverty line poverty threshold takes account household size age composition intended indicate income level subsistence needs met. poverty line US based value three times cost economy food basket 1963, adjusted inflation. See: “Census Bureau Measures Poverty (Official Measure)” : http://www.census.gov/hhes/poverty/ povdef.htmlpublic health surveillance system structure facilitates continuous systematic collection descriptive information monitoring health populations (Buehler, Chapter 22: Surveillance, Rothman Greenland, Modern Epidemiology, 2nd edition, 1998, p 435-457).racialized group US categories “race/ethnicity” “social, biological, category, referring social groups, often sharing cultural heritage ancestry, forged oppressive systems race relations, justified ideology, one group benefits dominating groups, defines others domination possession selective arbitrary physical characteristics (example, skin color)” (Krieger N. Glossary Social Epidemiology, J Epidemiol Community Health 2001; 55:693-700.). US, federal data, including US census data, must conform 1997 Office Management Budget Revisions Standards Classification Federal Data Race Ethnicity, require classification categories “races” “ethnicity” (defined solely “Hispanic” vs. “non-Hispanic”); see: https://www.govinfo.gov/content/pkg/FR-1997-10-30/pdf/97-28653.pdf . Work underway US Census shift asking 2 separate questions (one “race,” “ethnicity”) one question includes (along option tick many boxes relevant); see: https://www.census.gov/library/stories/2021/08/improved-race-ethnicity-measures-reveal-united-states-population-much--multiracial.htmlrate difference see incidence rate differencerate ratio see incidence rate ratiorelative index inequality summary measure “total population impact” takes account socioeconomic gradient outcome, well population distribution socioeconomic variable. RII interpretable ratio rate theoretically deprived segment population, compared rate theoretically least deprived segment.RII see relative index inequalitySEP see socioeconomic positionsex “biological construct premised upon biological characteristics enabling sexual reproduction” (Krieger N. Glossary Social Epidemiology, J Epidemiol Community Health 2001; 55:693-700.)social class “Refers social groups arising interdependent economic relationships among people” (Krieger N. Glossary Social Epidemiology, J Epidemiol Community Health 2001; 55:693-700.)social deprivation see deprivationsocioeconomic position “aggregate concept includes resource-based prestige-based measures, linked childhood adult social class position” (Krieger N. Glossary Social Epidemiology, J Epidemiol Community Health 2001; 55:693-700.)socioeconomic status term referring prestige-based measures socioeconomic position, determined rankings social hierarchy (Krieger N. Glossary Social Epidemiology, J Epidemiol Community Health 2001; 55:693-700.)spatiotemporal , relating , existing space time.spatiotemporal mismatch mismatch data derived different sources arises (1) inconsistency boundaries data sources /(2) inconsistency timeframe data sources.transpose reverse orientation matrix, values across rows become values columns, values columns become values across rows.wealth Conceptually, wealth refers accumulated assets. ABSM capture wealth operationalized census data percent owner-occupied homes worth 400% median value owned homes.ZCTA see “Zip code tabulation area”ZIPcode “Administrative units established United States Postal Service … efficient delivery mail, therefore generally respect political census statistical area boundaries” (Appendix . Census 2000 Geographic Terms Concepts).ZIPcode tabulation area statistical geographic area approximates delivery area U.S. Postal service Zip code. approximation replaces Zip code areas used Census Bureau conjunction 1990 earlier censuses.(Appendix . Census 2000 Geographic Terms Concepts.)Z-score Also referred Z-ratio Z-value, equal value X minus mean X, divided standard deviation.","code":""}]
